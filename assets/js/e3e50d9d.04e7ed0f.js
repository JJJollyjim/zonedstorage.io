"use strict";(self.webpackChunkzonedstorage_io=self.webpackChunkzonedstorage_io||[]).push([[571],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=d(n),u=o,f=m["".concat(l,".").concat(u)]||m[u]||p[u]||r;return n?a.createElement(f,i(i({ref:t},c),{},{components:n})):a.createElement(f,i({ref:t},c))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var d=2;d<r;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9272:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},metadata:function(){return l},toc:function(){return d},Yes:function(){return c},No:function(){return p},default:function(){return u}});var a=n(7462),o=n(3366),r=(n(7294),n(3905)),i=["components"],s={id:"fs",title:"File Systems",sidebar_label:"File Systems"},l={unversionedId:"linux/fs",id:"linux/fs",isDocsHomePage:!1,title:"File Systems",description:"The dm-zoned device mapper target allows using any file",source:"@site/docs/linux/fs.md",sourceDirName:"linux",slug:"/linux/fs",permalink:"/zonedstorage.io/docs/linux/fs",version:"current",sidebar_label:"File Systems",frontMatter:{id:"fs",title:"File Systems",sidebar_label:"File Systems"},sidebar:"docs",previous:{title:"Device Mapper",permalink:"/zonedstorage.io/docs/linux/dm"},next:{title:"Applications",permalink:"/zonedstorage.io/docs/applications"}},d=[{value:"zonefs",id:"zonefs",children:[{value:"Overview",id:"overview",children:[]},{value:"On-Disk Metadata",id:"on-disk-metadata",children:[]},{value:"Zone Type Sub-Directories",id:"zone-type-sub-directories",children:[]},{value:"Zone files",id:"zone-files",children:[]},{value:"Format options",id:"format-options",children:[]},{value:"IO error handling",id:"io-error-handling",children:[]},{value:"Mount options",id:"mount-options",children:[]},{value:"Zonefs User Space Tools",id:"zonefs-user-space-tools",children:[]},{value:"Examples",id:"examples",children:[]}]},{value:"f2fs",id:"f2fs",children:[{value:"Zoned Block Device Support",id:"zoned-block-device-support",children:[]},{value:"Zone Capacity Support",id:"zone-capacity-support",children:[]},{value:"Limitations",id:"limitations",children:[]},{value:"Usage Example with a Host Managed SMR HDD",id:"usage-example-with-a-host-managed-smr-hdd",children:[]},{value:"Usage Example with a NVMe ZNS SSD",id:"usage-example-with-a-nvme-zns-ssd",children:[]}]},{value:"Btrfs",id:"btrfs",children:[{value:"Block Allocation Changes",id:"block-allocation-changes",children:[]},{value:"I/O Management",id:"io-management",children:[]},{value:"Upstream Contribution",id:"upstream-contribution",children:[]}]},{value:"XFS",id:"xfs",children:[]},{value:"ext4",id:"ext4",children:[]}];function c(){return(0,r.kt)("span",{style:{color:"#00ff00"}},"yes")}function p(){return(0,r.kt)("span",{style:{color:"#ff0000"}},"no")}var m={toc:d,Yes:c,No:p};function u(e){var t=e.components,n=(0,o.Z)(e,i);return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/dm#dm-zoned"},(0,r.kt)("em",{parentName:"a"},"dm-zoned"))," device mapper target allows using any file\nsystem with host managed zoned block devices by hiding the device sequential\nwrite constraints. This is a simple solution to enable a file system use but not\nnecessarily the most efficient due to the potentially high overhead of a block\nbased zone reclaim process."),(0,r.kt)("p",null,"Supporting zoned block devices directly in a file system implementation can\nlead to a more efficient zone reclaim processing as the file system metadata\nand file abstraction provide more information on the usage and validity status\nof storage blocks compared to the raw block device based approach."),(0,r.kt)("p",null,"Furthermore, a file system design may lend itself well to the sequential write\nconstraint of host managed zoned block devices. This is the case for\nlog-structured file systems such as ",(0,r.kt)("em",{parentName:"p"},"f2fs")," and copy-on-write (CoW) file systems\nsuch as ",(0,r.kt)("em",{parentName:"p"},"Btrfs"),"."),(0,r.kt)("h2",{id:"zonefs"},"zonefs"),(0,r.kt)("p",null,"zonefs is a very simple file system exposing each zone of a zoned block device\nas a file. ",(0,r.kt)("em",{parentName:"p"},"zonefs")," is included with the upstream Linux kernel since version\n5.6.0."),(0,r.kt)("h3",{id:"overview"},"Overview"),(0,r.kt)("p",null,"Unlike a regular POSIX-compliant file system with native zoned block device\nsupport (e.g. ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/fs#f2fs"},(0,r.kt)("em",{parentName:"a"},"f2fs")),"), ",(0,r.kt)("em",{parentName:"p"},"zonefs")," does not hide the sequential write\nconstraint of zoned block devices to the user. Files representing sequential\nwrite zones of the device must be written sequentially starting from the end of\nthe file (append only writes)."),(0,r.kt)("p",null,"As such, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," is in essence closer to a raw block device access interface\nthan to a full-featured POSIX file system. The goal of ",(0,r.kt)("em",{parentName:"p"},"zonefs"),' is to simplify\nthe implementation of zoned block device support in applications by replacing\nraw block device file accesses with the richer regular file API, avoiding\nrelying on direct block device file ioctls which may be more obscure to\ndevelopers. One example of this approach is the implementation of LSM\n(log-structured merge) tree structures (such as used in RocksDB and LevelDB) on\nzoned block devices by allowing SSTables to be stored in a zone file similarly\nto a regular file system rather than as a range of sectors of the entire disk.\nThe introduction of the higher level construct "one file is one zone" can help\nreducing the amount of changes needed in the application as well as introducing\nsupport for different application programming languages.'),(0,r.kt)("p",null,"The files representing zones are grouped by zone type, which are themselves\nrepresented by sub-directories. This file structure is built entirely using\nzone information provided by the device and so does not require any complex\non-disk metadata structure."),(0,r.kt)("h3",{id:"on-disk-metadata"},"On-Disk Metadata"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"zonefs")," on-disk metadata is composed only of an immutable super block which\npersistently stores a magic number and optional feature flags and values. On\nmount, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," uses the block layer API function ",(0,r.kt)("inlineCode",{parentName:"p"},"blkdev_report_zones()")," to\nobtain the device zone configuration and populates the mount point with a\nstatic file tree solely based on this information. File sizes come from the\ndevice zone type and write pointer position managed by the device itself."),(0,r.kt)("p",null,"The super block is always written on disk at sector 0. The first zone of the\ndevice storing the super block is never exposed as a zone file by ",(0,r.kt)("em",{parentName:"p"},"zonefs"),". If\nthe zone containing the super block is a sequential zone, the ",(0,r.kt)("inlineCode",{parentName:"p"},"mkzonefs"),' format\ntool always "finishes" the zone, that is, it transitions the zone to a full\nstate to make it read-only, preventing any data write.'),(0,r.kt)("h3",{id:"zone-type-sub-directories"},"Zone Type Sub-Directories"),(0,r.kt)("p",null,"Files representing zones of the same type are grouped together under the same\nsub-directory automatically created on mount."),(0,r.kt)("p",null,'For conventional zones, the sub-directory "cnv" is used. This directory is\nhowever created if and only if the device has usable conventional zones. If\nthe device only has a single conventional zone at sector 0, the zone will not\nbe exposed as a file as it will be used to store the ',(0,r.kt)("em",{parentName:"p"},"zonefs"),' super block. For\nsuch devices, the "cnv" sub-directory will not be created.'),(0,r.kt)("p",null,'For sequential write zones, the sub-directory "seq" is used.'),(0,r.kt)("p",null,"These two directories are the only directories that exist in ",(0,r.kt)("em",{parentName:"p"},"zonefs"),'. Users\ncannot create other directories and cannot rename nor delete the "cnv" and\n"seq" sub-directories.'),(0,r.kt)("p",null,"The size of the directories indicated by the ",(0,r.kt)("inlineCode",{parentName:"p"},"st_size")," field of ",(0,r.kt)("inlineCode",{parentName:"p"},"struct stat"),",\nobtained with the ",(0,r.kt)("inlineCode",{parentName:"p"},"stat()")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"fstat()")," system calls, indicates the number of\nfiles existing under the directory."),(0,r.kt)("h3",{id:"zone-files"},"Zone files"),(0,r.kt)("p",null,'Zone files are named using the number of the zone they represent within the set\nof zones of a particular type. That is, both the "cnv" and "seq" directories\ncontain files named "0", "1", "2", ... The file numbers also represent\nincreasing zone start sector on the device.'),(0,r.kt)("p",null,"All read and write operations to zone files are not allowed beyond the file\nmaximum size, that is, beyond the zone size. Any access exceeding the zone\nsize is failed with the ",(0,r.kt)("inlineCode",{parentName:"p"},"-EFBIG")," error."),(0,r.kt)("p",null,"Creating, deleting, renaming or modifying any attribute of files is not allowed."),(0,r.kt)("p",null,"The number of blocks of a file as reported by ",(0,r.kt)("inlineCode",{parentName:"p"},"stat()")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"fstat()")," indicates\nthe size of the file zone, or in other words, the maximum file size."),(0,r.kt)("h4",{id:"conventional-zone-files"},"Conventional Zone Files"),(0,r.kt)("p",null,"The size of conventional zone files is fixed to the size of the zone they\nrepresent. Conventional zone files cannot be truncated."),(0,r.kt)("p",null,"These files can be randomly read and written using any type of I/O operation:\nbuffered I/Os, direct I/Os, memory mapped I/Os (mmap), etc. There are no I/O\nconstraint for these files beyond the file size limit mentioned above."),(0,r.kt)("h4",{id:"sequential-zone-files"},"Sequential zone files"),(0,r.kt)("p",null,'The size of sequential zone files grouped in the "seq" sub-directory represents\nthe file\'s zone write pointer position relative to the zone start sector.'),(0,r.kt)("p",null,"Sequential zone files can only be written sequentially, starting from the file\nend, that is, write operations can only be append writes. Zonefs makes no\nattempt at accepting random writes and will fail any write request that has a\nstart offset not corresponding to the end of the file, or to the end of the last\nwrite issued and still in-flight (for asynchronous I/O operations)."),(0,r.kt)("p",null,"Since dirty page writeback by the page cache does not guarantee a sequential\nwrite pattern, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," prevents buffered writes and writeable shared mappings\non sequential files. Only direct I/O writes are accepted for these files.\n",(0,r.kt)("em",{parentName:"p"},"zonefs")," relies on the sequential delivery of write I/O requests to the device\nimplemented by the block layer elevator (See ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/sched"},"Write Command Ordering"),")."),(0,r.kt)("p",null,"There are no restrictions on the type of I/O used for read operations in\nsequential zone files. Buffered I/Os, direct I/Os and shared read mappings are\nall accepted."),(0,r.kt)("p",null,"Truncating sequential zone files is allowed only down to 0, in which case, the\nzone is reset to rewind the file zone write pointer position to the start of\nthe zone, or up to the zone size, in which case the file's zone is transitioned\nto the FULL state (finish zone operation)."),(0,r.kt)("h3",{id:"format-options"},"Format options"),(0,r.kt)("p",null,"Several optional features of zonefs can be enabled at format time."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Conventional zone aggregation: ranges of contiguous conventional zones can be\naggregated into a single larger file instead of the default one file per zone."),(0,r.kt)("li",{parentName:"ul"},"File ownership: The owner UID and GID of zone files is by default 0 (root)\nbut can be changed to any valid UID/GID."),(0,r.kt)("li",{parentName:"ul"},"File access permissions: the default 640 access permissions can be changed.")),(0,r.kt)("h3",{id:"io-error-handling"},"IO error handling"),(0,r.kt)("p",null,"Zoned block devices may fail I/O requests for reasons similar to regular block\ndevices, e.g. due to bad sectors. However, in addition to such known I/O\nfailure pattern, the standards governing zoned block devices behavior define\nadditional conditions that can result in I/O errors."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"A zone may transition to the read-only condition:\nWhile the data already written in the zone is still readable, the zone can\nno longer be written. No user action on the zone (zone management command or\nread/write access) can change the zone condition back to a normal read/write\nstate. While the reasons for the device to transition a zone to read-only\nstate are not defined by the standards, a typical cause for such transition\nwould be a defective write head on an HDD (all zones under this head are\nchanged to read-only).")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"A zone may transition to the offline condition:\nAn offline zone cannot be read nor written. No user action can transition an\noffline zone back to an operational good state. Similarly to zone read-only\ntransitions, the reasons for a drive to transition a zone to the offline\ncondition are undefined. A typical cause would be a defective read-write head\non an HDD causing all zones on the platter under the broken head to be\ninaccessible.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Unaligned write errors:\nThese errors result from the host issuing write requests with a start sector\nthat does not correspond to a zone write pointer position when the write\nrequest is executed by the device. Even though ",(0,r.kt)("em",{parentName:"p"},"zonefs")," enforces sequential\nfile write for sequential zones, unaligned write errors may still happen in\nthe case of a partial failure of a very large direct I/O operation split into\nmultiple BIOs/requests or asynchronous I/O operations.  If one of the write\nrequest within the set of sequential write requests issued to the device\nfails, all write requests queued after it will become unaligned and fail.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Delayed write errors:\nSimilarly to regular block devices, if the device side write cache is enabled,\nwrite errors may occur in ranges of previously completed writes when the\ndevice write cache is flushed, e.g. on ",(0,r.kt)("inlineCode",{parentName:"p"},"fsync()"),".  Similarly to the previous\nimmediate unaligned write error case, delayed write errors can propagate\nthrough a stream of cached sequential data for a zone causing all data to be\ndropped after the sector that caused the error."))),(0,r.kt)("p",null,"All I/O errors detected by ",(0,r.kt)("em",{parentName:"p"},"zonefs")," are notified to the user with an error code\nreturn for the system call that triggered or detected the error. The recovery\nactions taken by ",(0,r.kt)("em",{parentName:"p"},"zonefs")," in response to I/O errors depend on the I/O type\n(read vs write) and on the reason for the error (bad sector, unaligned writes or\nzone condition change)."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For read I/O errors, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," does not execute any particular recovery action,\nbut only if the file zone is still in a good condition and there is no\ninconsistency between the file inode size and its zone write pointer position.\nIf a problem is detected, I/O error recovery is executed (see below table).")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For write I/O errors, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," I/O error recovery is always executed.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"A zone condition change to read-only or offline also always triggers ",(0,r.kt)("em",{parentName:"p"},"zonefs"),"\nI/O error recovery."))),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"zonefs")," minimal I/O error recovery may change a file size and file access\npermissions."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"File size changes:\nImmediate or delayed write errors in a sequential zone file may cause the file\ninode size to be inconsistent with the amount of data successfully written in\nthe file zone. For instance, the partial failure of a multi-BIO large write\noperation will cause the zone write pointer to advance partially, even though\nthe entire write operation will be reported as failed to the user. In such\ncase, the file inode size must be advanced to reflect the zone write pointer\nchange and eventually allow the user to restart writing at the end of the\nfile.\nA file size may also be reduced to reflect a delayed write error detected on\nfsync(): in this case, the amount of data effectively written in the zone may\nbe less than originally indicated by the file inode size. After such I/O\nerror, ",(0,r.kt)("em",{parentName:"p"},"zonefs")," always fixes the file inode size to reflect the amount of data\npersistently stored in the file zone.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Access permission changes:\nA zone condition change to read-only is indicated with a change in the file\naccess permissions to render the file read-only. This disables changes to the\nfile attributes and data modification. For offline zones, all permissions\n(read and write) to the file are disabled."))),(0,r.kt)("p",null,"Further action taken by ",(0,r.kt)("em",{parentName:"p"},"zonefs"),' I/O error recovery can be controlled by the\nuser with the "errors=xxx" mount option. The table below summarizes the result\nof ',(0,r.kt)("em",{parentName:"p"},"zonefs")," I/O error processing depending on the mount option and on the zone\nconditions."),(0,r.kt)("center",null,(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"center"},'"errors=xxx" mount option'),(0,r.kt)("th",{parentName:"tr",align:"center"},"Device zone condition"),(0,r.kt)("th",{parentName:"tr",align:"center"},"File size"),(0,r.kt)("th",{parentName:"tr",align:"center"},"File read"),(0,r.kt)("th",{parentName:"tr",align:"center"},"File write"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Device read"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Device write"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"remount-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"good"),(0,r.kt)("td",{parentName:"tr",align:"center"},"fixed"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"remount-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"read-only"),(0,r.kt)("td",{parentName:"tr",align:"center"},"as is"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"remount-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"good"),(0,r.kt)("td",{parentName:"tr",align:"center"},"fixed"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"read-only"),(0,r.kt)("td",{parentName:"tr",align:"center"},"as is"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-ro"),(0,r.kt)("td",{parentName:"tr",align:"center"},"offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"good"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"read-only"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"zone-offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"repair"),(0,r.kt)("td",{parentName:"tr",align:"center"},"good"),(0,r.kt)("td",{parentName:"tr",align:"center"},"fixed"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"repair"),(0,r.kt)("td",{parentName:"tr",align:"center"},"read-only"),(0,r.kt)("td",{parentName:"tr",align:"center"},"as is"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(c,{mdxType:"Yes"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"repair"),(0,r.kt)("td",{parentName:"tr",align:"center"},"offline"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0"),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)(p,{mdxType:"No"})))))),(0,r.kt)("p",null,"Further notes:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'The "errors=remount-ro" mount option is the default behavior of zonefs I/O\nerror processing if no errors mount option is specified.'),(0,r.kt)("li",{parentName:"ul"},'With the "errors=remount-ro" mount option, the change of the file access\npermissions to read-only applies to all files. The file system is remounted\nread-only.'),(0,r.kt)("li",{parentName:"ul"},"Access permission and file size changes due to the device transitioning zones\nto the offline condition are permanent. Remounting or reformatting the device\nwith mkfs.zonefs (mkzonefs) will not change back offline zone files to a good\nstate."),(0,r.kt)("li",{parentName:"ul"},"File access permission changes to read-only due to the device transitioning\nzones to the read-only condition are permanent. Remounting or reformatting\nthe device will not re-enable file write access."),(0,r.kt)("li",{parentName:"ul"},"File access permission changes implied by the remount-ro, zone-ro and\nzone-offline mount options are temporary for zones in a good condition.\nUnmounting and remounting the file system will restore the previous default\n(format time values) access rights to the files affected."),(0,r.kt)("li",{parentName:"ul"},"The repair mount option triggers only the minimal set of I/O error recovery\nactions, that is, file size fixes for zones in a good condition. Zones\nindicated as being read-only or offline by the device still imply changes to\nthe zone file access permissions as noted in the table above.")),(0,r.kt)("h3",{id:"mount-options"},"Mount options"),(0,r.kt)("p",null,'zonefs define the "errors=',(0,r.kt)("em",{parentName:"p"},"behavior"),'" mount option to allow the user to specify\nzonefs behavior in response to I/O errors, inode size inconsistencies or zone\ncondition changes. The defined behaviors are as follow.'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"remount-ro (default)"),(0,r.kt)("li",{parentName:"ul"},"zone-ro"),(0,r.kt)("li",{parentName:"ul"},"zone-offline"),(0,r.kt)("li",{parentName:"ul"},"repair")),(0,r.kt)("p",null,"The run-time I/O error actions defined for each behavior are detailed in the\nprevious section. Mount time I/O errors will cause the mount operation to fail.\nThe handling of read-only zones also differs between mount-time and run-time.\nIf a read-only zone is found at mount time, the zone is always treated in the\nsame manner as offline zones, that is, all accesses are disabled and the zone\nfile size set to 0. This is necessary as the write pointer of read-only zones\nis defined as invalib by the ZBC and ZAC standards, making it impossible to\ndiscover the amount of data that has been written to the zone. In the case of a\nread-only zone discovered at run-time, as indicated in the previous section.\nthe size of the zone file is left unchanged from its last updated value."),(0,r.kt)("h3",{id:"zonefs-user-space-tools"},"Zonefs User Space Tools"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"mkzonefs")," tool is used to format zoned block devices for use with ",(0,r.kt)("em",{parentName:"p"},"zonefs"),".\nThis tool is available on ",(0,r.kt)("a",{href:"https://github.com/westerndigitalcorporation/zonefs-tools",target:"_blank"},"GitHub"),"."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"zonefs-tools")," also includes a test suite which can be run against any zoned\nblock device, including\n",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/getting-started/nullblk"},(0,r.kt)("em",{parentName:"a"},"nullblk")," block device created with zoned mode"),"."),(0,r.kt)("h3",{id:"examples"},"Examples"),(0,r.kt)("p",null,"The following formats a 15TB host-managed SMR HDD with 256 MB zones\nwith the conventional zones aggregation feature enabled::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# mkzonefs -o aggr_cnv /dev/sdX\n# mount -t zonefs /dev/sdX /mnt\n# ls -l /mnt/\ntotal 0\ndr-xr-xr-x 2 root root     1 Nov 25 13:23 cnv\ndr-xr-xr-x 2 root root 55356 Nov 25 13:23 seq\n")),(0,r.kt)("p",null,"The size of the zone files sub-directories indicate the number of files\nexisting for each type of zones. In this example, there is only one\nconventional zone file (all conventional zones are aggregated under a single\nfile)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# ls -l /mnt/cnv\ntotal 137101312\n-rw-r----- 1 root root 140391743488 Nov 25 13:23 0\n")),(0,r.kt)("p",null,"This aggregated conventional zone file can be used as a regular file::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# mkfs.ext4 /mnt/cnv/0\n# mount -o loop /mnt/cnv/0 /data\n")),(0,r.kt)("p",null,'The "seq" sub-directory grouping files for sequential write zones has in this\nexample 55356 zones::'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# ls -lv /mnt/seq\ntotal 14511243264\n-rw-r----- 1 root root 0 Nov 25 13:23 0\n-rw-r----- 1 root root 0 Nov 25 13:23 1\n-rw-r----- 1 root root 0 Nov 25 13:23 2\n...\n-rw-r----- 1 root root 0 Nov 25 13:23 55354\n-rw-r----- 1 root root 0 Nov 25 13:23 55355\n")),(0,r.kt)("p",null,"For sequential write zone files, the file size changes as data is appended at\nthe end of the file, similarly to any regular file system::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# dd if=/dev/zero of=/mnt/seq/0 bs=4096 count=1 conv=notrunc oflag=direct\n1+0 records in\n1+0 records out\n4096 bytes (4.1 kB, 4.0 KiB) copied, 0.00044121 s, 9.3 MB/s\n\n# ls -l /mnt/seq/0\n-rw-r----- 1 root root 4096 Nov 25 13:23 /mnt/seq/0\n")),(0,r.kt)("p",null,"The written file can be truncated to the zone size, preventing any further\nwrite operation::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# truncate -s 268435456 /mnt/seq/0\n# ls -l /mnt/seq/0\n-rw-r----- 1 root root 268435456 Nov 25 13:49 /mnt/seq/0\n")),(0,r.kt)("p",null,"Truncation to 0 size allows freeing the file zone storage space and restart\nappend-writes to the file::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# truncate -s 0 /mnt/seq/0\n# ls -l /mnt/seq/0\n-rw-r----- 1 root root 0 Nov 25 13:49 /mnt/seq/0\n")),(0,r.kt)("p",null,"Since files are statically mapped to zones on the disk, the number of blocks of\na file as reported by stat() and fstat() indicates the size of the file zone::"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# stat /mnt/seq/0\nFile: /mnt/seq/0\nSize: 0             Blocks: 524288     IO Block: 4096   regular empty file\nDevice: 870h/2160d  Inode: 50431       Links: 1\nAccess: (0640/-rw-r-----)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2019-11-25 13:23:57.048971997 +0900\nModify: 2019-11-25 13:52:25.553805765 +0900\nChange: 2019-11-25 13:52:25.553805765 +0900\nBirth: -\n")),(0,r.kt)("p",null,'The number of blocks of the file ("Blocks") in units of 512B blocks gives the\nmaximum file size of 524288 * 512 B = 256 MB, corresponding to the device zone\nsize in this example. Of note is that the "IO block" field always indicates the\nminimum I/O size for writes and corresponds to the device physical sector size.'),(0,r.kt)("h2",{id:"f2fs"},"f2fs"),(0,r.kt)("p",null,"The ",(0,r.kt)("em",{parentName:"p"},"Flash-Friendly File System")," (",(0,r.kt)("em",{parentName:"p"},"f2fs"),") was designed on a basis of a\nlog-structured file system approach but modified to avoid the classical problems\nof the traditional log-structured approach (e.g. The snowball effect of\nwandering trees and the high cleaning overhead)."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"f2fs")," supports various parameters not only for configuring on-disk layout but\nalso for selecting allocation and cleaning algorithms."),(0,r.kt)("h3",{id:"zoned-block-device-support"},"Zoned Block Device Support"),(0,r.kt)("p",null,"Zoned block device support was added to ",(0,r.kt)("em",{parentName:"p"},"f2fs")," with kernel 4.10. Since ",(0,r.kt)("em",{parentName:"p"},"f2fs"),"\nuses a metadata block on-disk format with fixed block location, only zoned block\ndevices which include conventional zones can be supported. Zoned devices composed\nentirely of sequential zones cannot be used with ",(0,r.kt)("em",{parentName:"p"},"f2fs")," as a standalone device\nand require a multi-device setup to place metadata blocks on a randomly\nwritable storage. ",(0,r.kt)("em",{parentName:"p"},"f2fs")," supports multi-device setup where multiple block device\naddress spaces are linearly concatenated to form a logically larger block\ndevice. The ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/dm#dm-linear"},(0,r.kt)("em",{parentName:"a"},"dm-linear"))," device mapper target can also be used\nto create a logical device composed of conventional zones and sequential zones\nsuitable for ",(0,r.kt)("em",{parentName:"p"},"f2fs"),"."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"f2fs")," zoned block device support was achieved using the following principles."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Section Alignment")," In ",(0,r.kt)("em",{parentName:"li"},"f2fs"),", a section is a group of fixed size\nsegments (2 MB). The number of segments in a section is determined to match\nthe zoned device zone size. For instance, with a 256 MB zone size, a section\ncontains 128 segments of 2MB."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Forced LFS mode")," By default, ",(0,r.kt)("em",{parentName:"li"},"f2fs")," tries to optimize block allocation to\navoid excessive append write by allowing some random writes within segments.\nThe LFS mode forces sequential writes to segments and the sequential use of\nsegments within sections, resulting in full compliance with zoned block\ndevices write constraint."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Zone reset as discard operation")," Block ",(0,r.kt)("em",{parentName:"li"},"discard")," (or ",(0,r.kt)("em",{parentName:"li"},"trim"),") used to\nindicate to a device that a block or range of blocks are no longer in use is\nreplaced with execution of a zone write pointer reset command when all blocks\nof all segments of a section are free, allowing the section to be reused.")),(0,r.kt)("p",null,"Compared to a solution using the ",(0,r.kt)("em",{parentName:"p"},"dm-zoned")," device mapper target, performance\nof ",(0,r.kt)("em",{parentName:"p"},"f2fs")," on zoned devices does not suffer from zone reclaim overhead as writes\nare always sequential and do not require on-disk temporary buffering. ",(0,r.kt)("em",{parentName:"p"},"f2fs"),"\ngarbage collection (segment cleanup) will generate overhead only for workloads\nfrequently deleting file or modifying files data."),(0,r.kt)("h3",{id:"zone-capacity-support"},"Zone Capacity Support"),(0,r.kt)("p",null,"NVMe ZNS SSDs can have a per\n",(0,r.kt)("a",{parentName:"p",href:"../introduction/zns#zone-capacity-and-zone-size"},"zone capacity that is smaller than the zone size"),".\nTo support ZNS devices, ",(0,r.kt)("em",{parentName:"p"},"f2fs")," ensures that block allocation and accounting\nonly considers the blocks in a zone that are within the zone capacity. This\nsupport for NVMe ZNS zone capacity is available since Linux kernel version 5.10."),(0,r.kt)("p",null,"Additionally, ",(0,r.kt)("em",{parentName:"p"},"f2fs")," volumes need some storage space that is randomly writable\nto store and update in-place metadata blocks for the volume. Since NVMe zoned\nnamespaces do not have conventional zones, a ",(0,r.kt)("em",{parentName:"p"},"f2fs")," volume cannot be\nself-contained within a single NVMe zoned namespace. To format a ",(0,r.kt)("em",{parentName:"p"},"f2fs")," volume\nusing a NVMe zoned namespace, a multi-device volume format must be used to\nprovide an additional regular block device to store the volume metadata blocks.\nThis additional regular block device can be either a regular namespace on\nthe same NVMe device or a regular namespace on another NVMe device."),(0,r.kt)("h3",{id:"limitations"},"Limitations"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"f2fs")," uses 32-bits block numbers with a block size of 4 KB. This results in a\nmaximum volume size of 16 TB. Any device or combination of devices (for a\nmulti-device volume) with a total capacity larger than 16 TB cannot be used\nwith ",(0,r.kt)("em",{parentName:"p"},"f2fs"),"."),(0,r.kt)("p",null,"To overcome this limit, the ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/dm#dm-linear"},(0,r.kt)("em",{parentName:"a"},"dm-linear"))," device mapper target\ncan be used to partition a zoned block device into serviceable smaller logical\ndevices.  This configuration must ensure that each logical device created is\nassigned a sufficient amount of conventional zones to store ",(0,r.kt)("em",{parentName:"p"},"f2fs")," fixed\nlocation metadata blocks."),(0,r.kt)("h3",{id:"usage-example-with-a-host-managed-smr-hdd"},"Usage Example with a Host Managed SMR HDD"),(0,r.kt)("p",null,"To format a zoned block device with ",(0,r.kt)("em",{parentName:"p"},"mkfs.f2fs"),", the option ",(0,r.kt)("inlineCode",{parentName:"p"},"-m")," must be\nspecified."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},'# mkfs.f2fs -m /dev/sdb\n\n    f2fs-tools: mkfs.f2fs Ver: 1.12.0 (2018-11-12)\n\nInfo: Disable heap-based policy\nInfo: Debug level = 0\nInfo: Trim is enabled\nInfo: [/dev/sdb] Disk Model: HGST HSH721415AL\nInfo: Host-managed zoned block device:\n      55880 zones, 524 randomly writeable zones\n      65536 blocks per zone\nInfo: Segments per section = 128\nInfo: Sections per zone = 1\nInfo: sector size = 4096\nInfo: total sectors = 3662151680 (14305280 MB)\nInfo: zone aligned segment0 blkaddr: 65536\nInfo: format version with\n  "Linux version 5.0.16-300.fc30.x86_64 (mockbuild@bkernel03.phx2.fedoraproject.org) (gcc version 9.1.1 20190503 (Red Hat 9.1.1-1) (GCC)) #1 SMP Tue May 14 19:33:09 UTC 2019"\nInfo: [/dev/sdb] Discarding device\nInfo: Discarded 14305280 MB\nInfo: Overprovision ratio = 0.600%\nInfo: Overprovision segments = 86254 (GC reserved = 43690)\nInfo: format successful\n')),(0,r.kt)("p",null,"The formatted zoned block device can now be directly mounted without any other\nsetup necessary."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# mount /dev/sdb /mnt\n")),(0,r.kt)("h3",{id:"usage-example-with-a-nvme-zns-ssd"},"Usage Example with a NVMe ZNS SSD"),(0,r.kt)("p",null,"Unlike SMR hard-disks, the kernel does not select by default the ",(0,r.kt)("em",{parentName:"p"},"mq-deadline"),"\nblock IO scheduler for block devices representing NVMe zoned namespaces. To\nensure that the regular write operations used by ",(0,r.kt)("em",{parentName:"p"},"f2fs")," are delivered to the\ndevice in sequential order, the IO scheduler for the NVMe zoned namespace block\ndevice must be set to ",(0,r.kt)("em",{parentName:"p"},"mq-deadline"),". This is done with the following command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# echo mq-deadline > /sys/block/nvme1n1/queue/scheduler\n")),(0,r.kt)("p",null,"Where /dev/nvme1n1 is the block device file of the zoned namespace that will be\nused for the ",(0,r.kt)("em",{parentName:"p"},"f2fs")," volume. Using this namespace, a multi-device ",(0,r.kt)("em",{parentName:"p"},"f2fs")," volume\nusing an additional regular block device (",(0,r.kt)("inlineCode",{parentName:"p"},"/dev/nvme0n1")," in the following\nexample) can be formatted using the ",(0,r.kt)("em",{parentName:"p"},"-c")," option of ",(0,r.kt)("em",{parentName:"p"},"mkfs.f2fs"),", as shown in the\nfollowing example."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# mkfs.f2fs -f -m -c /dev/nvme1n1 /dev/nvme0n1\n\n        F2FS-tools: mkfs.f2fs Ver: 1.14.0 (2021-06-23)\n\nInfo: Disable heap-based policy\nInfo: Debug level = 0\nInfo: Trim is enabled\nInfo: Host-managed zoned block device:\n      2048 zones, 0 randomly writeable zones\n      524288 blocks per zone\nInfo: Segments per section = 1024\nInfo: Sections per zone = 1\nInfo: sector size = 4096\nInfo: total sectors = 1107296256 (4325376 MB)\nInfo: zone aligned segment0 blkaddr: 524288\nInfo: format version with\n  \"Linux version 5.13.0-rc6+ (user1@brahmaputra) (gcc (Ubuntu 10.3.0-1ubuntu1) 10.3.0, GNU ld (GNU Binutils for Ubuntu) 2.36.1) #2 SMP Fri Jun 18 16:45:29 IST 2021\"\nInfo: [/dev/nvme0n1] Discarding device\nInfo: This device doesn't support BLKSECDISCARD\nInfo: This device doesn't support BLKDISCARD\nInfo: [/dev/nvme1n1] Discarding device\nInfo: Discarded 4194304 MB\nInfo: Overprovision ratio = 3.090%\nInfo: Overprovision segments = 74918 (GC reserved = 40216)\nInfo: format successful\n")),(0,r.kt)("p",null,"To mount the volume formatted with the above command, the regular block device\nmust be specified."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-plaintext"},"# mount -t f2fs /dev/nvme0n1 /mnt/f2fs/\n")),(0,r.kt)("h2",{id:"btrfs"},"Btrfs"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Btrfs")," is a file system based on the copy-on-write (CoW) principle resulting in\nany block update to never be written in-place. Work is ongoing to add native ZBD\nsupport by changing the block allocation algorithm and block IO issuing code."),(0,r.kt)("h3",{id:"block-allocation-changes"},"Block Allocation Changes"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Btrfs")," block management relies on grouping of blocks into ",(0,r.kt)("em",{parentName:"p"},"block groups"),", with\neach group composed of one or more ",(0,r.kt)("em",{parentName:"p"},"device extent"),". The device extents of a\nblock group may belong to different devices (e.g. In the case of a RAID volume).\nZBD support changes the default device extent size to the size of the device\nzones so that all device extents are always aligned to a zone."),(0,r.kt)("p",null,"Allocation of blocks within a block group is changed so that the allocation is\nalways sequential from the beginning of the block group. To do so, an allocation\npointer is added to block groups and used as the allocation hint. The changes\nalso ensure that block freed below the allocation pointer are ignored, resulting\nin sequential block allocation within each group regardless of the block group\nusage."),(0,r.kt)("h3",{id:"io-management"},"I/O Management"),(0,r.kt)("p",null,"While the introduction of the allocation pointer ensures that blocks are\nallocated sequentially within groups, so sequentially within zones, I/Os to\nwrite out newly allocated blocks may be issued out of order causing errors when\nwriting to sequential zones. This problem is solved by introducing a write I/O\nrequest staging list to each block group. This list is used to delay the\nexecution of unaligned write requests within a block group."),(0,r.kt)("p",null,"The zones of a block group are reset to allow rewriting only when the block\ngroup is being freed, that is, when all the blocks within the block group are\nunused."),(0,r.kt)("p",null,"For ",(0,r.kt)("em",{parentName:"p"},"Btrfs")," volumes composed of multiple disks, restrictions are added to ensure\nthat all disks have the same zone model and in the case of zoned block devices,\nthe same zone size. This matches the existing ",(0,r.kt)("em",{parentName:"p"},"Btrfs")," constraint that all device\nextents in a block group must have the same size."),(0,r.kt)("h3",{id:"upstream-contribution"},"Upstream Contribution"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Btrfs")," zoned block device support is still in development and will be available\nin stable releases after the usual upstream review process completes."),(0,r.kt)("h2",{id:"xfs"},"XFS"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"XFS")," currently does not support zoned block devices. The\n",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/dm#dm-zoned"},(0,r.kt)("em",{parentName:"a"},"dm-zoned"))," device mapper target must be used to enable zoned\ndevice use with ",(0,r.kt)("em",{parentName:"p"},"XFS"),"."),(0,r.kt)("p",null,"An early ",(0,r.kt)("a",{href:"http://xfs.org/images/f/f6/Xfs-smr-structure-0.2.pdf",target:"_blank"}," design document")," discussed the development work necessary to\nsupport host aware and host managed disks with ",(0,r.kt)("em",{parentName:"p"},"XFS"),". Parts of this design have\nalready been implemented and included into the kernel stable releases (e.g. Per\ninode reverse block mapping b-trees feature). However, more work is necessary to\nfully support zoned block devices."),(0,r.kt)("h2",{id:"ext4"},"ext4"),(0,r.kt)("p",null,"describes\nAttempts at improving ",(0,r.kt)("em",{parentName:"p"},"ext4")," performance with host aware zoned block\ndevices using changes to the file system journal management are described in\nin ",(0,r.kt)("a",{href:"https://lwn.net/Articles/720226/",target:"_blank"},"this article"),".\nThe changes are small and succeed in maintaining good performance. However,\nsupport for host managed zoned block devices is not provided as some fundamental\n",(0,r.kt)("em",{parentName:"p"},"ext4")," design aspects cannot be easily changed to match host managed device\nconstraints."),(0,r.kt)("p",null,"These optimizations for host aware zoned block devices is a research work and is\nnot included in ",(0,r.kt)("em",{parentName:"p"},"ext4")," stable kernel releases. ",(0,r.kt)("em",{parentName:"p"},"ext4")," also does not support host\nmanaged disks. Similarly to ",(0,r.kt)("em",{parentName:"p"},"XFS"),", the ",(0,r.kt)("em",{parentName:"p"},"ext4")," file system can however be used\ntogether with the ",(0,r.kt)("a",{parentName:"p",href:"/zonedstorage.io/docs/linux/dm#dm-zoned"},(0,r.kt)("em",{parentName:"a"},"dm-zoned"))," device mapper target."))}u.isMDXComponent=!0}}]);