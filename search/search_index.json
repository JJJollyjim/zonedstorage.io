{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ZONED STORAGE Zoned Storage is a class of storage devices that enables host and storage devices to cooperate to achieve higher storage capacities, increased throughput, and lower latencies. The zoned storage interface is available through the SCSI Zoned Block Commands (ZBC) and Zoned Device ATA Command Set (ZAC) standards for Shingled Magnetic Recording (SMR) hard disks and with the NVMe Zoned Namespaces (ZNS) standard for NVMe Solid State Disks. Learn more about Zoned Storage Devices \u00bb Learn more about Linux\u00ae Software Support \u00bb QUICK START GUIDE Learn how to setup a Linux system supporting zoned block devices and start experimenting with physical and emulated zoned storage devices. Get Started \u00bb Linux Kernel Features System Compliance Tests Linux kernel supports zoned storage devices through various I/O paths with different access characteristics, such as raw device access, file systems and device mapper targets. Learn how to verify a system readiness for zoned storage devices and test hadrware components compliance to standards with automated test suites. View Details \u00bb View Details \u00bb Linux Distributions Applications and Libraries Benchmarking Many Linux distributions today ship with a Linux kernel including zoned block device support and a varying range of additional features. See here a summary of the current support status. Various open source tools, utilities and libraries now include zoned storage support, greatly facilitating system management and application development. Learn how to measure performance with zoned storage compliant workloads using the industry standard Flexible I/O tester (fio) application. View Details \u00bb View Details \u00bb View Details \u00bb","title":"Home"},{"location":"about/","text":"About ZonedStorage.io As a natural extension to Western Digital's ongoing contributions to various open source projects and standard organizations, and its long history of making its ZBC/ZAC tools available to the open-source community, the company has launched ZonedStorage.io as part of its Zoned Storage Initiative . ZonedStorage.io is aimed to document support for zoned storage devices available through various open source projects to jump-start application development initiatives within the developer community as well as help data center infrastructure engineers take advantage of zoned storage technologies. This site hosts information about software development libraries, tools, standards information and software resources that provide a unified framework to manage purpose-built ZNS SSDs and capacity-optimized ZBC/ZAC HDDs. Through this open-source initiative, developers can gain confidence in the burgeoning ecosystem of community and industry contributors and supporters. Contacts Questions, requests for additional content, and error reports can be addressed to Western Digital ZonedStorage.io team . An IRC channel and Matrix groups are also available to chat with the zoned storage community . All questions and remarks including confidential information, information specific to devices that are vendor samples or not publicly available, or specific to a particular zoned storage device product will be ignored. Product specific questions should addressed to the product vendor. About Western Digital Western Digital creates environments for data to thrive. The company is driving the innovation needed to help customers capture, preserve, access and transform an ever-increasing diversity of data. Everywhere data lives, from advanced data centers to mobile sensors to personal devices, our industry-leading solutions deliver the possibilities of data. Western Digital data-centric solutions are marketed under the Western Digital, G-Technology, SanDisk, and WD brands. Please visit WesternDigital.com for more information.","title":"About"},{"location":"community/","text":"Zoned Storage Community For real-time chat, the zoned storage community gathers in Matrix rooms and IRC channel. Matrix: General discussions: #zonedstorage-general:matrix.org ZenFS RocksDB storage backend discussions: #zonedstorage-zenfs:matrix.org Update notifications for associated projects: #zonedstorage-updates:matrix.org IRC: General discussions: #zonedstorage @ OFTC IRC network This channel is linked with the #zonedstorage-general:matrix.org Matrix room.","title":"Community"},{"location":"benchmarking/fio/","text":"Benchmarking Zoned Block Devices The Flexible I/O Tester (fio) was originally written as a test tool for the kernel block I/O stack. Over the years, fio however gained many features and detailed performance statistics output that turned this tool into a standard benchmark application for storage devices. fio source code is available on GitHub . fio Zoned Block Device Support Support for zoned block devices was added to fio with version 3.9. All previous versions do not provide guarantees for write command ordering with host managed zoned block devices. Executing workloads is still possible, but requires writing complex fio scripts. Command Line Options Changes to fio to support zoned block devices include several new options allowing a user to control zoned block device compliant workloads. fio already implemented the option --zonemode which allows defining workloads operating on disjoint ranges of blocks. This option was reused to define the new zbd zone mode. When the zbd zone mode is used by an fio job, the --zonerange option is ignored and the --zonesize option is automatically set to the device zone size. Furthermore, the behavior of read and write commands is modified as follows. Read and write commands are split when a zone boundary is crossed. For sequential writes, the write stream is always started from a zone write pointer position. If the next zone to be written is not empty, the write stream \"jumps\" to that zone write pointer and resumes. For random write workloads, write commands are always issued at the write pointer position of the target zone of the write command. Any write command targeting a sequential zone that is full (entirely written) will trigger a reset of the zone write pointer before issuing the write I/O. By default, all read commands always target written sectors, that is, sectors between the start sector and the write pointer position of sequential write zones. This behavior can be disabled, allowing read commands to be issued to any sector, using the new option read_beyond_wp . Additionally, finer control over the workload operation can be added with the following new options. --max_open_zones This option limits the number of zones that are being written by a workload. With this option, a random write workload cannot issue write commands targeting more zones than the limit set. Once a zone that is being written becomes full, another zone is chosen and writes are allowed to target the zone, resulting in a constant number of zones being written always at most equal to the max_open_zones limit. --zone_reset_threshold and --zone_reset_frequency These two options allow a user to emulate the execution of zone reset commands being issued by an application. In addition to these options, the zbd zone mode automatically enables job synchronization to ensure that a workload spanning multiple threads or processes can concurrently execute write I/Os targeting the same zone. Restrictions As discussed in the kernel support document , using direct write I/O is mandatory for zoned block devices. The zbd zone mode, when enabled, enforces this requirement by checking that the option --direct=1 is specified for any job executing write I/Os. The --offset and --size options must specify values that are aligned to the device zone size. zonemode=zbd Examples This section provides various examples showing how to use fio new zbd zone mode. In all examples, a 15TB ZAC host managed SATA disk is used. The disk zone size is 256 MiB. The disk has 524 conventional zones starting at offset 0. The first sequential write required zone of the disk starts at sector 274726912 (512 B sector unit), that is, at the byte offset 140660178944. Sequential Write Workload The following command sequentially writes the first 4 sequential zones of the disk using the libaio I/O engine with a queue depth of 8. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [W(1)][100.0%][w=239MiB/s][w=955 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4124: Fri May 24 11:49:18 2019 write: IOPS=938, BW=235MiB/s (246MB/s)(1024MiB/4365msec); 0 zone resets slat (nsec): min=5930, max=39068, avg=9729.06, stdev=2048.99 clat (usec): min=783, max=55846, avg=8511.40, stdev=4079.36 lat (usec): min=809, max=55854, avg=8521.19, stdev=4079.36 clat percentiles (usec): | 1.00th=[ 3884], 5.00th=[ 7701], 10.00th=[ 8094], 20.00th=[ 8225], | 30.00th=[ 8225], 40.00th=[ 8225], 50.00th=[ 8225], 60.00th=[ 8291], | 70.00th=[ 8356], 80.00th=[ 8356], 90.00th=[ 8356], 95.00th=[ 8356], | 99.00th=[30540], 99.50th=[45351], 99.90th=[55837], 99.95th=[55837], | 99.99th=[55837] bw ( KiB/s): min=224830, max=249357, per=99.49%, avg=239009.50, stdev=9032.95, samples=8 iops : min= 878, max= 974, avg=933.50, stdev=35.36, samples=8 lat (usec) : 1000=0.02% lat (msec) : 4=2.91%, 10=95.70%, 20=0.20%, 50=0.78%, 100=0.39% cpu : usr=0.73%, sys=0.64%, ctx=1045, majf=0, minf=11 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4096,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): WRITE: bw=235MiB/s (246MB/s), 235MiB/s-235MiB/s (246MB/s-246MB/s), io=1024MiB (1074MB), run=4365-4365msec Disk stats (read/write): sdd: ios=0/984, merge=0/2943, ticks=0/8365, in_queue=7383, util=24.13% The first four sequential write required zones of the disk are now full. # zbc_report_zones /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: ... one 00524: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 275251200 Zone 00525: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275775488 Zone 00526: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 275775488, 524288 sectors, wp 276299776 Zone 00527: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 276299776, 524288 sectors, wp 276824064 Zone 00528: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 276824064, 524288 sectors, wp 276824064 ... With the disk in this state, executing the same command again without the zbd zone mode enabled, fio will attempt to write to full zones, resulting in I/O errors. # fio --name=zbc --filename=/dev/sdd --direct=1 \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process fio: io_u error on file /dev/sdd: Remote I/O error : write offset=140660178944, buflen=262144 fio: pid=4206, err=121/file:io_u.c:1791, func=io_u error, error=Remote I/O error zbc: (groupid=0, jobs=1): err=121 (file:io_u.c:1791, func=io_u error, error=Remote I/O error): pid=4206: Fri May 24 12:34:27 2019 cpu : usr=1.22%, sys=0.00%, ctx=3, majf=0, minf=16 IO depths : 1=12.5%, 2=25.0%, 4=50.0%, 8=12.5%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,8,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=48/3, merge=0/5, ticks=13/35, in_queue=32, util=5.39% With the zbd zone mode enabled, the same command executed again with the zones full succeeds. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [W(1)][100.0%][w=243MiB/s][w=973 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4220: Fri May 24 12:37:29 2019 write: IOPS=949, BW=237MiB/s (249MB/s)(1024MiB/4316msec); 4 zone resets slat (nsec): min=5937, max=40055, avg=9651.92, stdev=2104.34 clat (usec): min=795, max=36562, avg=8243.26, stdev=2031.55 lat (usec): min=818, max=36571, avg=8252.96, stdev=2031.50 clat percentiles (usec): | 1.00th=[ 3884], 5.00th=[ 7701], 10.00th=[ 8160], 20.00th=[ 8225], | 30.00th=[ 8225], 40.00th=[ 8225], 50.00th=[ 8225], 60.00th=[ 8291], | 70.00th=[ 8291], 80.00th=[ 8356], 90.00th=[ 8356], 95.00th=[ 8356], | 99.00th=[10159], 99.50th=[27919], 99.90th=[33817], 99.95th=[36439], | 99.99th=[36439] bw ( KiB/s): min=234538, max=249357, per=99.93%, avg=242777.88, stdev=5032.30, samples=8 iops : min= 916, max= 974, avg=948.25, stdev=19.70, samples=8 lat (usec) : 1000=0.02% lat (msec) : 2=0.07%, 4=2.78%, 10=96.02%, 20=0.51%, 50=0.59% cpu : usr=1.27%, sys=0.67%, ctx=1051, majf=0, minf=12 IO depths : 1=0.1%, 2=0.2%, 4=0.4%, 8=99.3%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4096,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): WRITE: bw=237MiB/s (249MB/s), 237MiB/s-237MiB/s (249MB/s-249MB/s), io=1024MiB (1074MB), run=4316-4316msec Disk stats (read/write): sdd: ios=4/998, merge=0/2985, ticks=81/8205, in_queue=7286, util=24.38% Note that fio output in this case indicates the number of zones that were reset prior to writing. Sequential Read Workload With the disk previous state preserved (with the first four sequential write zones full), the previous command can be changed to read operations targeting the written zones. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=243MiB/s][r=970 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4236: Fri May 24 12:40:18 2019 read: IOPS=951, BW=238MiB/s (249MB/s)(1024MiB/4304msec) slat (usec): min=5, max=148, avg= 6.62, stdev= 5.68 clat (usec): min=976, max=39536, avg=8394.44, stdev=1933.57 lat (usec): min=1125, max=39543, avg=8401.12, stdev=1934.43 clat percentiles (usec): | 1.00th=[ 6390], 5.00th=[ 6718], 10.00th=[ 6915], 20.00th=[ 7439], | 30.00th=[ 8094], 40.00th=[ 8291], 50.00th=[ 8356], 60.00th=[ 8356], | 70.00th=[ 8356], 80.00th=[ 8356], 90.00th=[ 9765], 95.00th=[10290], | 99.00th=[14615], 99.50th=[25560], 99.90th=[29492], 99.95th=[39584], | 99.99th=[39584] bw ( KiB/s): min=223808, max=249868, per=99.57%, avg=242586.38, stdev=8265.29, samples=8 iops : min= 874, max= 976, avg=947.50, stdev=32.35, samples=8 lat (usec) : 1000=0.02% lat (msec) : 2=0.05%, 4=0.05%, 10=92.53%, 20=6.81%, 50=0.54% cpu : usr=0.40%, sys=0.72%, ctx=4113, majf=0, minf=522 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=4096,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): READ: bw=238MiB/s (249MB/s), 238MiB/s-238MiB/s (249MB/s-249MB/s), io=1024MiB (1074MB), run=4304-4304msec Disk stats (read/write): sdd: ios=4031/0, merge=0/0, ticks=33836/0, in_queue=29809, util=57.74% If the zones are reset before executing this command, no read I/O will be executed as fio will be enable to find zones with written sectors. # blkzone reset -o 274726912 -l 2097152 /dev/sdd # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00% Forcing the execution of read I/Os targeting empty zones can be done using the --read_beyond_wp option. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G --read_beyond_wp=1 \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [R(1)][-.-%][r=353MiB/s][r=1412 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4322: Fri May 24 12:46:32 2019 read: IOPS=1411, BW=353MiB/s (370MB/s)(1024MiB/2901msec) slat (usec): min=5, max=147, avg= 6.50, stdev= 5.72 clat (usec): min=1978, max=8845, avg=5654.86, stdev=112.79 lat (usec): min=2126, max=8851, avg=5661.41, stdev=111.86 clat percentiles (usec): | 1.00th=[ 5604], 5.00th=[ 5604], 10.00th=[ 5604], 20.00th=[ 5669], | 30.00th=[ 5669], 40.00th=[ 5669], 50.00th=[ 5669], 60.00th=[ 5669], | 70.00th=[ 5669], 80.00th=[ 5669], 90.00th=[ 5669], 95.00th=[ 5735], | 99.00th=[ 5735], 99.50th=[ 5800], 99.90th=[ 7177], 99.95th=[ 7963], | 99.99th=[ 8848] bw ( KiB/s): min=360239, max=361261, per=99.81%, avg=360750.00, stdev=361.33, samples=5 iops : min= 1407, max= 1411, avg=1409.00, stdev= 1.41, samples=5 lat (msec) : 2=0.02%, 10=99.98% cpu : usr=0.24%, sys=1.34%, ctx=4108, majf=0, minf=522 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=4096,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): READ: bw=353MiB/s (370MB/s), 353MiB/s-353MiB/s (370MB/s-370MB/s), io=1024MiB (1074MB), run=2901-2901msec Disk stats (read/write): sdd: ios=3869/0, merge=0/0, ticks=21868/0, in_queue=18002, util=81.20% Note The higher IOPS performance observed with this test compared to the previous one (i.e. IOPS=1411 vs. IOPS=951) results from the disk not physically executing any media access as there is no data to read (no written sectors). The disks returns a fill pattern as data without seeking to the sectors specified by the read commands. Random Read and Write Workloads The following command randomly write sequential write zones of the disk using 4 jobs, each job operating at a queue depth of 4 (overall queue depth of 16 for the disk). The run time is set to 30 seconds. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randwrite --bs=256K zbc: (g=0): rw=randwrite, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=4 ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [w(4)][100.0%][w=33.0MiB/s][w=132 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=4): err= 0: pid=4425: Fri May 24 12:58:43 2019 write: IOPS=160, BW=40.2MiB/s (42.2MB/s)(1209MiB/30064msec); 0 zone resets slat (nsec): min=7815, max=75710, avg=12304.05, stdev=3091.33 clat (usec): min=1410, max=221285, avg=98856.71, stdev=29971.24 lat (usec): min=1435, max=221295, avg=98869.07, stdev=29970.46 clat percentiles (msec): | 1.00th=[ 12], 5.00th=[ 24], 10.00th=[ 75], 20.00th=[ 84], | 30.00th=[ 89], 40.00th=[ 94], 50.00th=[ 99], 60.00th=[ 105], | 70.00th=[ 110], 80.00th=[ 117], 90.00th=[ 128], 95.00th=[ 138], | 99.00th=[ 194], 99.50th=[ 203], 99.90th=[ 218], 99.95th=[ 220], | 99.99th=[ 222] bw ( KiB/s): min=27092, max=138608, per=99.80%, avg=41096.13, stdev=3423.45, samples=240 iops : min= 103, max= 540, avg=159.05, stdev=13.38, samples=240 lat (msec) : 2=0.02%, 4=0.06%, 10=0.43%, 20=3.74%, 50=1.26% lat (msec) : 100=47.27%, 250=47.21% cpu : usr=0.09%, sys=0.15%, ctx=84537, majf=0, minf=261 IO depths : 1=0.1%, 2=0.2%, 4=99.8%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4836,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): WRITE: bw=40.2MiB/s (42.2MB/s), 40.2MiB/s-40.2MiB/s (42.2MB/s-42.2MB/s), io=1209MiB (1268MB), run=30064-30064msec Disk stats (read/write): sdd: ios=0/4807, merge=0/0, ticks=0/474905, in_queue=470163, util=9.89% zbc_report_zones can be used to explore the state of the disk at the end of this workload execution. # zbc_report_zones -ro full -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 0 zone from 0, reporting option 0x05 # zbc_report_zones -ro closed -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 4498 zones from 0, reporting option 0x04 # zbc_report_zones -ro imp_open -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 128 zones from 0, reporting option 0x02 This indicates that 4498+128=4626 zones were written to, with none of the sequential write zones fully written (no full zone). Switching the operation mode to read, the sectors written in this last run can be randomly read. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randread --bs=256K ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [r(4)][0.0%][r=31.0MiB/s][r=124 IOPS][eta 23d:02h:02m:24s] zbc: (groupid=0, jobs=4): err= 0: pid=4494: Fri May 24 13:24:08 2019 read: IOPS=118, BW=29.7MiB/s (31.1MB/s)(894MiB/30156msec) slat (usec): min=6, max=183, avg= 7.86, stdev= 8.75 clat (usec): min=1252, max=1537.8k, avg=133931.63, stdev=123584.49 lat (usec): min=1260, max=1537.8k, avg=133939.56, stdev=123584.42 clat percentiles (msec): | 1.00th=[ 8], 5.00th=[ 16], 10.00th=[ 20], 20.00th=[ 35], | 30.00th=[ 53], 40.00th=[ 72], 50.00th=[ 96], 60.00th=[ 127], | 70.00th=[ 169], 80.00th=[ 218], 90.00th=[ 296], 95.00th=[ 368], | 99.00th=[ 558], 99.50th=[ 625], 99.90th=[ 869], 99.95th=[ 919], | 99.99th=[ 1536] bw ( KiB/s): min=15855, max=50152, per=100.00%, avg=30364.05, stdev=1762.17, samples=240 iops : min= 60, max= 195, avg=117.22, stdev= 6.92, samples=240 lat (msec) : 2=0.22%, 4=0.06%, 10=1.40%, 20=8.44%, 50=18.26% lat (msec) : 100=23.15%, 250=33.30%, 500=13.47%, 750=1.51%, 1000=0.17% cpu : usr=0.06%, sys=0.12%, ctx=82815, majf=0, minf=1281 IO depths : 1=0.1%, 2=0.2%, 4=99.7%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=3577,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): READ: bw=29.7MiB/s (31.1MB/s), 29.7MiB/s-29.7MiB/s (31.1MB/s-31.1MB/s), io=894MiB (938MB), run=30156-30156msec Disk stats (read/write): sdd: ios=3565/0, merge=0/0, ticks=475774/0, in_queue=472274, util=11.93% Resetting all sequential write zones of the disk and executing again the random read workload leads to similar results as for the previous sequential read workload case, that is, no read I/O is executed. # blkzone reset -o 274726912 /dev/sdd # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randread --bs=256K ... fio-3.13 Starting 4 processes Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00% Changing the range to be read to include the conventional zones of the disk will result in read I/Os being executed. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=0 --size=140660178944 --numjobs=4 --group_reporting=1 \\ --runtime=30 --ioengine=libaio --iodepth=4 --rw=randread --bs=256K zbc: (g=0): rw=randread, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=4 ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=58.0MiB/s][r=232 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=4): err= 0: pid=4570: Fri May 24 13:35:15 2019 read: IOPS=215, BW=53.8MiB/s (56.4MB/s)(1617MiB/30079msec) slat (usec): min=6, max=143, avg= 7.29, stdev= 6.22 clat (usec): min=1065, max=959229, avg=74313.80, stdev=83291.27 lat (usec): min=1072, max=959237, avg=74321.15, stdev=83291.39 clat percentiles (msec): | 1.00th=[ 8], 5.00th=[ 9], 10.00th=[ 12], 20.00th=[ 17], | 30.00th=[ 24], 40.00th=[ 34], 50.00th=[ 47], 60.00th=[ 62], | 70.00th=[ 82], 80.00th=[ 113], 90.00th=[ 174], 95.00th=[ 239], | 99.00th=[ 409], 99.50th=[ 472], 99.90th=[ 634], 99.95th=[ 651], | 99.99th=[ 961] bw ( KiB/s): min=27136, max=81920, per=99.97%, avg=55030.83, stdev=2685.56, samples=240 iops : min= 106, max= 320, avg=214.10, stdev=10.53, samples=240 lat (msec) : 2=0.02%, 10=8.23%, 20=16.22%, 50=28.40%, 100=23.61% lat (msec) : 250=19.08%, 500=4.07%, 750=0.36%, 1000=0.03% cpu : usr=0.02%, sys=0.07%, ctx=6483, majf=0, minf=1070 IO depths : 1=0.1%, 2=0.1%, 4=99.8%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=6468,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): READ: bw=53.8MiB/s (56.4MB/s), 53.8MiB/s-53.8MiB/s (56.4MB/s-56.4MB/s), io=1617MiB (1696MB), run=30079-30079msec Disk stats (read/write): sdd: ios=6452/0, merge=0/0, ticks=479006/0, in_queue=472619, util=21.62% Direct Access sg I/O Engine The SCSI generic direct access interface can also be used with the zbd zone mode, as long as the block device file ( /dev/sdX ) is used to specify the disk. The zbd zone mode will not be enabled if the SCSI generic node file ( /dev/sgY ) is used to specify the disk. The example below illustrates the use of the sg I/O engine with 8 jobs executing a 64KB random write workload to sequential write zones. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G --numjobs=8 --group_reporting=1 \\ --ioengine=sg --rw=randwrite --bs=64K ... fio-3.13 Starting 8 processes zbc: (groupid=0, jobs=8): err= 0: pid=4792: Fri May 24 14:18:52 2019 write: IOPS=2007, BW=125MiB/s (132MB/s)(8192MiB/65278msec); 32 zone resets clat (usec): min=148, max=327494, avg=1589.26, stdev=4060.13 lat (usec): min=149, max=327497, avg=1590.55, stdev=4060.13 clat percentiles (usec): | 1.00th=[ 848], 5.00th=[ 988], 10.00th=[ 1090], 20.00th=[ 1172], | 30.00th=[ 1221], 40.00th=[ 1287], 50.00th=[ 1369], 60.00th=[ 1434], | 70.00th=[ 1500], 80.00th=[ 1631], 90.00th=[ 2180], 95.00th=[ 2638], | 99.00th=[ 3064], 99.50th=[ 3392], 99.90th=[ 24773], 99.95th=[ 77071], | 99.99th=[291505] bw ( KiB/s): min=39001, max=178678, per=100.00%, avg=131457.71, stdev=3143.56, samples=1015 iops : min= 606, max= 2791, avg=2052.81, stdev=49.16, samples=1015 lat (usec) : 250=0.07%, 500=0.12%, 750=0.38%, 1000=4.89% lat (msec) : 2=81.56%, 4=12.76%, 10=0.05%, 20=0.06%, 50=0.05% lat (msec) : 100=0.03%, 250=0.02%, 500=0.01% cpu : usr=0.14%, sys=0.20%, ctx=251460, majf=0, minf=139 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,131072,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=125MiB/s (132MB/s), 125MiB/s-125MiB/s (132MB/s-132MB/s), io=8192MiB (8590MB), run=65278-65278msec Disk stats (read/write): sdd: ios=278/0, merge=0/0, ticks=1953/0, in_queue=1677, util=0.39% Note SCSI generic direct access bypasses the block layer I/O scheduler. For zoned block devices, this means that the deadline I/O scheduler zone write locking is enable to provide write command ordering guarantees. However, the zbd mode ensures mutual exclusion between jobs for write access to the same zone. SUch synchronization is in essence identical to zone write locking and execute all write commands without any error. Zone Write Streams A typical zoned block device compliant application will write zones sequentially until the zone is full, then switch to another zone and continue writing. Multiple threads may be operating in this manner, with each thread operating on a different zone. Such typical behavior can be emulated using the option --rw_sequencer together with a number of I/O operations specified at the end of the --rw=randwrite argument. Below is an example script of 4 jobs sequentially writing zones up to full using 512KB write operations (that is, 512 I/Os per 256 MB zone). The zones being written are chosen randomly within disjoint zone ranges for each job. This is controlled with the offset , size and rw arguments. The script file streams.fio achieving such workload is shown below. # # streams.fio: 4 write streams # [global] ioengine=psync direct=1 thread=1 bs=512K continue_on_error=none filename=/dev/sdd group_reporting=1 zonemode=zbd [stream1] offset=140660178944 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream2] offset=3855538454528 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream3] offset=7570416730112 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream4] offset=11285295005696 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G The result for this script execution is shown below. # fio streams.fio stream1: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream2: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream3: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream4: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 fio-3.13 Starting 4 threads Jobs: 1 (f=1): [_(3),w(1)][98.3%][w=159MiB/s][w=318 IOPS][eta 00m:01s] stream1: (groupid=0, jobs=4): err= 0: pid=5161: Fri May 24 15:01:21 2019 write: IOPS=285, BW=143MiB/s (150MB/s)(8192MiB/57362msec); 0 zone resets clat (usec): min=992, max=5788.2k, avg=12731.27, stdev=95315.03 lat (usec): min=996, max=5788.2k, avg=12742.01, stdev=95315.03 clat percentiles (usec): | 1.00th=[ 1106], 5.00th=[ 2024], 10.00th=[ 2114], | 20.00th=[ 2278], 30.00th=[ 2769], 40.00th=[ 3687], | 50.00th=[ 3884], 60.00th=[ 4047], 70.00th=[ 4228], | 80.00th=[ 4817], 90.00th=[ 5342], 95.00th=[ 50594], | 99.00th=[ 196084], 99.50th=[ 258999], 99.90th=[ 876610], | 99.95th=[2071987], 99.99th=[4731175] bw ( KiB/s): min= 9211, max=840925, per=100.00%, avg=185793.45, stdev=44491.80, samples=352 iops : min= 17, max= 1642, avg=361.42, stdev=86.97, samples=352 lat (usec) : 1000=0.23% lat (msec) : 2=1.95%, 4=54.69%, 10=37.05%, 20=0.15%, 50=0.90% lat (msec) : 100=2.34%, 250=2.16%, 500=0.42%, 750=0.01%, 1000=0.01% lat (msec) : 2000=0.04%, >=2000=0.06% cpu : usr=0.15%, sys=0.12%, ctx=16505, majf=0, minf=0 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,16384,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=143MiB/s (150MB/s), 143MiB/s-143MiB/s (150MB/s-150MB/s), io=8192MiB (8590MB), run=57362-57362msec Disk stats (read/write): sdd: ios=144/16333, merge=0/0, ticks=573/208225, in_queue=192356, util=28.64%","title":"Benchmarking"},{"location":"benchmarking/fio/#benchmarking-zoned-block-devices","text":"The Flexible I/O Tester (fio) was originally written as a test tool for the kernel block I/O stack. Over the years, fio however gained many features and detailed performance statistics output that turned this tool into a standard benchmark application for storage devices. fio source code is available on GitHub .","title":"Benchmarking Zoned Block Devices"},{"location":"benchmarking/fio/#fio-zoned-block-device-support","text":"Support for zoned block devices was added to fio with version 3.9. All previous versions do not provide guarantees for write command ordering with host managed zoned block devices. Executing workloads is still possible, but requires writing complex fio scripts.","title":"fio Zoned Block Device Support"},{"location":"benchmarking/fio/#command-line-options","text":"Changes to fio to support zoned block devices include several new options allowing a user to control zoned block device compliant workloads. fio already implemented the option --zonemode which allows defining workloads operating on disjoint ranges of blocks. This option was reused to define the new zbd zone mode. When the zbd zone mode is used by an fio job, the --zonerange option is ignored and the --zonesize option is automatically set to the device zone size. Furthermore, the behavior of read and write commands is modified as follows. Read and write commands are split when a zone boundary is crossed. For sequential writes, the write stream is always started from a zone write pointer position. If the next zone to be written is not empty, the write stream \"jumps\" to that zone write pointer and resumes. For random write workloads, write commands are always issued at the write pointer position of the target zone of the write command. Any write command targeting a sequential zone that is full (entirely written) will trigger a reset of the zone write pointer before issuing the write I/O. By default, all read commands always target written sectors, that is, sectors between the start sector and the write pointer position of sequential write zones. This behavior can be disabled, allowing read commands to be issued to any sector, using the new option read_beyond_wp . Additionally, finer control over the workload operation can be added with the following new options. --max_open_zones This option limits the number of zones that are being written by a workload. With this option, a random write workload cannot issue write commands targeting more zones than the limit set. Once a zone that is being written becomes full, another zone is chosen and writes are allowed to target the zone, resulting in a constant number of zones being written always at most equal to the max_open_zones limit. --zone_reset_threshold and --zone_reset_frequency These two options allow a user to emulate the execution of zone reset commands being issued by an application. In addition to these options, the zbd zone mode automatically enables job synchronization to ensure that a workload spanning multiple threads or processes can concurrently execute write I/Os targeting the same zone.","title":"Command Line Options"},{"location":"benchmarking/fio/#restrictions","text":"As discussed in the kernel support document , using direct write I/O is mandatory for zoned block devices. The zbd zone mode, when enabled, enforces this requirement by checking that the option --direct=1 is specified for any job executing write I/Os. The --offset and --size options must specify values that are aligned to the device zone size.","title":"Restrictions"},{"location":"benchmarking/fio/#zonemodezbd-examples","text":"This section provides various examples showing how to use fio new zbd zone mode. In all examples, a 15TB ZAC host managed SATA disk is used. The disk zone size is 256 MiB. The disk has 524 conventional zones starting at offset 0. The first sequential write required zone of the disk starts at sector 274726912 (512 B sector unit), that is, at the byte offset 140660178944.","title":"zonemode=zbd Examples"},{"location":"benchmarking/fio/#sequential-write-workload","text":"The following command sequentially writes the first 4 sequential zones of the disk using the libaio I/O engine with a queue depth of 8. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [W(1)][100.0%][w=239MiB/s][w=955 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4124: Fri May 24 11:49:18 2019 write: IOPS=938, BW=235MiB/s (246MB/s)(1024MiB/4365msec); 0 zone resets slat (nsec): min=5930, max=39068, avg=9729.06, stdev=2048.99 clat (usec): min=783, max=55846, avg=8511.40, stdev=4079.36 lat (usec): min=809, max=55854, avg=8521.19, stdev=4079.36 clat percentiles (usec): | 1.00th=[ 3884], 5.00th=[ 7701], 10.00th=[ 8094], 20.00th=[ 8225], | 30.00th=[ 8225], 40.00th=[ 8225], 50.00th=[ 8225], 60.00th=[ 8291], | 70.00th=[ 8356], 80.00th=[ 8356], 90.00th=[ 8356], 95.00th=[ 8356], | 99.00th=[30540], 99.50th=[45351], 99.90th=[55837], 99.95th=[55837], | 99.99th=[55837] bw ( KiB/s): min=224830, max=249357, per=99.49%, avg=239009.50, stdev=9032.95, samples=8 iops : min= 878, max= 974, avg=933.50, stdev=35.36, samples=8 lat (usec) : 1000=0.02% lat (msec) : 4=2.91%, 10=95.70%, 20=0.20%, 50=0.78%, 100=0.39% cpu : usr=0.73%, sys=0.64%, ctx=1045, majf=0, minf=11 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4096,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): WRITE: bw=235MiB/s (246MB/s), 235MiB/s-235MiB/s (246MB/s-246MB/s), io=1024MiB (1074MB), run=4365-4365msec Disk stats (read/write): sdd: ios=0/984, merge=0/2943, ticks=0/8365, in_queue=7383, util=24.13% The first four sequential write required zones of the disk are now full. # zbc_report_zones /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: ... one 00524: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 275251200 Zone 00525: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275775488 Zone 00526: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 275775488, 524288 sectors, wp 276299776 Zone 00527: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 276299776, 524288 sectors, wp 276824064 Zone 00528: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 276824064, 524288 sectors, wp 276824064 ... With the disk in this state, executing the same command again without the zbd zone mode enabled, fio will attempt to write to full zones, resulting in I/O errors. # fio --name=zbc --filename=/dev/sdd --direct=1 \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process fio: io_u error on file /dev/sdd: Remote I/O error : write offset=140660178944, buflen=262144 fio: pid=4206, err=121/file:io_u.c:1791, func=io_u error, error=Remote I/O error zbc: (groupid=0, jobs=1): err=121 (file:io_u.c:1791, func=io_u error, error=Remote I/O error): pid=4206: Fri May 24 12:34:27 2019 cpu : usr=1.22%, sys=0.00%, ctx=3, majf=0, minf=16 IO depths : 1=12.5%, 2=25.0%, 4=50.0%, 8=12.5%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,8,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=48/3, merge=0/5, ticks=13/35, in_queue=32, util=5.39% With the zbd zone mode enabled, the same command executed again with the zones full succeeds. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=write --bs=256K zbc: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [W(1)][100.0%][w=243MiB/s][w=973 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4220: Fri May 24 12:37:29 2019 write: IOPS=949, BW=237MiB/s (249MB/s)(1024MiB/4316msec); 4 zone resets slat (nsec): min=5937, max=40055, avg=9651.92, stdev=2104.34 clat (usec): min=795, max=36562, avg=8243.26, stdev=2031.55 lat (usec): min=818, max=36571, avg=8252.96, stdev=2031.50 clat percentiles (usec): | 1.00th=[ 3884], 5.00th=[ 7701], 10.00th=[ 8160], 20.00th=[ 8225], | 30.00th=[ 8225], 40.00th=[ 8225], 50.00th=[ 8225], 60.00th=[ 8291], | 70.00th=[ 8291], 80.00th=[ 8356], 90.00th=[ 8356], 95.00th=[ 8356], | 99.00th=[10159], 99.50th=[27919], 99.90th=[33817], 99.95th=[36439], | 99.99th=[36439] bw ( KiB/s): min=234538, max=249357, per=99.93%, avg=242777.88, stdev=5032.30, samples=8 iops : min= 916, max= 974, avg=948.25, stdev=19.70, samples=8 lat (usec) : 1000=0.02% lat (msec) : 2=0.07%, 4=2.78%, 10=96.02%, 20=0.51%, 50=0.59% cpu : usr=1.27%, sys=0.67%, ctx=1051, majf=0, minf=12 IO depths : 1=0.1%, 2=0.2%, 4=0.4%, 8=99.3%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4096,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): WRITE: bw=237MiB/s (249MB/s), 237MiB/s-237MiB/s (249MB/s-249MB/s), io=1024MiB (1074MB), run=4316-4316msec Disk stats (read/write): sdd: ios=4/998, merge=0/2985, ticks=81/8205, in_queue=7286, util=24.38% Note that fio output in this case indicates the number of zones that were reset prior to writing.","title":"Sequential Write Workload"},{"location":"benchmarking/fio/#sequential-read-workload","text":"With the disk previous state preserved (with the first four sequential write zones full), the previous command can be changed to read operations targeting the written zones. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=243MiB/s][r=970 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4236: Fri May 24 12:40:18 2019 read: IOPS=951, BW=238MiB/s (249MB/s)(1024MiB/4304msec) slat (usec): min=5, max=148, avg= 6.62, stdev= 5.68 clat (usec): min=976, max=39536, avg=8394.44, stdev=1933.57 lat (usec): min=1125, max=39543, avg=8401.12, stdev=1934.43 clat percentiles (usec): | 1.00th=[ 6390], 5.00th=[ 6718], 10.00th=[ 6915], 20.00th=[ 7439], | 30.00th=[ 8094], 40.00th=[ 8291], 50.00th=[ 8356], 60.00th=[ 8356], | 70.00th=[ 8356], 80.00th=[ 8356], 90.00th=[ 9765], 95.00th=[10290], | 99.00th=[14615], 99.50th=[25560], 99.90th=[29492], 99.95th=[39584], | 99.99th=[39584] bw ( KiB/s): min=223808, max=249868, per=99.57%, avg=242586.38, stdev=8265.29, samples=8 iops : min= 874, max= 976, avg=947.50, stdev=32.35, samples=8 lat (usec) : 1000=0.02% lat (msec) : 2=0.05%, 4=0.05%, 10=92.53%, 20=6.81%, 50=0.54% cpu : usr=0.40%, sys=0.72%, ctx=4113, majf=0, minf=522 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=4096,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): READ: bw=238MiB/s (249MB/s), 238MiB/s-238MiB/s (249MB/s-249MB/s), io=1024MiB (1074MB), run=4304-4304msec Disk stats (read/write): sdd: ios=4031/0, merge=0/0, ticks=33836/0, in_queue=29809, util=57.74% If the zones are reset before executing this command, no read I/O will be executed as fio will be enable to find zones with written sectors. # blkzone reset -o 274726912 -l 2097152 /dev/sdd # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00% Forcing the execution of read I/Os targeting empty zones can be done using the --read_beyond_wp option. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G --read_beyond_wp=1 \\ --ioengine=libaio --iodepth=8 --rw=read --bs=256K zbc: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=8 fio-3.13 Starting 1 process Jobs: 1 (f=1): [R(1)][-.-%][r=353MiB/s][r=1412 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=1): err= 0: pid=4322: Fri May 24 12:46:32 2019 read: IOPS=1411, BW=353MiB/s (370MB/s)(1024MiB/2901msec) slat (usec): min=5, max=147, avg= 6.50, stdev= 5.72 clat (usec): min=1978, max=8845, avg=5654.86, stdev=112.79 lat (usec): min=2126, max=8851, avg=5661.41, stdev=111.86 clat percentiles (usec): | 1.00th=[ 5604], 5.00th=[ 5604], 10.00th=[ 5604], 20.00th=[ 5669], | 30.00th=[ 5669], 40.00th=[ 5669], 50.00th=[ 5669], 60.00th=[ 5669], | 70.00th=[ 5669], 80.00th=[ 5669], 90.00th=[ 5669], 95.00th=[ 5735], | 99.00th=[ 5735], 99.50th=[ 5800], 99.90th=[ 7177], 99.95th=[ 7963], | 99.99th=[ 8848] bw ( KiB/s): min=360239, max=361261, per=99.81%, avg=360750.00, stdev=361.33, samples=5 iops : min= 1407, max= 1411, avg=1409.00, stdev= 1.41, samples=5 lat (msec) : 2=0.02%, 10=99.98% cpu : usr=0.24%, sys=1.34%, ctx=4108, majf=0, minf=522 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=99.8%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.1%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=4096,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=8 Run status group 0 (all jobs): READ: bw=353MiB/s (370MB/s), 353MiB/s-353MiB/s (370MB/s-370MB/s), io=1024MiB (1074MB), run=2901-2901msec Disk stats (read/write): sdd: ios=3869/0, merge=0/0, ticks=21868/0, in_queue=18002, util=81.20% Note The higher IOPS performance observed with this test compared to the previous one (i.e. IOPS=1411 vs. IOPS=951) results from the disk not physically executing any media access as there is no data to read (no written sectors). The disks returns a fill pattern as data without seeking to the sectors specified by the read commands.","title":"Sequential Read Workload"},{"location":"benchmarking/fio/#random-read-and-write-workloads","text":"The following command randomly write sequential write zones of the disk using 4 jobs, each job operating at a queue depth of 4 (overall queue depth of 16 for the disk). The run time is set to 30 seconds. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randwrite --bs=256K zbc: (g=0): rw=randwrite, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=4 ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [w(4)][100.0%][w=33.0MiB/s][w=132 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=4): err= 0: pid=4425: Fri May 24 12:58:43 2019 write: IOPS=160, BW=40.2MiB/s (42.2MB/s)(1209MiB/30064msec); 0 zone resets slat (nsec): min=7815, max=75710, avg=12304.05, stdev=3091.33 clat (usec): min=1410, max=221285, avg=98856.71, stdev=29971.24 lat (usec): min=1435, max=221295, avg=98869.07, stdev=29970.46 clat percentiles (msec): | 1.00th=[ 12], 5.00th=[ 24], 10.00th=[ 75], 20.00th=[ 84], | 30.00th=[ 89], 40.00th=[ 94], 50.00th=[ 99], 60.00th=[ 105], | 70.00th=[ 110], 80.00th=[ 117], 90.00th=[ 128], 95.00th=[ 138], | 99.00th=[ 194], 99.50th=[ 203], 99.90th=[ 218], 99.95th=[ 220], | 99.99th=[ 222] bw ( KiB/s): min=27092, max=138608, per=99.80%, avg=41096.13, stdev=3423.45, samples=240 iops : min= 103, max= 540, avg=159.05, stdev=13.38, samples=240 lat (msec) : 2=0.02%, 4=0.06%, 10=0.43%, 20=3.74%, 50=1.26% lat (msec) : 100=47.27%, 250=47.21% cpu : usr=0.09%, sys=0.15%, ctx=84537, majf=0, minf=261 IO depths : 1=0.1%, 2=0.2%, 4=99.8%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,4836,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): WRITE: bw=40.2MiB/s (42.2MB/s), 40.2MiB/s-40.2MiB/s (42.2MB/s-42.2MB/s), io=1209MiB (1268MB), run=30064-30064msec Disk stats (read/write): sdd: ios=0/4807, merge=0/0, ticks=0/474905, in_queue=470163, util=9.89% zbc_report_zones can be used to explore the state of the disk at the end of this workload execution. # zbc_report_zones -ro full -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 0 zone from 0, reporting option 0x05 # zbc_report_zones -ro closed -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 4498 zones from 0, reporting option 0x04 # zbc_report_zones -ro imp_open -n /dev/sdd Device /dev/sdd: Vendor ID: ATA xxxx xxxxxxxxxxx xxxx Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 128 zones from 0, reporting option 0x02 This indicates that 4498+128=4626 zones were written to, with none of the sequential write zones fully written (no full zone). Switching the operation mode to read, the sectors written in this last run can be randomly read. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randread --bs=256K ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [r(4)][0.0%][r=31.0MiB/s][r=124 IOPS][eta 23d:02h:02m:24s] zbc: (groupid=0, jobs=4): err= 0: pid=4494: Fri May 24 13:24:08 2019 read: IOPS=118, BW=29.7MiB/s (31.1MB/s)(894MiB/30156msec) slat (usec): min=6, max=183, avg= 7.86, stdev= 8.75 clat (usec): min=1252, max=1537.8k, avg=133931.63, stdev=123584.49 lat (usec): min=1260, max=1537.8k, avg=133939.56, stdev=123584.42 clat percentiles (msec): | 1.00th=[ 8], 5.00th=[ 16], 10.00th=[ 20], 20.00th=[ 35], | 30.00th=[ 53], 40.00th=[ 72], 50.00th=[ 96], 60.00th=[ 127], | 70.00th=[ 169], 80.00th=[ 218], 90.00th=[ 296], 95.00th=[ 368], | 99.00th=[ 558], 99.50th=[ 625], 99.90th=[ 869], 99.95th=[ 919], | 99.99th=[ 1536] bw ( KiB/s): min=15855, max=50152, per=100.00%, avg=30364.05, stdev=1762.17, samples=240 iops : min= 60, max= 195, avg=117.22, stdev= 6.92, samples=240 lat (msec) : 2=0.22%, 4=0.06%, 10=1.40%, 20=8.44%, 50=18.26% lat (msec) : 100=23.15%, 250=33.30%, 500=13.47%, 750=1.51%, 1000=0.17% cpu : usr=0.06%, sys=0.12%, ctx=82815, majf=0, minf=1281 IO depths : 1=0.1%, 2=0.2%, 4=99.7%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=3577,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): READ: bw=29.7MiB/s (31.1MB/s), 29.7MiB/s-29.7MiB/s (31.1MB/s-31.1MB/s), io=894MiB (938MB), run=30156-30156msec Disk stats (read/write): sdd: ios=3565/0, merge=0/0, ticks=475774/0, in_queue=472274, util=11.93% Resetting all sequential write zones of the disk and executing again the random read workload leads to similar results as for the previous sequential read workload case, that is, no read I/O is executed. # blkzone reset -o 274726912 /dev/sdd # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --numjobs=4 --group_reporting=1 --runtime=30 \\ --ioengine=libaio --iodepth=4 --rw=randread --bs=256K ... fio-3.13 Starting 4 processes Run status group 0 (all jobs): Disk stats (read/write): sdd: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00% Changing the range to be read to include the conventional zones of the disk will result in read I/Os being executed. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=0 --size=140660178944 --numjobs=4 --group_reporting=1 \\ --runtime=30 --ioengine=libaio --iodepth=4 --rw=randread --bs=256K zbc: (g=0): rw=randread, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=libaio, iodepth=4 ... fio-3.13 Starting 4 processes Jobs: 4 (f=4): [r(4)][100.0%][r=58.0MiB/s][r=232 IOPS][eta 00m:00s] zbc: (groupid=0, jobs=4): err= 0: pid=4570: Fri May 24 13:35:15 2019 read: IOPS=215, BW=53.8MiB/s (56.4MB/s)(1617MiB/30079msec) slat (usec): min=6, max=143, avg= 7.29, stdev= 6.22 clat (usec): min=1065, max=959229, avg=74313.80, stdev=83291.27 lat (usec): min=1072, max=959237, avg=74321.15, stdev=83291.39 clat percentiles (msec): | 1.00th=[ 8], 5.00th=[ 9], 10.00th=[ 12], 20.00th=[ 17], | 30.00th=[ 24], 40.00th=[ 34], 50.00th=[ 47], 60.00th=[ 62], | 70.00th=[ 82], 80.00th=[ 113], 90.00th=[ 174], 95.00th=[ 239], | 99.00th=[ 409], 99.50th=[ 472], 99.90th=[ 634], 99.95th=[ 651], | 99.99th=[ 961] bw ( KiB/s): min=27136, max=81920, per=99.97%, avg=55030.83, stdev=2685.56, samples=240 iops : min= 106, max= 320, avg=214.10, stdev=10.53, samples=240 lat (msec) : 2=0.02%, 10=8.23%, 20=16.22%, 50=28.40%, 100=23.61% lat (msec) : 250=19.08%, 500=4.07%, 750=0.36%, 1000=0.03% cpu : usr=0.02%, sys=0.07%, ctx=6483, majf=0, minf=1070 IO depths : 1=0.1%, 2=0.1%, 4=99.8%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=6468,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=4 Run status group 0 (all jobs): READ: bw=53.8MiB/s (56.4MB/s), 53.8MiB/s-53.8MiB/s (56.4MB/s-56.4MB/s), io=1617MiB (1696MB), run=30079-30079msec Disk stats (read/write): sdd: ios=6452/0, merge=0/0, ticks=479006/0, in_queue=472619, util=21.62%","title":"Random Read and Write Workloads"},{"location":"benchmarking/fio/#direct-access-sg-io-engine","text":"The SCSI generic direct access interface can also be used with the zbd zone mode, as long as the block device file ( /dev/sdX ) is used to specify the disk. The zbd zone mode will not be enabled if the SCSI generic node file ( /dev/sgY ) is used to specify the disk. The example below illustrates the use of the sg I/O engine with 8 jobs executing a 64KB random write workload to sequential write zones. # fio --name=zbc --filename=/dev/sdd --direct=1 --zonemode=zbd \\ --offset=140660178944 --size=1G --numjobs=8 --group_reporting=1 \\ --ioengine=sg --rw=randwrite --bs=64K ... fio-3.13 Starting 8 processes zbc: (groupid=0, jobs=8): err= 0: pid=4792: Fri May 24 14:18:52 2019 write: IOPS=2007, BW=125MiB/s (132MB/s)(8192MiB/65278msec); 32 zone resets clat (usec): min=148, max=327494, avg=1589.26, stdev=4060.13 lat (usec): min=149, max=327497, avg=1590.55, stdev=4060.13 clat percentiles (usec): | 1.00th=[ 848], 5.00th=[ 988], 10.00th=[ 1090], 20.00th=[ 1172], | 30.00th=[ 1221], 40.00th=[ 1287], 50.00th=[ 1369], 60.00th=[ 1434], | 70.00th=[ 1500], 80.00th=[ 1631], 90.00th=[ 2180], 95.00th=[ 2638], | 99.00th=[ 3064], 99.50th=[ 3392], 99.90th=[ 24773], 99.95th=[ 77071], | 99.99th=[291505] bw ( KiB/s): min=39001, max=178678, per=100.00%, avg=131457.71, stdev=3143.56, samples=1015 iops : min= 606, max= 2791, avg=2052.81, stdev=49.16, samples=1015 lat (usec) : 250=0.07%, 500=0.12%, 750=0.38%, 1000=4.89% lat (msec) : 2=81.56%, 4=12.76%, 10=0.05%, 20=0.06%, 50=0.05% lat (msec) : 100=0.03%, 250=0.02%, 500=0.01% cpu : usr=0.14%, sys=0.20%, ctx=251460, majf=0, minf=139 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,131072,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=125MiB/s (132MB/s), 125MiB/s-125MiB/s (132MB/s-132MB/s), io=8192MiB (8590MB), run=65278-65278msec Disk stats (read/write): sdd: ios=278/0, merge=0/0, ticks=1953/0, in_queue=1677, util=0.39% Note SCSI generic direct access bypasses the block layer I/O scheduler. For zoned block devices, this means that the deadline I/O scheduler zone write locking is enable to provide write command ordering guarantees. However, the zbd mode ensures mutual exclusion between jobs for write access to the same zone. SUch synchronization is in essence identical to zone write locking and execute all write commands without any error.","title":"Direct Access sg I/O Engine"},{"location":"benchmarking/fio/#zone-write-streams","text":"A typical zoned block device compliant application will write zones sequentially until the zone is full, then switch to another zone and continue writing. Multiple threads may be operating in this manner, with each thread operating on a different zone. Such typical behavior can be emulated using the option --rw_sequencer together with a number of I/O operations specified at the end of the --rw=randwrite argument. Below is an example script of 4 jobs sequentially writing zones up to full using 512KB write operations (that is, 512 I/Os per 256 MB zone). The zones being written are chosen randomly within disjoint zone ranges for each job. This is controlled with the offset , size and rw arguments. The script file streams.fio achieving such workload is shown below. # # streams.fio: 4 write streams # [global] ioengine=psync direct=1 thread=1 bs=512K continue_on_error=none filename=/dev/sdd group_reporting=1 zonemode=zbd [stream1] offset=140660178944 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream2] offset=3855538454528 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream3] offset=7570416730112 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G [stream4] offset=11285295005696 size=3714878275584 rw=randwrite:512 rw_sequencer=sequential io_size=2G The result for this script execution is shown below. # fio streams.fio stream1: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream2: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream3: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 stream4: (g=0): rw=randwrite, bs=(R) 512KiB-512KiB, (W) 512KiB-512KiB, (T) 512KiB-512KiB, ioengine=psync, iodepth=1 fio-3.13 Starting 4 threads Jobs: 1 (f=1): [_(3),w(1)][98.3%][w=159MiB/s][w=318 IOPS][eta 00m:01s] stream1: (groupid=0, jobs=4): err= 0: pid=5161: Fri May 24 15:01:21 2019 write: IOPS=285, BW=143MiB/s (150MB/s)(8192MiB/57362msec); 0 zone resets clat (usec): min=992, max=5788.2k, avg=12731.27, stdev=95315.03 lat (usec): min=996, max=5788.2k, avg=12742.01, stdev=95315.03 clat percentiles (usec): | 1.00th=[ 1106], 5.00th=[ 2024], 10.00th=[ 2114], | 20.00th=[ 2278], 30.00th=[ 2769], 40.00th=[ 3687], | 50.00th=[ 3884], 60.00th=[ 4047], 70.00th=[ 4228], | 80.00th=[ 4817], 90.00th=[ 5342], 95.00th=[ 50594], | 99.00th=[ 196084], 99.50th=[ 258999], 99.90th=[ 876610], | 99.95th=[2071987], 99.99th=[4731175] bw ( KiB/s): min= 9211, max=840925, per=100.00%, avg=185793.45, stdev=44491.80, samples=352 iops : min= 17, max= 1642, avg=361.42, stdev=86.97, samples=352 lat (usec) : 1000=0.23% lat (msec) : 2=1.95%, 4=54.69%, 10=37.05%, 20=0.15%, 50=0.90% lat (msec) : 100=2.34%, 250=2.16%, 500=0.42%, 750=0.01%, 1000=0.01% lat (msec) : 2000=0.04%, >=2000=0.06% cpu : usr=0.15%, sys=0.12%, ctx=16505, majf=0, minf=0 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,16384,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=143MiB/s (150MB/s), 143MiB/s-143MiB/s (150MB/s-150MB/s), io=8192MiB (8590MB), run=57362-57362msec Disk stats (read/write): sdd: ios=144/16333, merge=0/0, ticks=573/208225, in_queue=192356, util=28.64%","title":"Zone Write Streams"},{"location":"distributions/linux/","text":"Linux Distributions As discussed here , the version and compile time configuration of a Linux\u00ae kernel defines the level of support for zoned block devices. This section gives an overview of the support provided by the pre-compiled binary kernels shipped with various Linux distributions. Fedora Linux Fedora\u00ae is a Linux distribution developed by the community-supported Fedora Project and sponsored by Red Hat\u00ae The following table gives an overview of the kernel versions and configuration used with the latest releases of the Fedora distribution. A more complete list of kernel versions for all releases can be found here . Distribution version Initial Kernel version ZBD support dm-zoned support ZNS support Fedora 26 (EOL) 4.11 YES No No Fedora 27 (EOL) 4.13 YES YES No Fedora 28 4.16 YES YES No Fedora 29 4.18 YES YES No Fedora 30 5.0 YES YES No Fedora 31 5.3 YES YES No Fedora 32 5.6 YES YES No Fedora 33 5.8 YES YES YES (after updates) Support for the zoned block interface is present and enabled by default in the binary kernel of all releases of Fedora since release 26. Starting with release 27, the pre-compiled kernel packages also include the dm-zoned device mapper target compiled as a loadable kernel module. Fedora 33 also provides NVMe Zoned Namespace (ZNS) support after updating the distribution. Detailed information on how to download and install Fedora can be found here . Debian Debian is one of the earliest Unix-like operating systems based on the Linux kernel. Debian can be shipped with different operating system kernels, such as Linux, kFreeBSD or GNU Hurd . The table below summarizes Debian Linux ZBD readiness for the most recent distribution releases. Distribution version Kernel version ZBD support dm-zoned support 9 (Stretch) 4.9 No No 10 (Buster) 4.19 YES YES Ubuntu Ubuntu is a popular free and open-source Linux distribution originally based on Debian . Ubuntu is released every six months with long-term support (LTS) releases every two years. A complete list of the kernel versions shipped with Ubuntu releases can be found here . The table below summarizes zoned block device support readiness for the most recent releases. Distribution version Kernel version ZBD support dm-zoned support 12.04 LTS (Precise Pangolin) 3.2 No No 14.04 LTS (Trusty Tahr) 3.13 No No 16.04 LTS (Xenial Xerus) 4.4 No No 17.04 (Zesty Zapus) 4.10 YES No 17.10 (Artful Aardvark) 4.13 YES YES 18.04 LTS (Bionic Beaver) 4.15 YES YES 18.10 (Cosmic Cuttlefish) 4.18 YES YES 19.04 (Disco Dingo) 5.0 YES YES 19.10 (Eoan Ermine) 5.3 YES YES 20.04 LTS (Focal Fossa) 5.4 YES YES 20.10 (Groovy Gorilla) 5.8 YES YES Red Hat Enterprise Linux Red Hat Enterprise Linux\u00ae , often abbreviated RHEL , is a Linux distribution developed by Red Hat and targeted toward the commercial server market. Red Hat Enterprise Linux is released in server versions for several micro architectures. The list of kernel versions shipped with all RHEL releases can be found here . RHEL 8 The latest release 8 of RHEL is based on the kernel version 4.18 which includes zoned block device support. However, as shown in the table below, this support is not enabled at compile time for the binary kernel shipped with the distribution. Distribution version Kernel version ZBD support dm-zoned support RHEL 8 4.18.0-80 No No RHEL 8.1 4.18.0-147 No No RHEL 8.2 4.18.0-193 No No RHEL 8.3 4.18.0-240 No No Users who require zoned block device support can recompile the RHEL kernel after enabling zoned block device support . Using such recompiled kernel may however conflict with Red Hat support. Users should contact Red Hat support for more information. RHEL 7 and 6 As indicated in the table below, all releases of RHEL 7 are based on the kernel version 3.10 which lacks zoned block device support. Distribution version Kernel version ZBD support dm-zoned support RHEL 7.0 3.10.0-123 No No RHEL 7.1 3.10.0-229 No No RHEL 7.2 3.10.0-327 No No RHEL 7.3 3.10.0-514 No No RHEL 7.4 3.10.0-693 No No RHEL 7.5 3.10.0-862 No No RHEL 7.6 3.10.0-957 No No RHEL 7.7 3.10.0-1062 No No RHEL 7.8 3.10.0-1127 No No RHEL 7.9 3.10.0-1160 No No RHEL 6 being based on the older kernel 2.6.32, zoned block devices are not supported. Distribution version Kernel version ZBD support dm-zoned support RHEL 6.0 2.6.32-71 No No RHEL 6.1 2.6.32-131 No No RHEL 6.2 2.6.32-220 No No RHEL 6.3 2.6.32-279 No No RHEL 6.4 2.6.32-358 No No RHEL 6.5 2.6.32-431 No No RHEL 6.6 2.6.32-504 No No RHEL 6.7 2.6.32-573 No No RHEL 6.8 2.6.32-642 No No RHEL 6.9 2.6.32-696 No No RHEL 6.10 2.6.32-754 No No RHEL 6 ELS+ 2.6.32-754 No No CentOS CentOS is a community maintained Linux distribution derived from the sources of Red Hat Enterprise Linux (RHEL) . CentOS release versions follow closely RHEL releases, reusing the same version and release numbers. More information on the distribution releases and kernel versions can be found here . Due to this design approach, CentOS zoned block device support level is identical to that of Red Hat Enterprise Linux . There is currently no zoned block device support available with the pre-compiled kernels shipped with the distribution. SUSE Linux Enterprise Server SUSE Linux Enterprise Server (SLES) is a Linux-based operating system developed by SUSE\u00ae . SLES is designed primarily for servers, mainframes and workstations. Major versions of SLES are released at an interval of 3 to 4 years while minor versions called \"Service Packs\" are released about every 12 months. A complete list of the kernel versions used with SLES versions can be found here . The following table only lists the most recent versions under long term service support. Distribution version Kernel version ZBD support dm-zoned support 11.3 3.0.76 No No 11.4 3.0.101 No No 12.0 3.12 No No 12.1 3.12 No No 12.2 4.4 No No 12.3 4.4 No No 12.4 4.12 YES YES 15 4.12 YES YES 15.1 4.12.14 YES YES 15.2 5.3.18 YES YES openSUSE openSUSE , formerly called SUSE Linux and SuSE Linux Professional , is a widely used Linux distribution sponsored by SUSE Linux GmbH and other companies. openSUSE focus is creating usable open-source tools for software developers and system administrators while providing a user-friendly desktop and feature-rich server environment. openSUSE is available in a stable base with the openSUSE Leap version. The openSUSE Tumbleweed is a rolling release which offers more up-to-date free software. The list of kernel versions shipped with openSUSE releases can be found here . Zoned block device support with the shipped kernel for the latest releases is shown in the table below. Distribution version Kernel version ZBD support dm-zoned support Leap 15.0 4.12 YES NO Leap 15.1 4.12 YES NO Leap 15.2 5.3 YES YES Tumbleweed latest stable (5.9+) YES YES","title":"Linux Distributions"},{"location":"distributions/linux/#linux-distributions","text":"As discussed here , the version and compile time configuration of a Linux\u00ae kernel defines the level of support for zoned block devices. This section gives an overview of the support provided by the pre-compiled binary kernels shipped with various Linux distributions.","title":"Linux Distributions"},{"location":"distributions/linux/#fedora-linux","text":"Fedora\u00ae is a Linux distribution developed by the community-supported Fedora Project and sponsored by Red Hat\u00ae The following table gives an overview of the kernel versions and configuration used with the latest releases of the Fedora distribution. A more complete list of kernel versions for all releases can be found here . Distribution version Initial Kernel version ZBD support dm-zoned support ZNS support Fedora 26 (EOL) 4.11 YES No No Fedora 27 (EOL) 4.13 YES YES No Fedora 28 4.16 YES YES No Fedora 29 4.18 YES YES No Fedora 30 5.0 YES YES No Fedora 31 5.3 YES YES No Fedora 32 5.6 YES YES No Fedora 33 5.8 YES YES YES (after updates) Support for the zoned block interface is present and enabled by default in the binary kernel of all releases of Fedora since release 26. Starting with release 27, the pre-compiled kernel packages also include the dm-zoned device mapper target compiled as a loadable kernel module. Fedora 33 also provides NVMe Zoned Namespace (ZNS) support after updating the distribution. Detailed information on how to download and install Fedora can be found here .","title":"Fedora Linux"},{"location":"distributions/linux/#debian","text":"Debian is one of the earliest Unix-like operating systems based on the Linux kernel. Debian can be shipped with different operating system kernels, such as Linux, kFreeBSD or GNU Hurd . The table below summarizes Debian Linux ZBD readiness for the most recent distribution releases. Distribution version Kernel version ZBD support dm-zoned support 9 (Stretch) 4.9 No No 10 (Buster) 4.19 YES YES","title":"Debian"},{"location":"distributions/linux/#ubuntu","text":"Ubuntu is a popular free and open-source Linux distribution originally based on Debian . Ubuntu is released every six months with long-term support (LTS) releases every two years. A complete list of the kernel versions shipped with Ubuntu releases can be found here . The table below summarizes zoned block device support readiness for the most recent releases. Distribution version Kernel version ZBD support dm-zoned support 12.04 LTS (Precise Pangolin) 3.2 No No 14.04 LTS (Trusty Tahr) 3.13 No No 16.04 LTS (Xenial Xerus) 4.4 No No 17.04 (Zesty Zapus) 4.10 YES No 17.10 (Artful Aardvark) 4.13 YES YES 18.04 LTS (Bionic Beaver) 4.15 YES YES 18.10 (Cosmic Cuttlefish) 4.18 YES YES 19.04 (Disco Dingo) 5.0 YES YES 19.10 (Eoan Ermine) 5.3 YES YES 20.04 LTS (Focal Fossa) 5.4 YES YES 20.10 (Groovy Gorilla) 5.8 YES YES","title":"Ubuntu"},{"location":"distributions/linux/#red-hat-enterprise-linux","text":"Red Hat Enterprise Linux\u00ae , often abbreviated RHEL , is a Linux distribution developed by Red Hat and targeted toward the commercial server market. Red Hat Enterprise Linux is released in server versions for several micro architectures. The list of kernel versions shipped with all RHEL releases can be found here .","title":"Red Hat Enterprise Linux"},{"location":"distributions/linux/#rhel-8","text":"The latest release 8 of RHEL is based on the kernel version 4.18 which includes zoned block device support. However, as shown in the table below, this support is not enabled at compile time for the binary kernel shipped with the distribution. Distribution version Kernel version ZBD support dm-zoned support RHEL 8 4.18.0-80 No No RHEL 8.1 4.18.0-147 No No RHEL 8.2 4.18.0-193 No No RHEL 8.3 4.18.0-240 No No Users who require zoned block device support can recompile the RHEL kernel after enabling zoned block device support . Using such recompiled kernel may however conflict with Red Hat support. Users should contact Red Hat support for more information.","title":"RHEL 8"},{"location":"distributions/linux/#rhel-7-and-6","text":"As indicated in the table below, all releases of RHEL 7 are based on the kernel version 3.10 which lacks zoned block device support. Distribution version Kernel version ZBD support dm-zoned support RHEL 7.0 3.10.0-123 No No RHEL 7.1 3.10.0-229 No No RHEL 7.2 3.10.0-327 No No RHEL 7.3 3.10.0-514 No No RHEL 7.4 3.10.0-693 No No RHEL 7.5 3.10.0-862 No No RHEL 7.6 3.10.0-957 No No RHEL 7.7 3.10.0-1062 No No RHEL 7.8 3.10.0-1127 No No RHEL 7.9 3.10.0-1160 No No RHEL 6 being based on the older kernel 2.6.32, zoned block devices are not supported. Distribution version Kernel version ZBD support dm-zoned support RHEL 6.0 2.6.32-71 No No RHEL 6.1 2.6.32-131 No No RHEL 6.2 2.6.32-220 No No RHEL 6.3 2.6.32-279 No No RHEL 6.4 2.6.32-358 No No RHEL 6.5 2.6.32-431 No No RHEL 6.6 2.6.32-504 No No RHEL 6.7 2.6.32-573 No No RHEL 6.8 2.6.32-642 No No RHEL 6.9 2.6.32-696 No No RHEL 6.10 2.6.32-754 No No RHEL 6 ELS+ 2.6.32-754 No No","title":"RHEL 7 and 6"},{"location":"distributions/linux/#centos","text":"CentOS is a community maintained Linux distribution derived from the sources of Red Hat Enterprise Linux (RHEL) . CentOS release versions follow closely RHEL releases, reusing the same version and release numbers. More information on the distribution releases and kernel versions can be found here . Due to this design approach, CentOS zoned block device support level is identical to that of Red Hat Enterprise Linux . There is currently no zoned block device support available with the pre-compiled kernels shipped with the distribution.","title":"CentOS"},{"location":"distributions/linux/#suse-linux-enterprise-server","text":"SUSE Linux Enterprise Server (SLES) is a Linux-based operating system developed by SUSE\u00ae . SLES is designed primarily for servers, mainframes and workstations. Major versions of SLES are released at an interval of 3 to 4 years while minor versions called \"Service Packs\" are released about every 12 months. A complete list of the kernel versions used with SLES versions can be found here . The following table only lists the most recent versions under long term service support. Distribution version Kernel version ZBD support dm-zoned support 11.3 3.0.76 No No 11.4 3.0.101 No No 12.0 3.12 No No 12.1 3.12 No No 12.2 4.4 No No 12.3 4.4 No No 12.4 4.12 YES YES 15 4.12 YES YES 15.1 4.12.14 YES YES 15.2 5.3.18 YES YES","title":"SUSE Linux Enterprise Server"},{"location":"distributions/linux/#opensuse","text":"openSUSE , formerly called SUSE Linux and SuSE Linux Professional , is a widely used Linux distribution sponsored by SUSE Linux GmbH and other companies. openSUSE focus is creating usable open-source tools for software developers and system administrators while providing a user-friendly desktop and feature-rich server environment. openSUSE is available in a stable base with the openSUSE Leap version. The openSUSE Tumbleweed is a rolling release which offers more up-to-date free software. The list of kernel versions shipped with openSUSE releases can be found here . Zoned block device support with the shipped kernel for the latest releases is shown in the table below. Distribution version Kernel version ZBD support dm-zoned support Leap 15.0 4.12 YES NO Leap 15.1 4.12 YES NO Leap 15.2 5.3 YES YES Tumbleweed latest stable (5.9+) YES YES","title":"openSUSE"},{"location":"faq/faq/","text":"Frequently Asked Questions Can I change the size of the zones of a device? The size of the zones of a physical zoned device are fixed at manufacturing time by the device vendor. For a particular device model, it is not possible to change the zone size. Device emulation software such as tcmu-runner allow defining zoned block devices with different zone sizes. While this does not replace the ability to change an existing device zone size, such solution allows exploring the impact of the device zone size on the application being developed. With a host managed drive, how do I change the position of a zone write pointer? The write pointer position of a zone changes automatically in response to the following operations. A write operation (regular write command, write same command) issued with a starting LBA equal to the zone write pointer current location or a zone append write operation issued with a starting LBA equal to the zone start LBA: the write pointer position advances by an amount of LBAs written. A zone reset operation: the write pointer position of the target zone(s) is reset to the start of the zone. A zone finish operation: the write pointer position of the target zone(s) is changed an invalid value with the zone condition cnahed to FULL. A device low level format operation: the write pointer of all zones is reset. No other command, operation or user action can change the position of a zone write pointer to any position within the zone. How do I rewind the position of a zone write pointer? As mentioned in the previous question reply, resetting a zone will rewind the zone write pointer position to the first LBA of the zone. A low level device format operation will also reset all zones. How do I partially rewind the position of a zone write pointer? This is not possible. A zone write pointer can only be moved back to the start LBA of the zone using a reset write pointer command. Can I create partitions on my host managed device? Kernel versions 4.10 to 5.4 include support for partition tables on host managed zoned block devices. However, partitioning tools such as gparted generally used to create GUID partition tables do not support host managed zoned devices that do not have a conventional zone at the beginning and end of the LBA space. If the first zone and last zone of the device are sequential write required zones, writing the primary and secondary GPT header and table entries will likely fail, resulting in an incorrect or corrupted partition table that will not be recognized by the kernel block layer. Note Linux\u00ae kernel mandates that partitions of a zoned block device be zone aligned. That is, the start sector of all device partitions must be the start sector of a zone and the end sector of the partitions must be the last sector of a zone. Support for partition tables on host managed zoned block devices has been removed from the kernel with version 5.5.0. Using the dm-linear device mapper target to logically isolate smaller portions of a host managed device is the replacement and preferred solution. More information on partition support can be found here .","title":"Frequently Asked Questions"},{"location":"faq/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/faq/#can-i-change-the-size-of-the-zones-of-a-device","text":"The size of the zones of a physical zoned device are fixed at manufacturing time by the device vendor. For a particular device model, it is not possible to change the zone size. Device emulation software such as tcmu-runner allow defining zoned block devices with different zone sizes. While this does not replace the ability to change an existing device zone size, such solution allows exploring the impact of the device zone size on the application being developed.","title":"Can I change the size of the zones of a device?"},{"location":"faq/faq/#with-a-host-managed-drive-how-do-i-change-the-position-of-a-zone-write-pointer","text":"The write pointer position of a zone changes automatically in response to the following operations. A write operation (regular write command, write same command) issued with a starting LBA equal to the zone write pointer current location or a zone append write operation issued with a starting LBA equal to the zone start LBA: the write pointer position advances by an amount of LBAs written. A zone reset operation: the write pointer position of the target zone(s) is reset to the start of the zone. A zone finish operation: the write pointer position of the target zone(s) is changed an invalid value with the zone condition cnahed to FULL. A device low level format operation: the write pointer of all zones is reset. No other command, operation or user action can change the position of a zone write pointer to any position within the zone.","title":"With a host managed drive, how do I change the position of a zone write pointer?"},{"location":"faq/faq/#how-do-i-rewind-the-position-of-a-zone-write-pointer","text":"As mentioned in the previous question reply, resetting a zone will rewind the zone write pointer position to the first LBA of the zone. A low level device format operation will also reset all zones.","title":"How do I rewind the position of a zone write pointer?"},{"location":"faq/faq/#how-do-i-partially-rewind-the-position-of-a-zone-write-pointer","text":"This is not possible. A zone write pointer can only be moved back to the start LBA of the zone using a reset write pointer command.","title":"How do I partially rewind the position of a zone write pointer?"},{"location":"faq/faq/#can-i-create-partitions-on-my-host-managed-device","text":"Kernel versions 4.10 to 5.4 include support for partition tables on host managed zoned block devices. However, partitioning tools such as gparted generally used to create GUID partition tables do not support host managed zoned devices that do not have a conventional zone at the beginning and end of the LBA space. If the first zone and last zone of the device are sequential write required zones, writing the primary and secondary GPT header and table entries will likely fail, resulting in an incorrect or corrupted partition table that will not be recognized by the kernel block layer. Note Linux\u00ae kernel mandates that partitions of a zoned block device be zone aligned. That is, the start sector of all device partitions must be the start sector of a zone and the end sector of the partitions must be the last sector of a zone. Support for partition tables on host managed zoned block devices has been removed from the kernel with version 5.5.0. Using the dm-linear device mapper target to logically isolate smaller portions of a host managed device is the replacement and preferred solution. More information on partition support can be found here .","title":"Can I create partitions on my host managed device?"},{"location":"getting-started/","text":"Getting Started Learn how to setup a Linux system to quickly start using zoned storage devices. System Prerequisites : Learn how to setup a Linux system with zoned block device support enabled. Getting Started with an Emulated Zoned Block Device : Learn how to use the null_blk device driver to emulate zoned block devices with different zones configurations. Getting Started with an SMR Disk : Learn how to identify SMR disks and verify that the host system is operating correctly. Getting Started with an Emulated SMR Disk : Learn how to setup and use an emulated SMR disks equivalent to a real physical device. Getting Started with an Emulated NVMe ZNS Device : Learn how to setup and use an emulated NVMe device providing Zoned namespaces equivalent to a real physical device.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"Learn how to setup a Linux system to quickly start using zoned storage devices. System Prerequisites : Learn how to setup a Linux system with zoned block device support enabled. Getting Started with an Emulated Zoned Block Device : Learn how to use the null_blk device driver to emulate zoned block devices with different zones configurations. Getting Started with an SMR Disk : Learn how to identify SMR disks and verify that the host system is operating correctly. Getting Started with an Emulated SMR Disk : Learn how to setup and use an emulated SMR disks equivalent to a real physical device. Getting Started with an Emulated NVMe ZNS Device : Learn how to setup and use an emulated NVMe device providing Zoned namespaces equivalent to a real physical device.","title":"Getting Started"},{"location":"getting-started/nullblk/","text":"Zoned Block Device Emulation with null_blk Linux\u00ae null_blk driver is a powerful tool to emulate various types of block devices for tests. Since kernel 4.19, the null_blk driver gain the ability to emulate zoned block devices. With the addition of memory backup for data read and writen to a null_blk device, this driver provides an easy to use but yet powerful tool for application development and tests. Creating a Zoned null Block Device The simplest method to create a null_blk emulated zoned block device is to specify the zoned=1 argument to null_blk modrpobe command line. # modprobe null_blk nr_devices=1 zoned=1 This creates a single host managed zoned block device with a zone size of 256M and a total capacity of 250 GB (1000 zones). No conventional zones are created with this simple command. # blkzone report /dev/nullb0 start: 0x000000000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000080000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000100000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000180000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x01f300000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x01f380000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] null_blk Zoned Block Device Parameters The null_blk kernel module accepts many arguments to adjust the zone configuration of the emulated device. The zone related arguments can be listed using the modinfo command and using configfs once the null_blk module is loaded. # modinfo null_blk ... parm: zoned:Make device as a host-managed zoned block device. Default: false (bool) parm: zone_size:Zone size in MB when block device is zoned. Must be power-of-two: Default: 256 (ulong) parm: zone_capacity:Zone capacity in MB when block device is zoned. Can be less than or equal to zone size. Default: Zone size (ulong) parm: zone_nr_conv:Number of conventional zones when block device is zoned. Default: 0 (uint) parm: zone_max_open:Maximum number of open zones when block device is zoned. Default: 0 (no limit) (uint) parm: zone_max_active:Maximum number of active zones when block device is zoned. Default: 0 (no limit) (uint) The parameters related to zoned device emulation are shown in the table below. Argument Value Description zoned 0 or 1 Disable or enable zoned mode (default: disabled) zone_size zone size in MiB The size of each zone (default: 256) zone_capacity zone capacity in MiB The capacity of each zone (default: zone size) zone_nr_conv number Number of conventional zones (default: 0) zone_max_open number Maximum number of open zones (default: 0, meaning no limit) zone_max_active number Maximum number of active zones (default: 0, meaning no limit) Creating a null_blk Zoned Block Device To create an emulated zoned block device with null_blk , as shown above, the modprobe command can be used. Additional parameters can be passed to this command to configure the emulated disk. # modprobe null_blk nr_devices=1 \\ zoned=1 \\ zone_nr_conv=4 \\ zone_size=64 \\ The configfs interface of the null_blk driver provides a more powerful method for creating emulated zoned block devices. The configfs parameters of the null_blk driver can be listed with the following command. # modprobe null_blk nr_devices=0 # cat /sys/kernel/config/nullb/features memory_backed,discard,bandwidth,cache,badblocks,zoned,zone_size,zone_capacity,zone_nr_conv,zone_max_open,zone_max_active The configfs interface can be used to script the creation of emulated zoned block devices with different zone configurations. An example is provided below. #!/bin/bash if [ $# != 4 ]; then echo \"Usage: $0 <sect size (B)> <zone size (MB)> <nr conv zones> <nr seq zones>\" exit 1 fi scriptdir=$(cd $(dirname \"$0\") && pwd) modprobe null_blk nr_devices=0 || return $? function create_zoned_nullb() { local nid=0 local bs=$1 local zs=$2 local nr_conv=$3 local nr_seq=$4 cap=$(( zs * (nr_conv + nr_seq) )) while [ 1 ]; do if [ ! -b \"/dev/nullb$nid\" ]; then break fi nid=$(( nid + 1 )) done dev=\"/sys/kernel/config/nullb/nullb$nid\" mkdir \"$dev\" echo $bs > \"$dev\"/blocksize echo 0 > \"$dev\"/completion_nsec echo 0 > \"$dev\"/irqmode echo 2 > \"$dev\"/queue_mode echo 1024 > \"$dev\"/hw_queue_depth echo 1 > \"$dev\"/memory_backed echo 1 > \"$dev\"/zoned echo $cap > \"$dev\"/size echo $zs > \"$dev\"/zone_size echo $nr_conv > \"$dev\"/zone_nr_conv echo 1 > \"$dev\"/power echo mq-deadline > /sys/block/nullb$nid/queue/scheduler echo \"$nid\" } nulldev=$(create_zoned_nullb $1 $2 $3 $4) echo \"Created /dev/nullb$nulldev\" This script ( nullblk-zoned.sh ) takes four arguments: the sector size in bytes, of the emulated device, the device zone size in MiB, the number of conventional zones (which can be 0) and the number of sequential write required zones. Memory backing for writen sectors is turned on with this script (memory_backed=1). This enables run-time persistancy of the data written to the sectors of the emulated device. The writen data is lost when the emulated device is destroyed. For example, a small zoned device with 4 conventional zones and 8 sequential write required zones of 64 MiB can be created with the following command. # nullblk-zoned.sh 4096 64 4 8 Created /dev/nullb0 # blkzone report /dev/nullb0 start: 0x000000000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000020000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000040000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000060000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000a0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000c0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000e0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000100000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x000820000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000840000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000860000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] Deleting a null_blk Zoned Block Device For emulated devices created through modprobe , the emulated devices can be deleted by simply removing the null_blk kernel module. # rmmod null_blk This command does not work to delete emulated devices created through the configfs interface. The following script is the counter part of the zoned block device creation script show above. It can be used to destroy null_blk devices created through configfs . #!/bin/bash if [ $# != 1 ]; then echo \"Usage: $0 <nullb ID>\" exit 1 fi nid=$1 if [ ! -b \"/dev/nullb$nid\" ]; then echo \"/dev/nullb$nid: No such device\" exit 1 fi echo 0 > /sys/kernel/config/nullb/nullb$nid/power rmdir /sys/kernel/config/nullb/nullb$nid echo \"Destroyed /dev/nullb$nid\" SMR HDD Emulation As is, the nullblk-zoned.sh script allows creating zoned block devices that correspond to a possible configuration of an SMR hard disk with no limit on the maximum number of open zones. This script can be easily modified to add a limit on the number of open zones of the emulated device ( zone_max_open parameter) to more facefully emulate an SMR HDD characteristics. The zone_capacity and zone_max_active parameters should not be used when the emulated device must mimic the characteristics of a SMR hard disk. NVMe ZNS SSD Emulation The zone_capacity and zone_max_active parameters allow creating an emulated zoned block device mimicking the characteristics of a NVMe Zoned Namespace SSD. The zone_capacity parameter can be used to specify the amount of sectors in each zone that can be read and written, while the zone_max_active argument can specify a limit on the number of zones that can be in the closed, implicit open and explicit open states.","title":"Getting Started with an Emulated Zoned Block Device"},{"location":"getting-started/nullblk/#zoned-block-device-emulation-with-null_blk","text":"Linux\u00ae null_blk driver is a powerful tool to emulate various types of block devices for tests. Since kernel 4.19, the null_blk driver gain the ability to emulate zoned block devices. With the addition of memory backup for data read and writen to a null_blk device, this driver provides an easy to use but yet powerful tool for application development and tests.","title":"Zoned Block Device Emulation with null_blk"},{"location":"getting-started/nullblk/#creating-a-zoned-null-block-device","text":"The simplest method to create a null_blk emulated zoned block device is to specify the zoned=1 argument to null_blk modrpobe command line. # modprobe null_blk nr_devices=1 zoned=1 This creates a single host managed zoned block device with a zone size of 256M and a total capacity of 250 GB (1000 zones). No conventional zones are created with this simple command. # blkzone report /dev/nullb0 start: 0x000000000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000080000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000100000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000180000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x01f300000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x01f380000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)]","title":"Creating a Zoned null Block Device"},{"location":"getting-started/nullblk/#null_blk-zoned-block-device-parameters","text":"The null_blk kernel module accepts many arguments to adjust the zone configuration of the emulated device. The zone related arguments can be listed using the modinfo command and using configfs once the null_blk module is loaded. # modinfo null_blk ... parm: zoned:Make device as a host-managed zoned block device. Default: false (bool) parm: zone_size:Zone size in MB when block device is zoned. Must be power-of-two: Default: 256 (ulong) parm: zone_capacity:Zone capacity in MB when block device is zoned. Can be less than or equal to zone size. Default: Zone size (ulong) parm: zone_nr_conv:Number of conventional zones when block device is zoned. Default: 0 (uint) parm: zone_max_open:Maximum number of open zones when block device is zoned. Default: 0 (no limit) (uint) parm: zone_max_active:Maximum number of active zones when block device is zoned. Default: 0 (no limit) (uint) The parameters related to zoned device emulation are shown in the table below. Argument Value Description zoned 0 or 1 Disable or enable zoned mode (default: disabled) zone_size zone size in MiB The size of each zone (default: 256) zone_capacity zone capacity in MiB The capacity of each zone (default: zone size) zone_nr_conv number Number of conventional zones (default: 0) zone_max_open number Maximum number of open zones (default: 0, meaning no limit) zone_max_active number Maximum number of active zones (default: 0, meaning no limit)","title":"null_blk Zoned Block Device Parameters"},{"location":"getting-started/nullblk/#creating-a-null_blk-zoned-block-device","text":"To create an emulated zoned block device with null_blk , as shown above, the modprobe command can be used. Additional parameters can be passed to this command to configure the emulated disk. # modprobe null_blk nr_devices=1 \\ zoned=1 \\ zone_nr_conv=4 \\ zone_size=64 \\ The configfs interface of the null_blk driver provides a more powerful method for creating emulated zoned block devices. The configfs parameters of the null_blk driver can be listed with the following command. # modprobe null_blk nr_devices=0 # cat /sys/kernel/config/nullb/features memory_backed,discard,bandwidth,cache,badblocks,zoned,zone_size,zone_capacity,zone_nr_conv,zone_max_open,zone_max_active The configfs interface can be used to script the creation of emulated zoned block devices with different zone configurations. An example is provided below. #!/bin/bash if [ $# != 4 ]; then echo \"Usage: $0 <sect size (B)> <zone size (MB)> <nr conv zones> <nr seq zones>\" exit 1 fi scriptdir=$(cd $(dirname \"$0\") && pwd) modprobe null_blk nr_devices=0 || return $? function create_zoned_nullb() { local nid=0 local bs=$1 local zs=$2 local nr_conv=$3 local nr_seq=$4 cap=$(( zs * (nr_conv + nr_seq) )) while [ 1 ]; do if [ ! -b \"/dev/nullb$nid\" ]; then break fi nid=$(( nid + 1 )) done dev=\"/sys/kernel/config/nullb/nullb$nid\" mkdir \"$dev\" echo $bs > \"$dev\"/blocksize echo 0 > \"$dev\"/completion_nsec echo 0 > \"$dev\"/irqmode echo 2 > \"$dev\"/queue_mode echo 1024 > \"$dev\"/hw_queue_depth echo 1 > \"$dev\"/memory_backed echo 1 > \"$dev\"/zoned echo $cap > \"$dev\"/size echo $zs > \"$dev\"/zone_size echo $nr_conv > \"$dev\"/zone_nr_conv echo 1 > \"$dev\"/power echo mq-deadline > /sys/block/nullb$nid/queue/scheduler echo \"$nid\" } nulldev=$(create_zoned_nullb $1 $2 $3 $4) echo \"Created /dev/nullb$nulldev\" This script ( nullblk-zoned.sh ) takes four arguments: the sector size in bytes, of the emulated device, the device zone size in MiB, the number of conventional zones (which can be 0) and the number of sequential write required zones. Memory backing for writen sectors is turned on with this script (memory_backed=1). This enables run-time persistancy of the data written to the sectors of the emulated device. The writen data is lost when the emulated device is destroyed. For example, a small zoned device with 4 conventional zones and 8 sequential write required zones of 64 MiB can be created with the following command. # nullblk-zoned.sh 4096 64 4 8 Created /dev/nullb0 # blkzone report /dev/nullb0 start: 0x000000000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000020000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000040000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000060000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000a0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000c0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x0000e0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000100000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x000820000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000840000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000860000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)]","title":"Creating a null_blk Zoned Block Device"},{"location":"getting-started/nullblk/#deleting-a-null_blk-zoned-block-device","text":"For emulated devices created through modprobe , the emulated devices can be deleted by simply removing the null_blk kernel module. # rmmod null_blk This command does not work to delete emulated devices created through the configfs interface. The following script is the counter part of the zoned block device creation script show above. It can be used to destroy null_blk devices created through configfs . #!/bin/bash if [ $# != 1 ]; then echo \"Usage: $0 <nullb ID>\" exit 1 fi nid=$1 if [ ! -b \"/dev/nullb$nid\" ]; then echo \"/dev/nullb$nid: No such device\" exit 1 fi echo 0 > /sys/kernel/config/nullb/nullb$nid/power rmdir /sys/kernel/config/nullb/nullb$nid echo \"Destroyed /dev/nullb$nid\"","title":"Deleting a null_blk Zoned Block Device"},{"location":"getting-started/nullblk/#smr-hdd-emulation","text":"As is, the nullblk-zoned.sh script allows creating zoned block devices that correspond to a possible configuration of an SMR hard disk with no limit on the maximum number of open zones. This script can be easily modified to add a limit on the number of open zones of the emulated device ( zone_max_open parameter) to more facefully emulate an SMR HDD characteristics. The zone_capacity and zone_max_active parameters should not be used when the emulated device must mimic the characteristics of a SMR hard disk.","title":"SMR HDD Emulation"},{"location":"getting-started/nullblk/#nvme-zns-ssd-emulation","text":"The zone_capacity and zone_max_active parameters allow creating an emulated zoned block device mimicking the characteristics of a NVMe Zoned Namespace SSD. The zone_capacity parameter can be used to specify the amount of sectors in each zone that can be read and written, while the zone_max_active argument can specify a limit on the number of zones that can be in the closed, implicit open and explicit open states.","title":"NVMe ZNS SSD Emulation"},{"location":"getting-started/prerequisite/","text":"System Prerequisite The zoned block device (ZBD) interface supporting ZBC and ZAC disks was added to Linux\u00ae kernel version 4.10. All Linux kernel versions since 4.10 include the ZBD interface. Note Linux kernels prior to version 4.10 do not have the ZBD interface implemented. As a result of using such older kernel, access and management of ZBC and ZAC disks will be limited but still possible. This is discussed in more details in the Linux Support document. In addition to a correct Linux kernel version, several user utilities should be installed on the test system to easily verify if the zoned device is discovered and initialized correctly. Linux Kernel A system with a Linux kernel version 4.10 or higher is recommended to get started with ZBC and ZAC hard disks. For a quick start, it is recommended to use a Linux distribution including ZBD support. More information on recommended Linux distributions can be found here . Advanced users may want to compile and install a specific Linux kernel version to be used in place of the default kernel shipped with the distribution being used. Instructions on how to enable ZBD support in the kernel configuration are provided here . It is recommended to always use the highest available stable kernel version, or a long term stable kernel version higher than 4.10. Information on available kernel versions can be found here . Kernel Version and ZBD Support Two conditions must be met to ensure that a system Linux kernel supports the ZBD interface. The kernel version must be 4.10.0 or higher, The kernel compilation configuration option CONFIG_BLK_DEV_ZONED must be enabled. Kernel Version The command uname allows checking the version of the kernel running on a system. For example, on a Fedora 29 distribution, this command output is as follows. # uname -r 5.0.13-200.fc29.x86_64 Zoned Block Device Support Zoned block device support is optional and may not be enabled in the running kernel. The kernel configuration option enabling zoned block device support is CONFIG_BLK_DEV_ZONED . To check if the CONFIG_BLK_DEV_ZONED option is enabled for the kernel, several method can be used. Not all method may work for a particular distribution. In many cases, the configuration file for the running kernel can be found under the /boot directory and/or within the directory containing the kernel modules. In such case, the following commands allow testing if the installed kernel supports zoned block devices. # cat /boot/config-`uname -r` | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y or # cat /lib/modules/`uname -r`/config | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y If the output of one of these commands is CONFIG_BLK_DEV_ZONED=y , then zoned block devices are supported by the kernel. If the output is CONFIG_BLK_DEV_ZONED=n , then block device support is disabled and the kernel needs to be recompiled. Note For kernels older than kernel version 4.10, the output of these commands is always empty. For kernels exporting the configuration through the proc file system, the following command can also be used. # modprobe configs # cat /proc/config.gz | gunzip | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y or # modprobe configs # zcat /proc/config.gz | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y Write Ordering Control By default, Linux kernel does not provide any guarantee on the order in which commands are delivered to a block device. That is, an application writing sequentially to a disk may see write commands being delivered to the disk in a different order. This can cause write errors when writing to a zoned device sequential zones. To avoid this problem, a zone write lock mechanism serializing writes to sequential zones is implemented by all kernels supporting zoned block devices. For kernel versions between 4.10 and 4.15 included, no special configuration is necessary and the kernel will provide guarantees that write commands are delivered to the device in the same order as the application write requests issuing order. However, starting with kernel version 4.16, zone write locking implementation was moved to the deadline and mq-deadline block I/O scheduler. Using this scheduler with zoned block devices is mandatory to ensure write command order guarantees. Note The mq-deadline block I/O scheduler is enabled only if the SCSI multi-queue ( scsi-mq ) infrastructure is enabled. This feature use can be controlled using the kernel boot argument scsi_mod.use_blk_mq\". scsi-mq* is the default since kernel version 5.0 and the legacy single queue SCSI command path is no longer supported. To verify the block I/O scheduler of a zoned disk, the following command can be used. # cat /sys/block/sdb/queue/scheduler [none] mq-deadline kyber bfq If the disk block I/O scheduler selected is not mq-deadline as in the example above, the scheduler can be changed with the following command. # echo deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none User Utilities Various user level tools should also be installed in order to verify the correct operation of zoned block devices and to troubleshoot problems. lsblk The lsblk command in Linux lists block devices, including zoned block devices. This utility is generally packaged as part of the util-linux package which is installed by default on most Linux distributions. lsblk usage examples are provided here . blkzone Similarly to the lsblk utility, the blkzone utility is another program generally packaged as part of the util-linux package. This utility allows listing (reporting) the zones of a zoned block device and to reset the write pointer position of a range of zones of the device. blkzone usage examples are provided here . lsscsi The lsscsi command lists information about the SCSI devices connected to a Linux system. lsscsi is generally available as a package with most Linux distributions. Refer to your distribution documentation to find out the name of the package providing the lsscsi utility. The linux utilities page provides more information and usage examples. sg3_utils The sg3_utils package is a collection of command line tools that send SCSI commands to a SCSI device. Since in Linux all disks are exposed as SCSI disks, including all ATA drives, these utilities can be used to manage both SAS ZBC disks and SATA ZAC disks. For SATA disks connected to SATA ports (e.g. An AHCI adapter), the kernel SCSI subsystem translates SCSI commands to ATA commands. sg3_utils includes three command line tools specific to ZBC disks. Utility Name SCSI Command Invoked Description sg_rep_zones REPORT ZONES Get a ZBC disk zone information sg_reset_wp RESET WRITE POINTER Reset one or all zones of ZBC disk sg_zone CLOSE ZONE, FINISH ZONE, OPEN ZONE Sends one of these commands to the given ZBC disk This section shows some examples of these utilities execution libzbc libzbc is a user space library providing functions for manipulating ZBC and ZAC disks. The libzbc project is hosted on GitHub . Documentation is provided in the project README file. The API documentation can also be automatically generated using doxygen . libzbc also provides a set of command line utilities with similar functionalities as the blkzone utility and the sg3_utils command line tools. More information on how to compile and install libzbc as well as usage examples of the command line utilities provided can be found here .","title":"System Prerequisite"},{"location":"getting-started/prerequisite/#system-prerequisite","text":"The zoned block device (ZBD) interface supporting ZBC and ZAC disks was added to Linux\u00ae kernel version 4.10. All Linux kernel versions since 4.10 include the ZBD interface. Note Linux kernels prior to version 4.10 do not have the ZBD interface implemented. As a result of using such older kernel, access and management of ZBC and ZAC disks will be limited but still possible. This is discussed in more details in the Linux Support document. In addition to a correct Linux kernel version, several user utilities should be installed on the test system to easily verify if the zoned device is discovered and initialized correctly.","title":"System Prerequisite"},{"location":"getting-started/prerequisite/#linux-kernel","text":"A system with a Linux kernel version 4.10 or higher is recommended to get started with ZBC and ZAC hard disks. For a quick start, it is recommended to use a Linux distribution including ZBD support. More information on recommended Linux distributions can be found here . Advanced users may want to compile and install a specific Linux kernel version to be used in place of the default kernel shipped with the distribution being used. Instructions on how to enable ZBD support in the kernel configuration are provided here . It is recommended to always use the highest available stable kernel version, or a long term stable kernel version higher than 4.10. Information on available kernel versions can be found here .","title":"Linux Kernel"},{"location":"getting-started/prerequisite/#kernel-version-and-zbd-support","text":"Two conditions must be met to ensure that a system Linux kernel supports the ZBD interface. The kernel version must be 4.10.0 or higher, The kernel compilation configuration option CONFIG_BLK_DEV_ZONED must be enabled.","title":"Kernel Version and ZBD Support"},{"location":"getting-started/prerequisite/#kernel-version","text":"The command uname allows checking the version of the kernel running on a system. For example, on a Fedora 29 distribution, this command output is as follows. # uname -r 5.0.13-200.fc29.x86_64","title":"Kernel Version"},{"location":"getting-started/prerequisite/#zoned-block-device-support","text":"Zoned block device support is optional and may not be enabled in the running kernel. The kernel configuration option enabling zoned block device support is CONFIG_BLK_DEV_ZONED . To check if the CONFIG_BLK_DEV_ZONED option is enabled for the kernel, several method can be used. Not all method may work for a particular distribution. In many cases, the configuration file for the running kernel can be found under the /boot directory and/or within the directory containing the kernel modules. In such case, the following commands allow testing if the installed kernel supports zoned block devices. # cat /boot/config-`uname -r` | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y or # cat /lib/modules/`uname -r`/config | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y If the output of one of these commands is CONFIG_BLK_DEV_ZONED=y , then zoned block devices are supported by the kernel. If the output is CONFIG_BLK_DEV_ZONED=n , then block device support is disabled and the kernel needs to be recompiled. Note For kernels older than kernel version 4.10, the output of these commands is always empty. For kernels exporting the configuration through the proc file system, the following command can also be used. # modprobe configs # cat /proc/config.gz | gunzip | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y or # modprobe configs # zcat /proc/config.gz | grep CONFIG_BLK_DEV_ZONED CONFIG_BLK_DEV_ZONED=y","title":"Zoned Block Device Support"},{"location":"getting-started/prerequisite/#write-ordering-control","text":"By default, Linux kernel does not provide any guarantee on the order in which commands are delivered to a block device. That is, an application writing sequentially to a disk may see write commands being delivered to the disk in a different order. This can cause write errors when writing to a zoned device sequential zones. To avoid this problem, a zone write lock mechanism serializing writes to sequential zones is implemented by all kernels supporting zoned block devices. For kernel versions between 4.10 and 4.15 included, no special configuration is necessary and the kernel will provide guarantees that write commands are delivered to the device in the same order as the application write requests issuing order. However, starting with kernel version 4.16, zone write locking implementation was moved to the deadline and mq-deadline block I/O scheduler. Using this scheduler with zoned block devices is mandatory to ensure write command order guarantees. Note The mq-deadline block I/O scheduler is enabled only if the SCSI multi-queue ( scsi-mq ) infrastructure is enabled. This feature use can be controlled using the kernel boot argument scsi_mod.use_blk_mq\". scsi-mq* is the default since kernel version 5.0 and the legacy single queue SCSI command path is no longer supported. To verify the block I/O scheduler of a zoned disk, the following command can be used. # cat /sys/block/sdb/queue/scheduler [none] mq-deadline kyber bfq If the disk block I/O scheduler selected is not mq-deadline as in the example above, the scheduler can be changed with the following command. # echo deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none","title":"Write Ordering Control"},{"location":"getting-started/prerequisite/#user-utilities","text":"Various user level tools should also be installed in order to verify the correct operation of zoned block devices and to troubleshoot problems.","title":"User Utilities"},{"location":"getting-started/prerequisite/#lsblk","text":"The lsblk command in Linux lists block devices, including zoned block devices. This utility is generally packaged as part of the util-linux package which is installed by default on most Linux distributions. lsblk usage examples are provided here .","title":"lsblk"},{"location":"getting-started/prerequisite/#blkzone","text":"Similarly to the lsblk utility, the blkzone utility is another program generally packaged as part of the util-linux package. This utility allows listing (reporting) the zones of a zoned block device and to reset the write pointer position of a range of zones of the device. blkzone usage examples are provided here .","title":"blkzone"},{"location":"getting-started/prerequisite/#lsscsi","text":"The lsscsi command lists information about the SCSI devices connected to a Linux system. lsscsi is generally available as a package with most Linux distributions. Refer to your distribution documentation to find out the name of the package providing the lsscsi utility. The linux utilities page provides more information and usage examples.","title":"lsscsi"},{"location":"getting-started/prerequisite/#sg3_utils","text":"The sg3_utils package is a collection of command line tools that send SCSI commands to a SCSI device. Since in Linux all disks are exposed as SCSI disks, including all ATA drives, these utilities can be used to manage both SAS ZBC disks and SATA ZAC disks. For SATA disks connected to SATA ports (e.g. An AHCI adapter), the kernel SCSI subsystem translates SCSI commands to ATA commands. sg3_utils includes three command line tools specific to ZBC disks. Utility Name SCSI Command Invoked Description sg_rep_zones REPORT ZONES Get a ZBC disk zone information sg_reset_wp RESET WRITE POINTER Reset one or all zones of ZBC disk sg_zone CLOSE ZONE, FINISH ZONE, OPEN ZONE Sends one of these commands to the given ZBC disk This section shows some examples of these utilities execution","title":"sg3_utils"},{"location":"getting-started/prerequisite/#libzbc","text":"libzbc is a user space library providing functions for manipulating ZBC and ZAC disks. The libzbc project is hosted on GitHub . Documentation is provided in the project README file. The API documentation can also be automatically generated using doxygen . libzbc also provides a set of command line utilities with similar functionalities as the blkzone utility and the sg3_utils command line tools. More information on how to compile and install libzbc as well as usage examples of the command line utilities provided can be found here .","title":"libzbc"},{"location":"getting-started/smr-disk/","text":"Getting Started with SMR Disks Hard disk drives using the Shingled Magnetic Recording technology can have different interface implementations resulting in different usage models. Drive Managed Interface SMR disks implementing this interface are, from the host kernel and applications point of view, identical to regular disks and do not need any particular attention. Most Linux\u00ae kernels will be able to handle these disks and no LBA space zoning information will be available to the host. Drive Managed disks should thus not be considered as zoned block devices. Zoned Block Interface SMR hard-disk drives implementing the ZBC and ZAC feature sets provide to the host commands allowing the indentification and control of the device zones. This interface has two different variations, or model implementations (See SMR Interface Implementations ). Host Aware While this zone model offers the convenience and flexibility of Drive Managed disks (i.e. random write capabilities), Host Aware disks support the full set of zone commands defined by the ZBC and ZAC standards. As such, Host Aware disks can support both the regular block device abstraction (regular disk) as well as the zoned block device abstraction. Host Managed This zone model defines a device type that is different from regular disk. Host-Managed disks can only be used as zoned block devices in order to satisfy the strong sequential write constraints the model defines. In the following sections, Host Aware disk models are considered as zoned block devices with similar characteristics as Host Managed drives, that is, sequential writes are assumed to be a constraint for the correct operation of the disk. Serial ATA ZAC Disks and SATA Host Controllers Serial ATA (SATA) host adapters, including those using the Advance Host Controller Interface (AHCI) standard, should have no problem scanning and initializing a connection with Host Aware disks. Most AHCI host adapters are also know to work with Host Managed disk drives as the adapter itself generally does not react to the device signature of the connected disk. Verifying The Disk For a system fullfilling all prerequisites , a SATA host Aware or Host Managed disk can simply be connected directly to a SATA port of the host controller. After booting the system, the disk should be listed by the lsscsi utility. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 In this example, the disk '/dev/sda' is the system boot disk and the disk '/dev/sdb' is being recognized as a ZBC disk. The second column of lsscsi output indicates the device type. The value zbc is always used for Host Managed ZBC and ZAC disks. This corresponds to the ZBC defined device type 0x14 for SAS disks and to the ZAC defined device signature 0xabcd for SATA disks. Since Host Aware disks have the same device type or device signature as regular disks, lsscsi will list host aware disks as disk . Note The lsscsi utility will not list a SATA ZAC disk with the type zac . The type zbc is always used as the kernel internally implements a SCSI to ATA translation layer (SAT), a;;owing ther representation of all SATA devices as SCSI devices. Checking The Disk Information The zone model of the disk can be verified through the zoned sysfs attribute. # cat /sys/block/sdb/queue/zoned host-managed The possible values of the zoned attribute are shown in the table below. Value Description none Regular disk or drive managed ZBC/ZAC disk host-aware Host aware ZBC/ZAC disk host-managed Host managed ZBC/ZAC disk The kernel messages also include useful information on the disk. # dmesg ahci 0000:00:11.5: version 3.0 ahci 0000:00:11.5: AHCI 0001.0301 32 slots 2 ports 6 Gbps 0x3 impl SATA mode ahci 0000:00:11.5: flags: 64bit ncq sntf led clo only pio slum part ems deso sadm sds apst ... scsi host5: ahci ata5: SATA max UDMA/133 abar m524288@0x9d100000 port 0x9d100200 irq 55 ata5: SATA link up 6.0 Gbps (SStatus 133 SControl 300) ata5.00: ATA-9: HGST HSH721414ALN6M0, L4GMT220, max UDMA/133 ata5.00: 27344764928 sectors, multi 0: LBA48 NCQ (depth 32), AA ata5.00: configured for UDMA/133 ... sd 5:0:0:0: Attached scsi generic sg1 type 20 sd 5:0:0:0: [sdb] Host-managed zoned block device sd 5:0:0:0: [sdb] 3662151680 4096-byte logical blocks: (15.0 TB/13.6 TiB) sd 5:0:0:0: [sdb] 55880 zones of 65536 logical blocks sd 5:0:0:0: [sdb] Write Protect is off sd 5:0:0:0: [sdb] Mode Sense: 00 3a 00 00 sd 5:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 5:0:0:0: [sdb] Attached SCSI disk ... Among other information, the zone model of the disk is confirmed as host managed. The total number of zones of the disk is also displayed. In this example, the disk capacity is 15 TB and has 55880 zones. The zone size of the disk can also be inspected through sysfs, with the attribute chunk_sectors . # cat /sys/block/sdb/queue/chunk_sectors 524288 The value is displayed as a number of 512B sectors, regardless of the actual logical and physical block size of the disk. In this example, the disk zone size is 524288 x 512 = 256 MiB . Starting with Linux kernel version 4.20.0, the sysfs attribute nr_zones is also available to verify the total number of zones of the disk. # cat /sys/block/sdb/queue/nr_zones 55880 Discovering The Disk Zone Configuration To obtain detailed information on the disk zone configuration, for instance the number of conventional zones available, the blkzone utility can be used. # blkzone report /dev/sdb start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010480000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, wptr 0x000008 reset:0 non-seq:0, zcond: 4(cl) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x6d2280000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] From the output, one can see that the 512B sector range from 0 up to 0x010600000 is divided into 524 conventional zones. The sector space starting from 0x010600000 until the last sector of the disk is devided into 55356 sequential write required zones. The zbc_report_zones of libzbc provides more detailed information in a more readable format. # zbc_report_zones /dev/sdb Device /dev/sdb: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors ... Zone 00521: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273154048, 524288 sectors Zone 00522: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273678336, 524288 sectors Zone 00523: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 274202624, 524288 sectors Zone 00524: type 0x2 (Sequential-write-required), cond 0x4 (Closed), reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726920 Zone 00525: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275251200 Zone 00526: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275775488, 524288 sectors, wp 275775488 ... Zone 55877: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29295640576, 524288 sectors, wp 29295640576 Zone 55878: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296164864, 524288 sectors, wp 29296164864 Zone 55879: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296689152, 524288 sectors, wp 29296689152 Using a SAS Host Bus Adapter AHCI adapters can only accomodate Serial ATA disks and generally only provide a limited number of ports. SAS Host Bus Adapters (HBA) are widely used in enterprise applications to overcome AHCI limitations. The SAS transport layer used by SAS HBAs can equally accomodate both Serial ATA and SCSI disks. HBA Compatibility While most AHCI adapters for Serial ATA disks generally do not cause any problem with host managed ZAC disks identification, SAS HBAs on the other hand may suffer from a lack of support depending on the HBA model being used. The compatibility of a SAS HBA with host managed disks mainly depends on the following factors. The HBA must have the ability to recognize the host managed device type 0x14 of host managed SAS disks (ZBC/SCSI). The HBA must have the ability to recognize the host managed device signature 0xabcd of SATA host managed ZAC disks and translate this signature into the ZBC defined SCSI device type 0x14 . Generalizing the previous point, the HBA must implement a SCSI-to-ATA translation (SAT) layer supporting the conversion of host issued ZBC zone commands into ZAC zone commands that can be executed by a SATA ZAC disk connected to the HBA. Any HBA failing the first requirement will not expose a ZBC host managed disk to the host. Similarly, an HBA failing to comply with the second and third requirement will fail to expose to the host a host managed ZAC disk as a ZBC host managed disk. In the case of a host aware disk model, the device type and device signature handling will not cause any problem (recall that host aware disks use the regular disk device type and signatur 0x00 ). Host aware disks will thus always be useable as regular disks with any HBA. The execution of ZBC zone commands with a SAS host aware disk may also work most of the time. However, similarly to host managed disk, the absence of a ZBC/ZAC compatible SAT layer will prevent the use of a Serial ATA host aware disk as a ZBC host aware disk. The ZBC zone commands sent to the SATA disk will not be translated and result in command failures. The compatibility of an HBA model with the ZBC and ZAC standards should be checked with the HBA vendor. Under some conditions, an HBA compatibility can also be checked using the libzbc conformance test suite . Verifying The Disk Assuming that a compatible HBA is being used, after connecting the drive and eventually rebooting the system (most SAS HBAs have plug-and-play features in which case rebooting the system after connecting or disconnecting a disk is not necessary), verifying the disk identification and checking the disk parameters and zone configuration can be done in the exact same manner as with Serial ATA disks as discussed above. In these examples, /dev/sdc is a SAS disk connected to a SAS HBA and /dev/sdd is a SATA disk connected to the same HBA. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3 Inspecting the kernel messages shows no differences between the initialization of the SAS drive and the SATA disk. # dmesg ... scsi 10:0:2:0: Direct-Access-ZBC HGST HSH721414AL52M0 a220 PQ: 0 ANSI: 7 scsi 10:0:2:0: SSP: handle(0x001b), sas_addr(0x5000cca0000025c5), phy(1), device_name(0x5000cca0000025c7) scsi 10:0:2:0: enclosure logical id (0x500062b200f35d40), slot(2) scsi 10:0:2:0: enclosure level(0x0000), connector name( ) sd 10:0:2:0: Attached scsi generic sg2 type 20 sd 10:0:2:0: [sdc] Host-managed zoned block device sd 10:0:2:0: [sdc] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB) sd 10:0:2:0: [sdc] 4096-byte physical blocks sd 10:0:2:0: [sdc] 52156 zones of 524288 logical blocks sd 10:0:2:0: [sdc] Write Protect is off sd 10:0:2:0: [sdc] Mode Sense: f7 00 10 08 sd 10:0:2:0: [sdc] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 10:0:2:0: [sdc] Attached SCSI disk ... scsi 10:0:3:0: Direct-Access-ZBC ATA HGST HSH721415AL T220 PQ: 0 ANSI: 6 scsi 10:0:3:0: SATA: handle(0x001c), sas_addr(0x300062b200f35d43), phy(3), device_name(0x5000cca25bc2e26f) scsi 10:0:3:0: enclosure logical id (0x500062b200f35d40), slot(1) scsi 10:0:3:0: enclosure level(0x0000), connector name( ) scsi 10:0:3:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y) sd 10:0:3:0: Attached scsi generic sg3 type 20 sd 10:0:3:0: [sdd] Host-managed zoned block device sd 10:0:3:0: [sdd] 3662151680 4096-byte logical blocks: (15.0 TB/13.6 TiB) sd 10:0:3:0: [sdd] 55880 zones of 65536 logical blocks sd 10:0:3:0: [sdd] Write Protect is off sd 10:0:3:0: [sdd] Mode Sense: 9b 00 10 08 sd 10:0:3:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 10:0:3:0: [sdd] Attached SCSI disk ... Both disks are identified by the kernel as Direct-Access-ZBC devices indicating that the HBA is correctly translating the ZAC host managed device signature into a ZBC host managed device type.","title":"Getting Started With SMR Disks"},{"location":"getting-started/smr-disk/#getting-started-with-smr-disks","text":"Hard disk drives using the Shingled Magnetic Recording technology can have different interface implementations resulting in different usage models. Drive Managed Interface SMR disks implementing this interface are, from the host kernel and applications point of view, identical to regular disks and do not need any particular attention. Most Linux\u00ae kernels will be able to handle these disks and no LBA space zoning information will be available to the host. Drive Managed disks should thus not be considered as zoned block devices. Zoned Block Interface SMR hard-disk drives implementing the ZBC and ZAC feature sets provide to the host commands allowing the indentification and control of the device zones. This interface has two different variations, or model implementations (See SMR Interface Implementations ). Host Aware While this zone model offers the convenience and flexibility of Drive Managed disks (i.e. random write capabilities), Host Aware disks support the full set of zone commands defined by the ZBC and ZAC standards. As such, Host Aware disks can support both the regular block device abstraction (regular disk) as well as the zoned block device abstraction. Host Managed This zone model defines a device type that is different from regular disk. Host-Managed disks can only be used as zoned block devices in order to satisfy the strong sequential write constraints the model defines. In the following sections, Host Aware disk models are considered as zoned block devices with similar characteristics as Host Managed drives, that is, sequential writes are assumed to be a constraint for the correct operation of the disk.","title":"Getting Started with SMR Disks"},{"location":"getting-started/smr-disk/#serial-ata-zac-disks-and-sata-host-controllers","text":"Serial ATA (SATA) host adapters, including those using the Advance Host Controller Interface (AHCI) standard, should have no problem scanning and initializing a connection with Host Aware disks. Most AHCI host adapters are also know to work with Host Managed disk drives as the adapter itself generally does not react to the device signature of the connected disk.","title":"Serial ATA ZAC Disks and SATA Host Controllers"},{"location":"getting-started/smr-disk/#verifying-the-disk","text":"For a system fullfilling all prerequisites , a SATA host Aware or Host Managed disk can simply be connected directly to a SATA port of the host controller. After booting the system, the disk should be listed by the lsscsi utility. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 In this example, the disk '/dev/sda' is the system boot disk and the disk '/dev/sdb' is being recognized as a ZBC disk. The second column of lsscsi output indicates the device type. The value zbc is always used for Host Managed ZBC and ZAC disks. This corresponds to the ZBC defined device type 0x14 for SAS disks and to the ZAC defined device signature 0xabcd for SATA disks. Since Host Aware disks have the same device type or device signature as regular disks, lsscsi will list host aware disks as disk . Note The lsscsi utility will not list a SATA ZAC disk with the type zac . The type zbc is always used as the kernel internally implements a SCSI to ATA translation layer (SAT), a;;owing ther representation of all SATA devices as SCSI devices.","title":"Verifying The Disk"},{"location":"getting-started/smr-disk/#checking-the-disk-information","text":"The zone model of the disk can be verified through the zoned sysfs attribute. # cat /sys/block/sdb/queue/zoned host-managed The possible values of the zoned attribute are shown in the table below. Value Description none Regular disk or drive managed ZBC/ZAC disk host-aware Host aware ZBC/ZAC disk host-managed Host managed ZBC/ZAC disk The kernel messages also include useful information on the disk. # dmesg ahci 0000:00:11.5: version 3.0 ahci 0000:00:11.5: AHCI 0001.0301 32 slots 2 ports 6 Gbps 0x3 impl SATA mode ahci 0000:00:11.5: flags: 64bit ncq sntf led clo only pio slum part ems deso sadm sds apst ... scsi host5: ahci ata5: SATA max UDMA/133 abar m524288@0x9d100000 port 0x9d100200 irq 55 ata5: SATA link up 6.0 Gbps (SStatus 133 SControl 300) ata5.00: ATA-9: HGST HSH721414ALN6M0, L4GMT220, max UDMA/133 ata5.00: 27344764928 sectors, multi 0: LBA48 NCQ (depth 32), AA ata5.00: configured for UDMA/133 ... sd 5:0:0:0: Attached scsi generic sg1 type 20 sd 5:0:0:0: [sdb] Host-managed zoned block device sd 5:0:0:0: [sdb] 3662151680 4096-byte logical blocks: (15.0 TB/13.6 TiB) sd 5:0:0:0: [sdb] 55880 zones of 65536 logical blocks sd 5:0:0:0: [sdb] Write Protect is off sd 5:0:0:0: [sdb] Mode Sense: 00 3a 00 00 sd 5:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 5:0:0:0: [sdb] Attached SCSI disk ... Among other information, the zone model of the disk is confirmed as host managed. The total number of zones of the disk is also displayed. In this example, the disk capacity is 15 TB and has 55880 zones. The zone size of the disk can also be inspected through sysfs, with the attribute chunk_sectors . # cat /sys/block/sdb/queue/chunk_sectors 524288 The value is displayed as a number of 512B sectors, regardless of the actual logical and physical block size of the disk. In this example, the disk zone size is 524288 x 512 = 256 MiB . Starting with Linux kernel version 4.20.0, the sysfs attribute nr_zones is also available to verify the total number of zones of the disk. # cat /sys/block/sdb/queue/nr_zones 55880","title":"Checking The Disk Information"},{"location":"getting-started/smr-disk/#discovering-the-disk-zone-configuration","text":"To obtain detailed information on the disk zone configuration, for instance the number of conventional zones available, the blkzone utility can be used. # blkzone report /dev/sdb start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010480000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, wptr 0x000008 reset:0 non-seq:0, zcond: 4(cl) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x6d2280000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] From the output, one can see that the 512B sector range from 0 up to 0x010600000 is divided into 524 conventional zones. The sector space starting from 0x010600000 until the last sector of the disk is devided into 55356 sequential write required zones. The zbc_report_zones of libzbc provides more detailed information in a more readable format. # zbc_report_zones /dev/sdb Device /dev/sdb: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors ... Zone 00521: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273154048, 524288 sectors Zone 00522: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273678336, 524288 sectors Zone 00523: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 274202624, 524288 sectors Zone 00524: type 0x2 (Sequential-write-required), cond 0x4 (Closed), reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726920 Zone 00525: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275251200 Zone 00526: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275775488, 524288 sectors, wp 275775488 ... Zone 55877: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29295640576, 524288 sectors, wp 29295640576 Zone 55878: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296164864, 524288 sectors, wp 29296164864 Zone 55879: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296689152, 524288 sectors, wp 29296689152","title":"Discovering The Disk Zone Configuration"},{"location":"getting-started/smr-disk/#using-a-sas-host-bus-adapter","text":"AHCI adapters can only accomodate Serial ATA disks and generally only provide a limited number of ports. SAS Host Bus Adapters (HBA) are widely used in enterprise applications to overcome AHCI limitations. The SAS transport layer used by SAS HBAs can equally accomodate both Serial ATA and SCSI disks.","title":"Using a SAS Host Bus Adapter"},{"location":"getting-started/smr-disk/#hba-compatibility","text":"While most AHCI adapters for Serial ATA disks generally do not cause any problem with host managed ZAC disks identification, SAS HBAs on the other hand may suffer from a lack of support depending on the HBA model being used. The compatibility of a SAS HBA with host managed disks mainly depends on the following factors. The HBA must have the ability to recognize the host managed device type 0x14 of host managed SAS disks (ZBC/SCSI). The HBA must have the ability to recognize the host managed device signature 0xabcd of SATA host managed ZAC disks and translate this signature into the ZBC defined SCSI device type 0x14 . Generalizing the previous point, the HBA must implement a SCSI-to-ATA translation (SAT) layer supporting the conversion of host issued ZBC zone commands into ZAC zone commands that can be executed by a SATA ZAC disk connected to the HBA. Any HBA failing the first requirement will not expose a ZBC host managed disk to the host. Similarly, an HBA failing to comply with the second and third requirement will fail to expose to the host a host managed ZAC disk as a ZBC host managed disk. In the case of a host aware disk model, the device type and device signature handling will not cause any problem (recall that host aware disks use the regular disk device type and signatur 0x00 ). Host aware disks will thus always be useable as regular disks with any HBA. The execution of ZBC zone commands with a SAS host aware disk may also work most of the time. However, similarly to host managed disk, the absence of a ZBC/ZAC compatible SAT layer will prevent the use of a Serial ATA host aware disk as a ZBC host aware disk. The ZBC zone commands sent to the SATA disk will not be translated and result in command failures. The compatibility of an HBA model with the ZBC and ZAC standards should be checked with the HBA vendor. Under some conditions, an HBA compatibility can also be checked using the libzbc conformance test suite .","title":"HBA Compatibility"},{"location":"getting-started/smr-disk/#verifying-the-disk_1","text":"Assuming that a compatible HBA is being used, after connecting the drive and eventually rebooting the system (most SAS HBAs have plug-and-play features in which case rebooting the system after connecting or disconnecting a disk is not necessary), verifying the disk identification and checking the disk parameters and zone configuration can be done in the exact same manner as with Serial ATA disks as discussed above. In these examples, /dev/sdc is a SAS disk connected to a SAS HBA and /dev/sdd is a SATA disk connected to the same HBA. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3 Inspecting the kernel messages shows no differences between the initialization of the SAS drive and the SATA disk. # dmesg ... scsi 10:0:2:0: Direct-Access-ZBC HGST HSH721414AL52M0 a220 PQ: 0 ANSI: 7 scsi 10:0:2:0: SSP: handle(0x001b), sas_addr(0x5000cca0000025c5), phy(1), device_name(0x5000cca0000025c7) scsi 10:0:2:0: enclosure logical id (0x500062b200f35d40), slot(2) scsi 10:0:2:0: enclosure level(0x0000), connector name( ) sd 10:0:2:0: Attached scsi generic sg2 type 20 sd 10:0:2:0: [sdc] Host-managed zoned block device sd 10:0:2:0: [sdc] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB) sd 10:0:2:0: [sdc] 4096-byte physical blocks sd 10:0:2:0: [sdc] 52156 zones of 524288 logical blocks sd 10:0:2:0: [sdc] Write Protect is off sd 10:0:2:0: [sdc] Mode Sense: f7 00 10 08 sd 10:0:2:0: [sdc] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 10:0:2:0: [sdc] Attached SCSI disk ... scsi 10:0:3:0: Direct-Access-ZBC ATA HGST HSH721415AL T220 PQ: 0 ANSI: 6 scsi 10:0:3:0: SATA: handle(0x001c), sas_addr(0x300062b200f35d43), phy(3), device_name(0x5000cca25bc2e26f) scsi 10:0:3:0: enclosure logical id (0x500062b200f35d40), slot(1) scsi 10:0:3:0: enclosure level(0x0000), connector name( ) scsi 10:0:3:0: atapi(n), ncq(y), asyn_notify(n), smart(y), fua(y), sw_preserve(y) sd 10:0:3:0: Attached scsi generic sg3 type 20 sd 10:0:3:0: [sdd] Host-managed zoned block device sd 10:0:3:0: [sdd] 3662151680 4096-byte logical blocks: (15.0 TB/13.6 TiB) sd 10:0:3:0: [sdd] 55880 zones of 65536 logical blocks sd 10:0:3:0: [sdd] Write Protect is off sd 10:0:3:0: [sdd] Mode Sense: 9b 00 10 08 sd 10:0:3:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 10:0:3:0: [sdd] Attached SCSI disk ... Both disks are identified by the kernel as Direct-Access-ZBC devices indicating that the HBA is correctly translating the ZAC host managed device signature into a ZBC host managed device type.","title":"Verifying The Disk"},{"location":"getting-started/smr-emulation/","text":"Getting Started with Emulated SMR Disks For users without access to ZBC or ZAC disks, application development and kernel tests are possible using emulated ZBC disks. Several methods exist. null_blk : The null_blk kernel driver allows emulating zoned block devices. This method is discussed in more details here . tcmu-runner : This scsi device emulation application allows the emulation of both host aware or host managed ZBC SCSI disks with a storage backstore using a regular file. For all purposes and intent, the disks created using tcmu-runner operate exactly like physical disks. scsi_debug : The scsi_debug kernel driver can be configured to emulate host aware and host managed ZBC disks that appear to the kernel and application exactly as a real disk would. tcmu-runner Detailed information on how to install and operate tcmu-runner can be found here . tcmu-runner ZBC File Handler The ZBC file handler is tcmu-runner internal handler implementing the emulation of a ZBC SCSI disk using a file as a backstore. tcmu-runner infrastructure connects the emulated disk to a virtual HBA implemented as a kernel driver. This structure results in an identical command path for the emulated disk as for physical disks. Applications and kernel components will not perceive any difference. This section describes in more details the options available to create an emulated disk such as the disk zone model, the disk zone size, the disk capacity and the number of conventional zones of the disk. The following example shows how to create a small 20 GB host managed ZBC disk with 10 conventional zones and a 256 MiB zone size, with the emulated disk capacity stored in the file /var/local/zbc0.raw . # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /loopback /loopback> create Created target naa.500140529100d742. /loopback> cd naa.500140529100d742/luns /loopback/naa...9100d742/luns> create /backstores/user:zbc/zbc0 0 Created LUN 0. /loopback/naa...9100d742/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:poma .......................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 1] | o- naa.500140529100d742 ............................. [naa.50014059e05d5424] | o- luns ........................................................ [LUNs: 1] | o- lun0 ................................. [user/zbc0 (default_tg_pt_gp)] o- vhost ........................................................ [Targets: 0] /> exit Verifying The Emulated Disk Verifying the emulated disk identification, its parameters and its zone configuration can be done in the exact same manner as with Serial ATA disk and SAS disks, as discussed in the Getting started with an SMR disk chapter. Identification of the emulated disk is facilitated by looking at the disk vendor ID displayed by the lsscsi utility. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [11:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sdc /dev/sg2 The emulated disk is listed with the device vendor name \"LIO-ORG\" and the device model name \"TCMU ZBC device\". Similarly to physical ZBC or ZAC disks, the kernel messages will show the drive being identified and initialized. # dmesg ... scsi host11: TCM_Loopback scsi 11:0:1:0: Direct-Access-ZBC LIO-ORG TCMU ZBC device 0002 PQ: 0 ANSI: 5 sd 11:0:1:0: Attached scsi generic sg2 type 20 sd 11:0:1:0: [sdc] Host-managed zoned block device sd 11:0:1:0: [sdc] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB) sd 11:0:1:0: [sdc] 80 zones of 524288 logical blocks sd 11:0:1:0: [sdc] Write Protect is off sd 11:0:1:0: [sdc] Mode Sense: 0f 00 00 00 sd 11:0:1:0: [sdc] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 11:0:1:0: [sdc] Optimal transfer size 65536 bytes sd 11:0:1:0: [sdc] Attached SCSI disk ... The emulated disk is identified by the kernel exactly like a physical SAS host managed disk, that is, with a device type Direct-Access-ZBC . The emulated disk can now be used in the same manner as any physical disk. For instance, the blkzone or zbc_report_zones utilities can be used to inspect the disk zone configuration. # zbc_report_zones /dev/sdc Device /dev/sdc: Vendor ID: LIO-ORG TCMU ZBC device 0002 Zoned block device interface, Host-managed zone model 41943040 512-bytes sectors 41943040 logical blocks of 512 B 41943040 physical blocks of 512 B 21.475 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 35 80 zones from 0, reporting option 0x00 80 / 80 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors Zone 00003: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1572864, 524288 sectors Zone 00004: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 2097152, 524288 sectors Zone 00005: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 2621440, 524288 sectors Zone 00006: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 3145728, 524288 sectors Zone 00007: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 3670016, 524288 sectors Zone 00008: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4194304, 524288 sectors Zone 00009: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4718592, 524288 sectors Zone 00010: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 5242880, 524288 sectors, wp 5242880 Zone 00011: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 5767168, 524288 sectors, wp 5767168 ... Zone 00078: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 40894464, 524288 sectors, wp 40894464 Zone 00079: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 41418752, 524288 sectors, wp 41418752 scsi_debug The scsi_debug kernel module can be used to create emulated ZBC SCSI disks with memory backing used to store data written to sectors. Since memory is used as a backing store, creating large disks requires a host with a large amount of DRAM. Furthermore, the data written to the emulated device does not survive the device destruction or a host reboot. Creating an Emulated ZBC Disk scsi_debug ZBC disks can be created using modprobe with arguments. Below is an example to create a 16GiB capacity host managed ZBC disk with 64MiB zones and 32 conventional zones. # modprobe scsi_debug \\ max_luns=1 \\ sector_size=4096 \\ dev_size_mb=16384 \\ zbc=managed \\ zone_size_mb=64 \\ zone_nr_conv=32 The disk created can be seen using the lsscsi command. # lsscsi -g ... [11:0:0:0] zbc Linux scsi_debug 0190 /dev/sdj /dev/sg9 ... The vendor field of the disk created is set to \"scsi_debug\". The kernel messages also show this disk coming online. # dmesg ... scsi_debug:sdebug_driver_probe: scsi_debug: trim poll_queues to 0. poll_q/nr_hw = (0/1) scsi host11: scsi_debug: version 0190 [20200710] dev_size_mb=16384, opts=0x0, submit_queues=1, statistics=0 scsi 11:0:0:0: Direct-Access-ZBC Linux scsi_debug 0190 PQ: 0 ANSI: 7 sd 11:0:0:0: Power-on or device reset occurred sd 11:0:0:0: Attached scsi generic sg9 type 20 sd 11:0:0:0: [sdj] Host-managed zoned block device sd 11:0:0:0: [sdj] 4194304 4096-byte logical blocks: (17.2 GB/16.0 GiB) sd 11:0:0:0: [sdj] Write Protect is off sd 11:0:0:0: [sdj] Mode Sense: 5b 00 10 08 sd 11:0:0:0: [sdj] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 11:0:0:0: [sdj] Optimal transfer size 4194304 bytes sd 11:0:0:0: [sdj] 256 zones of 16384 logical blocks sd 11:0:0:0: [sdj] Attached SCSI disk Verifying The Emulated Disk The zone configuration of the emulated disk can be inspected using libzbc , sg3utils and util-linux tools. For instance, using zbc_report_zones : # zbc_report_zones /dev/sg9 Device /dev/sg9: Vendor ID: Linux scsi_debug 0190 SCSI ZBC device interface, Host-managed zone model 33554432 512-bytes sectors 4194304 logical blocks of 4096 B 4194304 physical blocks of 4096 B 17.180 GB capacity Read commands are unrestricted 4096 KiB max R/W size Maximum number of open sequential write required zones: 8 256 zones from 0, reporting option 0x00 256 / 256 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 131072 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 131072, 131072 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 262144, 131072 sectors ... Zone 00031: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4063232, 131072 sectors Zone 00032: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 4194304, 131072 sectors, wp 4194304 Zone 00033: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 4325376, 131072 sectors, wp 4325376 ... Zone 00254: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 33292288, 131072 sectors, wp 33292288 Zone 00255: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 33423360, 131072 sectors, wp 33423360 blkzone , the same information is obtained and displayed in a different format. # blkzone report /dev/sdj start: 0x000000000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000020000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000040000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x0003e0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000400000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000420000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x001fc0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x001fe0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)]","title":"Getting Started With Emulated SMR Disks"},{"location":"getting-started/smr-emulation/#getting-started-with-emulated-smr-disks","text":"For users without access to ZBC or ZAC disks, application development and kernel tests are possible using emulated ZBC disks. Several methods exist. null_blk : The null_blk kernel driver allows emulating zoned block devices. This method is discussed in more details here . tcmu-runner : This scsi device emulation application allows the emulation of both host aware or host managed ZBC SCSI disks with a storage backstore using a regular file. For all purposes and intent, the disks created using tcmu-runner operate exactly like physical disks. scsi_debug : The scsi_debug kernel driver can be configured to emulate host aware and host managed ZBC disks that appear to the kernel and application exactly as a real disk would.","title":"Getting Started with Emulated SMR Disks"},{"location":"getting-started/smr-emulation/#tcmu-runner","text":"Detailed information on how to install and operate tcmu-runner can be found here .","title":"tcmu-runner"},{"location":"getting-started/smr-emulation/#tcmu-runner-zbc-file-handler","text":"The ZBC file handler is tcmu-runner internal handler implementing the emulation of a ZBC SCSI disk using a file as a backstore. tcmu-runner infrastructure connects the emulated disk to a virtual HBA implemented as a kernel driver. This structure results in an identical command path for the emulated disk as for physical disks. Applications and kernel components will not perceive any difference. This section describes in more details the options available to create an emulated disk such as the disk zone model, the disk zone size, the disk capacity and the number of conventional zones of the disk. The following example shows how to create a small 20 GB host managed ZBC disk with 10 conventional zones and a 256 MiB zone size, with the emulated disk capacity stored in the file /var/local/zbc0.raw . # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /loopback /loopback> create Created target naa.500140529100d742. /loopback> cd naa.500140529100d742/luns /loopback/naa...9100d742/luns> create /backstores/user:zbc/zbc0 0 Created LUN 0. /loopback/naa...9100d742/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:poma .......................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 1] | o- naa.500140529100d742 ............................. [naa.50014059e05d5424] | o- luns ........................................................ [LUNs: 1] | o- lun0 ................................. [user/zbc0 (default_tg_pt_gp)] o- vhost ........................................................ [Targets: 0] /> exit","title":"tcmu-runner ZBC File Handler"},{"location":"getting-started/smr-emulation/#verifying-the-emulated-disk","text":"Verifying the emulated disk identification, its parameters and its zone configuration can be done in the exact same manner as with Serial ATA disk and SAS disks, as discussed in the Getting started with an SMR disk chapter. Identification of the emulated disk is facilitated by looking at the disk vendor ID displayed by the lsscsi utility. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [11:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sdc /dev/sg2 The emulated disk is listed with the device vendor name \"LIO-ORG\" and the device model name \"TCMU ZBC device\". Similarly to physical ZBC or ZAC disks, the kernel messages will show the drive being identified and initialized. # dmesg ... scsi host11: TCM_Loopback scsi 11:0:1:0: Direct-Access-ZBC LIO-ORG TCMU ZBC device 0002 PQ: 0 ANSI: 5 sd 11:0:1:0: Attached scsi generic sg2 type 20 sd 11:0:1:0: [sdc] Host-managed zoned block device sd 11:0:1:0: [sdc] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB) sd 11:0:1:0: [sdc] 80 zones of 524288 logical blocks sd 11:0:1:0: [sdc] Write Protect is off sd 11:0:1:0: [sdc] Mode Sense: 0f 00 00 00 sd 11:0:1:0: [sdc] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 11:0:1:0: [sdc] Optimal transfer size 65536 bytes sd 11:0:1:0: [sdc] Attached SCSI disk ... The emulated disk is identified by the kernel exactly like a physical SAS host managed disk, that is, with a device type Direct-Access-ZBC . The emulated disk can now be used in the same manner as any physical disk. For instance, the blkzone or zbc_report_zones utilities can be used to inspect the disk zone configuration. # zbc_report_zones /dev/sdc Device /dev/sdc: Vendor ID: LIO-ORG TCMU ZBC device 0002 Zoned block device interface, Host-managed zone model 41943040 512-bytes sectors 41943040 logical blocks of 512 B 41943040 physical blocks of 512 B 21.475 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 35 80 zones from 0, reporting option 0x00 80 / 80 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors Zone 00003: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1572864, 524288 sectors Zone 00004: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 2097152, 524288 sectors Zone 00005: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 2621440, 524288 sectors Zone 00006: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 3145728, 524288 sectors Zone 00007: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 3670016, 524288 sectors Zone 00008: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4194304, 524288 sectors Zone 00009: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4718592, 524288 sectors Zone 00010: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 5242880, 524288 sectors, wp 5242880 Zone 00011: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 5767168, 524288 sectors, wp 5767168 ... Zone 00078: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 40894464, 524288 sectors, wp 40894464 Zone 00079: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 41418752, 524288 sectors, wp 41418752","title":"Verifying The Emulated Disk"},{"location":"getting-started/smr-emulation/#scsi_debug","text":"The scsi_debug kernel module can be used to create emulated ZBC SCSI disks with memory backing used to store data written to sectors. Since memory is used as a backing store, creating large disks requires a host with a large amount of DRAM. Furthermore, the data written to the emulated device does not survive the device destruction or a host reboot.","title":"scsi_debug"},{"location":"getting-started/smr-emulation/#creating-an-emulated-zbc-disk","text":"scsi_debug ZBC disks can be created using modprobe with arguments. Below is an example to create a 16GiB capacity host managed ZBC disk with 64MiB zones and 32 conventional zones. # modprobe scsi_debug \\ max_luns=1 \\ sector_size=4096 \\ dev_size_mb=16384 \\ zbc=managed \\ zone_size_mb=64 \\ zone_nr_conv=32 The disk created can be seen using the lsscsi command. # lsscsi -g ... [11:0:0:0] zbc Linux scsi_debug 0190 /dev/sdj /dev/sg9 ... The vendor field of the disk created is set to \"scsi_debug\". The kernel messages also show this disk coming online. # dmesg ... scsi_debug:sdebug_driver_probe: scsi_debug: trim poll_queues to 0. poll_q/nr_hw = (0/1) scsi host11: scsi_debug: version 0190 [20200710] dev_size_mb=16384, opts=0x0, submit_queues=1, statistics=0 scsi 11:0:0:0: Direct-Access-ZBC Linux scsi_debug 0190 PQ: 0 ANSI: 7 sd 11:0:0:0: Power-on or device reset occurred sd 11:0:0:0: Attached scsi generic sg9 type 20 sd 11:0:0:0: [sdj] Host-managed zoned block device sd 11:0:0:0: [sdj] 4194304 4096-byte logical blocks: (17.2 GB/16.0 GiB) sd 11:0:0:0: [sdj] Write Protect is off sd 11:0:0:0: [sdj] Mode Sense: 5b 00 10 08 sd 11:0:0:0: [sdj] Write cache: enabled, read cache: enabled, supports DPO and FUA sd 11:0:0:0: [sdj] Optimal transfer size 4194304 bytes sd 11:0:0:0: [sdj] 256 zones of 16384 logical blocks sd 11:0:0:0: [sdj] Attached SCSI disk","title":"Creating an Emulated ZBC Disk"},{"location":"getting-started/smr-emulation/#verifying-the-emulated-disk_1","text":"The zone configuration of the emulated disk can be inspected using libzbc , sg3utils and util-linux tools. For instance, using zbc_report_zones : # zbc_report_zones /dev/sg9 Device /dev/sg9: Vendor ID: Linux scsi_debug 0190 SCSI ZBC device interface, Host-managed zone model 33554432 512-bytes sectors 4194304 logical blocks of 4096 B 4194304 physical blocks of 4096 B 17.180 GB capacity Read commands are unrestricted 4096 KiB max R/W size Maximum number of open sequential write required zones: 8 256 zones from 0, reporting option 0x00 256 / 256 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 131072 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 131072, 131072 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 262144, 131072 sectors ... Zone 00031: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 4063232, 131072 sectors Zone 00032: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 4194304, 131072 sectors, wp 4194304 Zone 00033: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 4325376, 131072 sectors, wp 4325376 ... Zone 00254: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 33292288, 131072 sectors, wp 33292288 Zone 00255: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 33423360, 131072 sectors, wp 33423360 blkzone , the same information is obtained and displayed in a different format. # blkzone report /dev/sdj start: 0x000000000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000020000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000040000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x0003e0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000400000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000420000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x001fc0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x001fe0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)]","title":"Verifying The Emulated Disk"},{"location":"getting-started/zns-emulation/","text":"Getting Started with Emulated NVMe ZNS Devices For users without access to NVMe ZNS devices, application development and kernel tests are possible using emulated devices. Two methods exist. null_blk : The null_blk kernel driver allows emulating zoned block devices with a zone configuration compatible with the real NVMe ZNS devices. This method is discussed in more details here . QEMU : This open source machine emulator and virtualizer allows for the creation of emulated NVMe devices with a regular file used as a backstore on the host. Recent versions of QEMU also support the emulation of Zoned Namespaces. QEMU QEMU supports the emulation of NVMe namespaces since version 1.6. However, the emulation of zoned namespaces requires the more recent version 6.0 or later of QEMU . If the host Linux distribution does not provide QEMU version 6.0 or or above, QEMU has to be compiled from source. Detailed information on how to compile and install QEMU from source can be found here . Creating an Emulated Zoned Namespace The creation of an emulated zoned namespace requires first a backing store file for the namespace. The size of the file determines the capacity of the namespace that will be seen from the guest OS running in the QEMU virtual machine. For example, to create a 32GiB zoned namespace, a 32 GiB file on the host must first be created. This can be done simply using the truncate command to create a sparse file or the dd command to create a fully allocated file. # truncate -s 32G /var/lib/qemu/images/zns.raw # ls -l /var/lib/qemu/images/zns.raw -rw-r--r-- 1 root root 34359738368 Jun 21 15:13 /var/lib/qemu/images/zns.raw Or using dd: # dd if=/dev/zero of=/var/lib/qemu/images/zns.raw bs=1M count=32768 32768+0 records in 32768+0 records out 34359738368 bytes (34 GB, 32 GiB) copied, 11.4072 s, 3.0 GB/s # ls -l /var/lib/qemu/images/zns.raw -rw-r--r-- 1 root root 34359738368 Jun 22 11:22 /var/lib/qemu/images/zns.raw Next, QEMU can be executed with additional command lint options and arguments to request the creation of a zoned namespace using the backstore file as storage. In the following example, the backing store file is used to emulate a zoned namespace with zones of 64 MiB and a zone capacity of 62 MiB. The namespace block size is 4096 B. The namespace is set to allow at most 16 open zones and 32 active zones. # /usr/local/bin/qemu-system-x86_64 \\ ... -device nvme,id=nvme0,serial=deadbeef,zoned.zasl=5 \\ -drive file=${znsimg},id=nvmezns0,format=raw,if=none \\ -device nvme-ns,drive=nvmezns0,bus=nvme0,nsid=1,logical_block_size=4096,\\ physical_block_size=4096,zoned=true,zoned.zone_size=64M,zoned.\\ zone_capacity=62M,zoned.max_open=16,zoned.max_active=32,\\ uuid=5e40ec5f-eeb6-4317-bc5e-c919796a5f79 ... Verifying an Emulated Zoned Namespace When running a Linux distribution as the guest operating system, with a kernel version higher that 5.9.0, the emulated NVMe ZNS device can be checked using the nvme command (see Linux Tools for ZNS . # nvme list Node SN Model Namespace Usage Format FW Rev ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- -------- /dev/nvme0n1 deadbeef QEMU NVMe Ctrl 1 34.36 GB / 34.36 GB 4 KiB + 0 B 1.0 The lsscsi utility will also show the emulated NVMe device. # lsscsi -g [2:0:0:0] cd/dvd QEMU QEMU DVD-ROM 2.5+ /dev/sr0 /dev/sg0 [N:0:0:1] disk QEMU NVMe Ctrl__1 /dev/nvme0n1 - Using the blkzone utility, the namespace zone configuration can be inspected. # blkzone report /dev/nvme0n1 | less start: 0x000000000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000020000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000040000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000060000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000080000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x003f80000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fa0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fc0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fe0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] Of note is that the total number of zones of the namespace directly depends on the size of the backstore file used and on the zone size configured. In the above example, the emulated namespace has 512 zones (32 GiB / 64 MiB). # cat /sys/block/nvme0n1/queue/nr_zones 512 If the emulated namespace is configured with a zone capacity smaller than the zone size, the entire capacity defined by the backstore file will not be usable. The effective usable capacity can be reported using blkzone with the capacity command. # blkzone capacity /dev/nvme0n1 0x003e00000 In this case, the namespace effective storage capacity is 0x003e00000 (65011712) 512-Bytes sectors, equivalent to 512 zones of 62 MiB capacity. Using an Emulated Zoned Namespace The behavior of a QEMU emulated NVMe ZNS device is fully compliant with the NVMe ZNS specifications, with one exception: the state of namespace zones is not persistent across restart of the QEMU emulation. The state of zones is preserved only as long as QEMU executes, even if the guest operating system is rebooted. If QEMU is restarted using the same backstore file, the guest operating system will see the namespace with all zones in the empty state. Emulated Zoned Namespace Options The implementation of NVMe device and ZNS namespace emulation in QEMU provides several configuration options to control the characteristics of the device. The full list of options and parameters is documented here . The options and parameters related to Zoned Namespaces are as follows. Option Default Value Description zoned.zasl=UINT32 0 Zone Append Size Limit. If left at the default (0), the zone append size limit will be equal to the maximum data transfer size (MDTS). Otherwise, the zone append size limit is equal to 2 to the power of zasl multiplied by the minimum memory page size (4096 B), but cannot exceed the maximum data transfer size. zoned.zone_size= SIZE 128MiB Define the zone size (ZSZE) zoned.zone_capacity= SIZE 0 Define the zone capacity (ZCAP). If left at the default (0), the zone capacity will equal the zone size. zoned.descr_ext_size= UINT32 0 Set the Zone Descriptor Extension Size (ZDES). Must be a multiple of 64 bytes. zoned.cross_read= BOOL off Set to on to allow reads to cross zone boundaries. zoned.max_active= UINT32 0 Set the maximum number of active resources (MAR). The default (0) allows all zones to be active. zoned.max_open= UINT32 0 Set the maximum number of open resources (MOR). The default (0) allows all zones to be open. If zoned.max_active is specified, this value must be less than or equal to that. QEMU Execution Example The following script execute QEMU to run a virtual machine with 8 CPU cores, 16 GiB of memory and bridged networking. The bridge device virbr0 is assumed to already exist. The last device added to the virtual machine on the QEMU command line is a 32 GiB NVMe ZNS device. #!/bin/sh # # Some variables # bridge=\"virbr0\" vmimg=\"/var/lib/qemu/boot-disk.qcow2\" znsimg=\"/var/lib/qemu/zns.raw\" # # Run QEMU # sudo /usr/local/bin/qemu-system-x86_64 \\ -name guest=FedoraServer34,debug-threads=on \\ -machine pc-q35-5.2,accel=kvm,usb=off,vmport=off,dump-guest-core=off,memory-backend=pc.ram \\ -cpu EPYC-Rome,x2apic=on,tsc-deadline=on,hypervisor=on,tsc-adjust=on,stibp=on,arch-capabilities=on,ssbd=on,xsaves=on,cmp-legacy=on,amd-ssbd=on,virt-ssbd=on,rdctl-no=on,skip-l1dfl-vmentry=on,mds-no=on,pschange-mc-no=on \\ -m 16384 \\ -object memory-backend-ram,id=pc.ram,size=17179869184 \\ -overcommit mem-lock=off \\ -smp 8,sockets=8,cores=1,threads=1 \\ -uuid e629893b-a9fd-48aa-9505-f32503ab3405 \\ -rtc base=utc,driftfix=slew \\ -global kvm-pit.lost_tick_policy=delay \\ -nographic \\ -no-hpet \\ -global ICH9-LPC.disable_s3=1 \\ -global ICH9-LPC.disable_s4=1 \\ -boot strict=on \\ -audiodev none,id=noaudio \\ -object rng-random,id=objrng0,filename=/dev/urandom \\ -msg timestamp=on \\ -device pcie-root-port,port=0x10,chassis=1,id=pci.1,bus=pcie.0,multifunction=on,addr=0x2 \\ -netdev bridge,id=hostnet0,br=${bridge} \\ -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:fa:2d:b9,bus=pci.1,addr=0x0 \\ -device pcie-root-port,port=0x11,chassis=2,id=pci.2,bus=pcie.0,addr=0x2.0x1 \\ -blockdev node-name=\"vmstorage\",driver=qcow2,file.driver=file,file.filename=\"${vmimg}\",file.node-name=\"vmstorage.qcow2\",file.discard=unmap \\ -device virtio-blk-pci,bus=pci.2,addr=0x0,drive=\"vmstorage\",id=virtio-disk0,bootindex=1 \\ -device pcie-root-port,port=0x12,chassis=3,id=pci.3,bus=pcie.0,addr=0x2.0x2 \\ -device virtio-balloon-pci,id=balloon0,bus=pci.3,addr=0x0 \\ -device pcie-root-port,port=0x13,chassis=4,id=pci.4,bus=pcie.0,addr=0x2.0x3 \\ -device virtio-rng-pci,rng=objrng0,id=rng0,bus=pci.4,addr=0x0 \\ -device pcie-root-port,port=0x14,chassis=5,id=pci.5,bus=pcie.0,addr=0x2.0x4 \\ -device nvme,id=nvme0,serial=deadbeef,zoned.zasl=5,bus=pci.5 \\ -drive file=${znsimg},id=nvmezns0,format=raw,if=none \\ -device nvme-ns,drive=nvmezns0,bus=nvme0,nsid=1,logical_block_size=4096,physical_block_size=4096,zoned=true,zoned.zone_size=64M,zoned.zone_capacity=62M,zoned.max_open=16,zoned.max_active=32,uuid=5e40ec5f-eeb6-4317-bc5e-c919796a5f79","title":"Getting Started With Emulated NVMe ZNS Devices"},{"location":"getting-started/zns-emulation/#getting-started-with-emulated-nvme-zns-devices","text":"For users without access to NVMe ZNS devices, application development and kernel tests are possible using emulated devices. Two methods exist. null_blk : The null_blk kernel driver allows emulating zoned block devices with a zone configuration compatible with the real NVMe ZNS devices. This method is discussed in more details here . QEMU : This open source machine emulator and virtualizer allows for the creation of emulated NVMe devices with a regular file used as a backstore on the host. Recent versions of QEMU also support the emulation of Zoned Namespaces.","title":"Getting Started with Emulated NVMe ZNS Devices"},{"location":"getting-started/zns-emulation/#qemu","text":"QEMU supports the emulation of NVMe namespaces since version 1.6. However, the emulation of zoned namespaces requires the more recent version 6.0 or later of QEMU . If the host Linux distribution does not provide QEMU version 6.0 or or above, QEMU has to be compiled from source. Detailed information on how to compile and install QEMU from source can be found here .","title":"QEMU"},{"location":"getting-started/zns-emulation/#creating-an-emulated-zoned-namespace","text":"The creation of an emulated zoned namespace requires first a backing store file for the namespace. The size of the file determines the capacity of the namespace that will be seen from the guest OS running in the QEMU virtual machine. For example, to create a 32GiB zoned namespace, a 32 GiB file on the host must first be created. This can be done simply using the truncate command to create a sparse file or the dd command to create a fully allocated file. # truncate -s 32G /var/lib/qemu/images/zns.raw # ls -l /var/lib/qemu/images/zns.raw -rw-r--r-- 1 root root 34359738368 Jun 21 15:13 /var/lib/qemu/images/zns.raw Or using dd: # dd if=/dev/zero of=/var/lib/qemu/images/zns.raw bs=1M count=32768 32768+0 records in 32768+0 records out 34359738368 bytes (34 GB, 32 GiB) copied, 11.4072 s, 3.0 GB/s # ls -l /var/lib/qemu/images/zns.raw -rw-r--r-- 1 root root 34359738368 Jun 22 11:22 /var/lib/qemu/images/zns.raw Next, QEMU can be executed with additional command lint options and arguments to request the creation of a zoned namespace using the backstore file as storage. In the following example, the backing store file is used to emulate a zoned namespace with zones of 64 MiB and a zone capacity of 62 MiB. The namespace block size is 4096 B. The namespace is set to allow at most 16 open zones and 32 active zones. # /usr/local/bin/qemu-system-x86_64 \\ ... -device nvme,id=nvme0,serial=deadbeef,zoned.zasl=5 \\ -drive file=${znsimg},id=nvmezns0,format=raw,if=none \\ -device nvme-ns,drive=nvmezns0,bus=nvme0,nsid=1,logical_block_size=4096,\\ physical_block_size=4096,zoned=true,zoned.zone_size=64M,zoned.\\ zone_capacity=62M,zoned.max_open=16,zoned.max_active=32,\\ uuid=5e40ec5f-eeb6-4317-bc5e-c919796a5f79 ...","title":"Creating an Emulated  Zoned Namespace"},{"location":"getting-started/zns-emulation/#verifying-an-emulated-zoned-namespace","text":"When running a Linux distribution as the guest operating system, with a kernel version higher that 5.9.0, the emulated NVMe ZNS device can be checked using the nvme command (see Linux Tools for ZNS . # nvme list Node SN Model Namespace Usage Format FW Rev ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- -------- /dev/nvme0n1 deadbeef QEMU NVMe Ctrl 1 34.36 GB / 34.36 GB 4 KiB + 0 B 1.0 The lsscsi utility will also show the emulated NVMe device. # lsscsi -g [2:0:0:0] cd/dvd QEMU QEMU DVD-ROM 2.5+ /dev/sr0 /dev/sg0 [N:0:0:1] disk QEMU NVMe Ctrl__1 /dev/nvme0n1 - Using the blkzone utility, the namespace zone configuration can be inspected. # blkzone report /dev/nvme0n1 | less start: 0x000000000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000020000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000040000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000060000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000080000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x003f80000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fa0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fc0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x003fe0000, len 0x020000, cap 0x01f000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] Of note is that the total number of zones of the namespace directly depends on the size of the backstore file used and on the zone size configured. In the above example, the emulated namespace has 512 zones (32 GiB / 64 MiB). # cat /sys/block/nvme0n1/queue/nr_zones 512 If the emulated namespace is configured with a zone capacity smaller than the zone size, the entire capacity defined by the backstore file will not be usable. The effective usable capacity can be reported using blkzone with the capacity command. # blkzone capacity /dev/nvme0n1 0x003e00000 In this case, the namespace effective storage capacity is 0x003e00000 (65011712) 512-Bytes sectors, equivalent to 512 zones of 62 MiB capacity.","title":"Verifying an Emulated  Zoned Namespace"},{"location":"getting-started/zns-emulation/#using-an-emulated-zoned-namespace","text":"The behavior of a QEMU emulated NVMe ZNS device is fully compliant with the NVMe ZNS specifications, with one exception: the state of namespace zones is not persistent across restart of the QEMU emulation. The state of zones is preserved only as long as QEMU executes, even if the guest operating system is rebooted. If QEMU is restarted using the same backstore file, the guest operating system will see the namespace with all zones in the empty state.","title":"Using an Emulated Zoned Namespace"},{"location":"getting-started/zns-emulation/#emulated-zoned-namespace-options","text":"The implementation of NVMe device and ZNS namespace emulation in QEMU provides several configuration options to control the characteristics of the device. The full list of options and parameters is documented here . The options and parameters related to Zoned Namespaces are as follows. Option Default Value Description zoned.zasl=UINT32 0 Zone Append Size Limit. If left at the default (0), the zone append size limit will be equal to the maximum data transfer size (MDTS). Otherwise, the zone append size limit is equal to 2 to the power of zasl multiplied by the minimum memory page size (4096 B), but cannot exceed the maximum data transfer size. zoned.zone_size= SIZE 128MiB Define the zone size (ZSZE) zoned.zone_capacity= SIZE 0 Define the zone capacity (ZCAP). If left at the default (0), the zone capacity will equal the zone size. zoned.descr_ext_size= UINT32 0 Set the Zone Descriptor Extension Size (ZDES). Must be a multiple of 64 bytes. zoned.cross_read= BOOL off Set to on to allow reads to cross zone boundaries. zoned.max_active= UINT32 0 Set the maximum number of active resources (MAR). The default (0) allows all zones to be active. zoned.max_open= UINT32 0 Set the maximum number of open resources (MOR). The default (0) allows all zones to be open. If zoned.max_active is specified, this value must be less than or equal to that.","title":"Emulated Zoned Namespace Options"},{"location":"getting-started/zns-emulation/#qemu-execution-example","text":"The following script execute QEMU to run a virtual machine with 8 CPU cores, 16 GiB of memory and bridged networking. The bridge device virbr0 is assumed to already exist. The last device added to the virtual machine on the QEMU command line is a 32 GiB NVMe ZNS device. #!/bin/sh # # Some variables # bridge=\"virbr0\" vmimg=\"/var/lib/qemu/boot-disk.qcow2\" znsimg=\"/var/lib/qemu/zns.raw\" # # Run QEMU # sudo /usr/local/bin/qemu-system-x86_64 \\ -name guest=FedoraServer34,debug-threads=on \\ -machine pc-q35-5.2,accel=kvm,usb=off,vmport=off,dump-guest-core=off,memory-backend=pc.ram \\ -cpu EPYC-Rome,x2apic=on,tsc-deadline=on,hypervisor=on,tsc-adjust=on,stibp=on,arch-capabilities=on,ssbd=on,xsaves=on,cmp-legacy=on,amd-ssbd=on,virt-ssbd=on,rdctl-no=on,skip-l1dfl-vmentry=on,mds-no=on,pschange-mc-no=on \\ -m 16384 \\ -object memory-backend-ram,id=pc.ram,size=17179869184 \\ -overcommit mem-lock=off \\ -smp 8,sockets=8,cores=1,threads=1 \\ -uuid e629893b-a9fd-48aa-9505-f32503ab3405 \\ -rtc base=utc,driftfix=slew \\ -global kvm-pit.lost_tick_policy=delay \\ -nographic \\ -no-hpet \\ -global ICH9-LPC.disable_s3=1 \\ -global ICH9-LPC.disable_s4=1 \\ -boot strict=on \\ -audiodev none,id=noaudio \\ -object rng-random,id=objrng0,filename=/dev/urandom \\ -msg timestamp=on \\ -device pcie-root-port,port=0x10,chassis=1,id=pci.1,bus=pcie.0,multifunction=on,addr=0x2 \\ -netdev bridge,id=hostnet0,br=${bridge} \\ -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:fa:2d:b9,bus=pci.1,addr=0x0 \\ -device pcie-root-port,port=0x11,chassis=2,id=pci.2,bus=pcie.0,addr=0x2.0x1 \\ -blockdev node-name=\"vmstorage\",driver=qcow2,file.driver=file,file.filename=\"${vmimg}\",file.node-name=\"vmstorage.qcow2\",file.discard=unmap \\ -device virtio-blk-pci,bus=pci.2,addr=0x0,drive=\"vmstorage\",id=virtio-disk0,bootindex=1 \\ -device pcie-root-port,port=0x12,chassis=3,id=pci.3,bus=pcie.0,addr=0x2.0x2 \\ -device virtio-balloon-pci,id=balloon0,bus=pci.3,addr=0x0 \\ -device pcie-root-port,port=0x13,chassis=4,id=pci.4,bus=pcie.0,addr=0x2.0x3 \\ -device virtio-rng-pci,rng=objrng0,id=rng0,bus=pci.4,addr=0x0 \\ -device pcie-root-port,port=0x14,chassis=5,id=pci.5,bus=pcie.0,addr=0x2.0x4 \\ -device nvme,id=nvme0,serial=deadbeef,zoned.zasl=5,bus=pci.5 \\ -drive file=${znsimg},id=nvmezns0,format=raw,if=none \\ -device nvme-ns,drive=nvmezns0,bus=nvme0,nsid=1,logical_block_size=4096,physical_block_size=4096,zoned=true,zoned.zone_size=64M,zoned.zone_capacity=62M,zoned.max_open=16,zoned.max_active=32,uuid=5e40ec5f-eeb6-4317-bc5e-c919796a5f79","title":"QEMU Execution Example"},{"location":"introduction/","text":"Introduction to Zoned Storage This section provides information about zoned storage principles and underlying storage device technologies as well as an overview of the Linux\u00ae ecosystem support. Zoned Storage Overview : Principle of operation of zoned storage devices. Shingled Magnetic Recording : Learn about Shingled Magnetic Recording (SMR) technology and its use in zoned hard-disks. NVMe Zoned Namespaces : Learn about NVMe\u2122 Zoned Namespace (ZNS) technical proposal.","title":"Introduction to Zoned Storage"},{"location":"introduction/#introduction-to-zoned-storage","text":"This section provides information about zoned storage principles and underlying storage device technologies as well as an overview of the Linux\u00ae ecosystem support. Zoned Storage Overview : Principle of operation of zoned storage devices. Shingled Magnetic Recording : Learn about Shingled Magnetic Recording (SMR) technology and its use in zoned hard-disks. NVMe Zoned Namespaces : Learn about NVMe\u2122 Zoned Namespace (ZNS) technical proposal.","title":"Introduction to Zoned Storage"},{"location":"introduction/smr/","text":"Shingled Magnetic Recording Shingled Magnetic Recording (SMR) is a magnetic storage data recording technology used in hard disk drives (HDDs) to provide increased areal density compared to same-generation drives using conventional magnetic recording (CMR) technology, resulting in a higher overall per-drive storage capacity. SMR Overview Conventional magnetic recording places gaps between recording tracks on HDDs to account for Track mis-registration (TMR) budget. These separators impact areal density, as portions of the platter surface are not being fully utilized. Shingled magnetic recording removes the gaps between tracks by writing tracks in an overlapping manner, forming a pattern similar to shingles on a roof. Physically, this is done by writing the data sequentially, then overlapping (or \u201cshingling\u201d) it with another track of data. By repeating this process, more data tracks can be placed on each magnetic surface. The figure below illustrates this principle. SMR disks track organization The write head designed for SMR drives is wider than required for a single track of data. It produces a stronger magnetic field suitable for magnetizing films of high coercivity. Once one track has been written, the recording head is advanced by only part of its width, so the next track will partially overwrite the previous one, leaving only a narrow band for reading. Overlapping tracks are grouped into bands called zones of fixed capacity for more effective data organization and partial update capability. Recording gaps between zones are laid to prevent data overwrite by the wide write head from one zone to another. SMR disks overlapping tracks Fundamental Implications of SMR Because of the shingled format of SMR, all data streams must be organized and written sequentially to the media. While the methods of SMR implementation may differ (see SMR Implementations section below), the data nonetheless must be written to the media sequentially. Consequently, should a particular data sector need to be modified or re-written, the entire \u201cband\u201d of tracks (zone) must be re-written. Because the modified data sector is potentially under another \u201cshingle\u201d of data, direct modification is not permitted, unlike traditional CMR drives In the case of SMR, the entire row of shingles above the modified track of the sector to modify needs to be rewritten in the process. SMR hard disks still provide true random-read capability, allowing rapid data access like any traditional CMR drive. This makes SMR an excellent technology candidate for both active archive and higher-performance sequential workloads. SMR Interface Implementations The command interface of SMR disks can take different forms, referred to as models, with visible differences from the host and user point of view. It is important to understand these differences, as not all implementation options are appropriate for a particular storage application. The three models that are in use today are: Host Managed This model accommodates only sequential write workloads to deliver both predictable performance and control at the host level. Host-software modifications are required to use host managed SMR drives. Drive Managed This model deals with the sequential write constraint internally, providing a bacward compatible interface. Drive managed disks accommodate both sequential and random writing. Host Aware This model offers the convenience and flexibility of the drive managed model, that is, backward compatibility with regular disks, while also providing the same host control interface as host managed models. Host Managed Model The host managed model does not provide backwards-compatibility with legacy host storage stacks, but rather, delegates management of the SMR sequential write constraint to the host. This is the key difference from drive managed disks: host managed devices do not allow any random write operations within (sequential write required) zones. With the host managed model, the device blocks are organized in a number of zones ranging from one to potentially many thousands. There are two types of zones: sequential write required zones and optional conventional zones. Conventional zones, which typically occupy a very small percentage of the overall drive capacity, can accept random writes and are typically used to store metadata. Sequential write required zones occupy the majority of the overall drive capacity where the device enforces sequentiality of all write commands within each zone. It should be noted that with the Host Managed model, random read commands are supported and perform comparably to that of standard drives. The host must manage all write operations to be in sequence within a particular sequential write required zone by following a write pointer. Once data is written to the zone, the write pointer increments to indicate the starting point of the next write operation in that zone. Any out-of-order writes, that is, a write operation not starting at the zone write pointer location, will force the drive to abort the operation and flag an error. Recovery from such an error is the responsibility of the controlling host software. This enforcement allows host managed devices to deliver predictable performance. Drive Managed Model In the drive managed SMR disk model the drive deals with the sequential write constraint internally, providing a backwards compatible interface. The performance characteristics of drive managed SMR disks will however depend on the applications used and workloads executed. Host Aware Model The host aware model is the superset of the host managed and drive managed models as it simultaneously preserves compatibility with legacy host storage stacks (backward compatibility with regular disks) and provides the same set of commands for a host to tightly control the disk write operation handling. Similarly to drive managed disks, the performance of host aware disks when used as regular disks will depend on application and workload. All host-side software support and optimization discussed in this site apply to host aware SMR disks when these devices are used similarly to host managed disks. ZonedStorage.io focuses on host managed devices The documentation pages of this site focus on host managed SMR disks. Drive managed and host aware disks used as regular devices are not discussed. Governing Standards A new specification of commands have been defined for SMR disks implementing the Host Managed and Host Aware models. These new command interfaces are all standards-based and developed by the INCITS T10 committee for SCSI drives and the INCITS T13 committee for ATA drives. There is no specific industry standard for the Drive Managed model because it is backward compatible and purely transparent to hosts. SCSI Standard: ZBC The Zoned Block Command (ZBC) revision 05 is the published approved standard defining the new zone management commands and read/write command behavior for Host Managed and Host Aware SCSI drives. Implemented in conjunction with the applicable clauses of the SPC-5 and SBC-4 specifications, the ZBC specifications define the model and command set extensions for zoned block devices. The Zoned Block Commands specifications document is published as ANSI INCITS 536-2016: Information technology \u2013 Zoned Block Commands (ZBC) and can be purchased from the ANSI webstore . This document is available at no cost to INCITS T10 member companies. Contact INCITS for further information. ATA Standard: ZAC The INCITS Technical Committee T13 is responsible for all interface standards relating to the popular AT Attachment (ATA) storage interface used with many disk drive today. The Zoned Device ATA Command Set (ZAC) is the published approved standard specifying the command set that host systems use to access storage devices that implement the Host Aware Zones feature set or the Host Managed Zones feature set. This standard is an extension to the ATA implementation standards described in AT Attachment - 8 ATA/ATAPI Architecture Model (ATA8-AAM) and provides a common command set for systems manufacturers, system integrators, software suppliers, and suppliers of storage devices that provide one of the zones feature sets. The Zoned Device ATA Command Set specifications document is published as INCITS 537-2016: Information technology \u2013 Zoned Device ATA Command Set (ZAC) and can be purchased from the ANSI webstore . This document is available at no cost to INCITS T13 member companies. Contact INCITS for further information. Zone Block Commands The ZAC and ZBC standards describe the set of commands necessary for a host application to manage zones of a Host Managed or Host Aware drive. While these two standards describe commands for two separate protocols (SCSI and ATA), the zone management commands defined are semantically identical and the behavior of read and write commands defined are also compatible. In addition to the zone management commands, the ZBC and ZAC standards also both define the zone models discussed in the SMR Interface Implementations section. Both standards define five zone management commands as extensions to the disk basic command set similar to that of a CMR drive. REPORT ZONES is the command that a host implementation can use to discover the zone organization of a host managed or host aware drive. The REPORT ZONES command returns a list of zone descriptors indicating the starting LBA, size, type and condition of a zone. For sequential write required zones (Host Managed drives) and sequential write preferred zones (Host Aware drives), a zone descriptor also indicates the current position of the zone write pointer. This information allows host software to implement sequential write streams to zones. RESET ZONE WRITE POINTER is the command that a host software can use to reset the location of a zone write pointer to the beginning of the zone. After execution of this command, all data that was written to the zone is lost and cannot be accessed. OPEN ZONE A zoned block device requires internal resources (e.g. Persistent zone resources) to maintain each zone. Insufficient resources may result in degraded functionality (e.g. Reduced performance or increased power consumption). The OPEN ZONE command allows an application to explicitly open a zone indicating to the drive that resources necessary for writing a zone are kept available until the zone is fully written or the zone is closed using the CLOSE ZONE command. The performance benefits that can be achieved using this command are dependent on the drive implementation of zone management. CLOSE ZONE allows an application to explicitly close a zone that was open using the OPEN ZONE command to indicate to the drive that the resources used for writing to a zone are no longer necessary and can be released. FINISH ZONE allows an application to move the write pointer of a zone to the end of the zone to prevent any further write operations to the zone until it is reset.","title":"Shingled Magnetic Recording (SMR) HDDs"},{"location":"introduction/smr/#shingled-magnetic-recording","text":"Shingled Magnetic Recording (SMR) is a magnetic storage data recording technology used in hard disk drives (HDDs) to provide increased areal density compared to same-generation drives using conventional magnetic recording (CMR) technology, resulting in a higher overall per-drive storage capacity.","title":"Shingled Magnetic Recording"},{"location":"introduction/smr/#smr-overview","text":"Conventional magnetic recording places gaps between recording tracks on HDDs to account for Track mis-registration (TMR) budget. These separators impact areal density, as portions of the platter surface are not being fully utilized. Shingled magnetic recording removes the gaps between tracks by writing tracks in an overlapping manner, forming a pattern similar to shingles on a roof. Physically, this is done by writing the data sequentially, then overlapping (or \u201cshingling\u201d) it with another track of data. By repeating this process, more data tracks can be placed on each magnetic surface. The figure below illustrates this principle. SMR disks track organization The write head designed for SMR drives is wider than required for a single track of data. It produces a stronger magnetic field suitable for magnetizing films of high coercivity. Once one track has been written, the recording head is advanced by only part of its width, so the next track will partially overwrite the previous one, leaving only a narrow band for reading. Overlapping tracks are grouped into bands called zones of fixed capacity for more effective data organization and partial update capability. Recording gaps between zones are laid to prevent data overwrite by the wide write head from one zone to another. SMR disks overlapping tracks","title":"SMR Overview"},{"location":"introduction/smr/#fundamental-implications-of-smr","text":"Because of the shingled format of SMR, all data streams must be organized and written sequentially to the media. While the methods of SMR implementation may differ (see SMR Implementations section below), the data nonetheless must be written to the media sequentially. Consequently, should a particular data sector need to be modified or re-written, the entire \u201cband\u201d of tracks (zone) must be re-written. Because the modified data sector is potentially under another \u201cshingle\u201d of data, direct modification is not permitted, unlike traditional CMR drives In the case of SMR, the entire row of shingles above the modified track of the sector to modify needs to be rewritten in the process. SMR hard disks still provide true random-read capability, allowing rapid data access like any traditional CMR drive. This makes SMR an excellent technology candidate for both active archive and higher-performance sequential workloads.","title":"Fundamental Implications of SMR"},{"location":"introduction/smr/#smr-interface-implementations","text":"The command interface of SMR disks can take different forms, referred to as models, with visible differences from the host and user point of view. It is important to understand these differences, as not all implementation options are appropriate for a particular storage application. The three models that are in use today are: Host Managed This model accommodates only sequential write workloads to deliver both predictable performance and control at the host level. Host-software modifications are required to use host managed SMR drives. Drive Managed This model deals with the sequential write constraint internally, providing a bacward compatible interface. Drive managed disks accommodate both sequential and random writing. Host Aware This model offers the convenience and flexibility of the drive managed model, that is, backward compatibility with regular disks, while also providing the same host control interface as host managed models.","title":"SMR Interface Implementations"},{"location":"introduction/smr/#host-managed-model","text":"The host managed model does not provide backwards-compatibility with legacy host storage stacks, but rather, delegates management of the SMR sequential write constraint to the host. This is the key difference from drive managed disks: host managed devices do not allow any random write operations within (sequential write required) zones. With the host managed model, the device blocks are organized in a number of zones ranging from one to potentially many thousands. There are two types of zones: sequential write required zones and optional conventional zones. Conventional zones, which typically occupy a very small percentage of the overall drive capacity, can accept random writes and are typically used to store metadata. Sequential write required zones occupy the majority of the overall drive capacity where the device enforces sequentiality of all write commands within each zone. It should be noted that with the Host Managed model, random read commands are supported and perform comparably to that of standard drives. The host must manage all write operations to be in sequence within a particular sequential write required zone by following a write pointer. Once data is written to the zone, the write pointer increments to indicate the starting point of the next write operation in that zone. Any out-of-order writes, that is, a write operation not starting at the zone write pointer location, will force the drive to abort the operation and flag an error. Recovery from such an error is the responsibility of the controlling host software. This enforcement allows host managed devices to deliver predictable performance.","title":"Host Managed Model"},{"location":"introduction/smr/#drive-managed-model","text":"In the drive managed SMR disk model the drive deals with the sequential write constraint internally, providing a backwards compatible interface. The performance characteristics of drive managed SMR disks will however depend on the applications used and workloads executed.","title":"Drive Managed Model"},{"location":"introduction/smr/#host-aware-model","text":"The host aware model is the superset of the host managed and drive managed models as it simultaneously preserves compatibility with legacy host storage stacks (backward compatibility with regular disks) and provides the same set of commands for a host to tightly control the disk write operation handling. Similarly to drive managed disks, the performance of host aware disks when used as regular disks will depend on application and workload. All host-side software support and optimization discussed in this site apply to host aware SMR disks when these devices are used similarly to host managed disks. ZonedStorage.io focuses on host managed devices The documentation pages of this site focus on host managed SMR disks. Drive managed and host aware disks used as regular devices are not discussed.","title":"Host Aware Model"},{"location":"introduction/smr/#governing-standards","text":"A new specification of commands have been defined for SMR disks implementing the Host Managed and Host Aware models. These new command interfaces are all standards-based and developed by the INCITS T10 committee for SCSI drives and the INCITS T13 committee for ATA drives. There is no specific industry standard for the Drive Managed model because it is backward compatible and purely transparent to hosts.","title":"Governing Standards"},{"location":"introduction/smr/#scsi-standard-zbc","text":"The Zoned Block Command (ZBC) revision 05 is the published approved standard defining the new zone management commands and read/write command behavior for Host Managed and Host Aware SCSI drives. Implemented in conjunction with the applicable clauses of the SPC-5 and SBC-4 specifications, the ZBC specifications define the model and command set extensions for zoned block devices. The Zoned Block Commands specifications document is published as ANSI INCITS 536-2016: Information technology \u2013 Zoned Block Commands (ZBC) and can be purchased from the ANSI webstore . This document is available at no cost to INCITS T10 member companies. Contact INCITS for further information.","title":"SCSI Standard: ZBC"},{"location":"introduction/smr/#ata-standard-zac","text":"The INCITS Technical Committee T13 is responsible for all interface standards relating to the popular AT Attachment (ATA) storage interface used with many disk drive today. The Zoned Device ATA Command Set (ZAC) is the published approved standard specifying the command set that host systems use to access storage devices that implement the Host Aware Zones feature set or the Host Managed Zones feature set. This standard is an extension to the ATA implementation standards described in AT Attachment - 8 ATA/ATAPI Architecture Model (ATA8-AAM) and provides a common command set for systems manufacturers, system integrators, software suppliers, and suppliers of storage devices that provide one of the zones feature sets. The Zoned Device ATA Command Set specifications document is published as INCITS 537-2016: Information technology \u2013 Zoned Device ATA Command Set (ZAC) and can be purchased from the ANSI webstore . This document is available at no cost to INCITS T13 member companies. Contact INCITS for further information.","title":"ATA Standard: ZAC"},{"location":"introduction/smr/#zone-block-commands","text":"The ZAC and ZBC standards describe the set of commands necessary for a host application to manage zones of a Host Managed or Host Aware drive. While these two standards describe commands for two separate protocols (SCSI and ATA), the zone management commands defined are semantically identical and the behavior of read and write commands defined are also compatible. In addition to the zone management commands, the ZBC and ZAC standards also both define the zone models discussed in the SMR Interface Implementations section. Both standards define five zone management commands as extensions to the disk basic command set similar to that of a CMR drive. REPORT ZONES is the command that a host implementation can use to discover the zone organization of a host managed or host aware drive. The REPORT ZONES command returns a list of zone descriptors indicating the starting LBA, size, type and condition of a zone. For sequential write required zones (Host Managed drives) and sequential write preferred zones (Host Aware drives), a zone descriptor also indicates the current position of the zone write pointer. This information allows host software to implement sequential write streams to zones. RESET ZONE WRITE POINTER is the command that a host software can use to reset the location of a zone write pointer to the beginning of the zone. After execution of this command, all data that was written to the zone is lost and cannot be accessed. OPEN ZONE A zoned block device requires internal resources (e.g. Persistent zone resources) to maintain each zone. Insufficient resources may result in degraded functionality (e.g. Reduced performance or increased power consumption). The OPEN ZONE command allows an application to explicitly open a zone indicating to the drive that resources necessary for writing a zone are kept available until the zone is fully written or the zone is closed using the CLOSE ZONE command. The performance benefits that can be achieved using this command are dependent on the drive implementation of zone management. CLOSE ZONE allows an application to explicitly close a zone that was open using the OPEN ZONE command to indicate to the drive that the resources used for writing to a zone are no longer necessary and can be released. FINISH ZONE allows an application to move the write pointer of a zone to the end of the zone to prevent any further write operations to the zone until it is reset.","title":"Zone Block Commands"},{"location":"introduction/zns/","text":"Zoned Namespaces (ZNS) SSDs Zoned Namespace (ZNS) SSDs is SSDs that implement the Zoned Namespace Command Set as defined by the NVM Express\u2122 (NVMe\u2122) organization. The specification provides a zoned storage device interface, that allows the SSD and host to collaborate on data placement, such that data can be aligned to the physical media of the SSD, improving the overall performance and increases the capacity that can be exposed to the host. The most reason version, ZNS Command Set 1.0, was released on June 16 th , 2020 to meet industry demands. The ZNS Command Set 1.0 specification addresses requests to improve the data placement capabilities of flash storage and to be exposed through a standardized interface. Note The ZNS Command Set 1.0 specification is currently available as a ratified technical proposal (TP 4053) for the NVMe\u2122 1.4a specification. It is available in the NVM Express 14 Ratified TPs. See the NVMe\u2122 specification page . Overview NVMe ZNS SSDs can provide several benefits over conventional NVMe SSDs. Reducing device-side write amplification leading to improved throughput and latencies Reducing device-side media over-provisioning Reducing device-side DRAM utilization Improved amount of drive writes per day NVMe ZNS SSDs achieve these benefits by exposing a set of zones in an NVMe\u2122 Namespace, where each zone requires to be sequentially written and reset explicitly, similarly to SMR hard-disks . The introduction of the zone abstraction allows a ZNS device implementation to optimize logical block mapping for the physical media. In the case of flash-based devices, the media requirement for sequential data writes within certain areas matches the ZNS protocol sequential write zone concept. The device controller no longer has to manage random writes, and therefore can have a more efficient implementation that can provide some or all of the above mentioned benefits. Conventional SSDs and ZNS SSDs internal data placement NVMe Zoned Namespace Zoned Storage Model The NVMe Zoned Namespace standard was developped based on the Host Managed Zoned Storage Model introduced for SMR hard-disks with the SCSI ZBC (Zoned Block Command) standard and the ATA ZAC (Zoned ATA Commands) standard. A compatible zone state machine was defined, and a similar set of Zone Block Commands was defined. These similarities simplify the implementation of the host storage stack and applications for simultaneously support both host managed SMR hard-disks and NVMe ZNS SSDs. However, the NVMe Zoned Namespace introduces some differences that need special attention. Zone types ZBC and ZAC SMR hard-disks can optionally expose a number of conventional zones which accept random write operations. This optional set of random write zones is not defined by the NVMe ZNS specifications. NVMe Zoned Namespaces only support one type of zone: Sequential Write Required zones. The NVMe device may however expose a random writable area as a separate conventional namespace on the same controller. Zone Capacity and Zone Size The NVMe Zoned Namespace specification introduced the concept of a Zone Capacity. This concept is not defined in the SCSI ZBC and ATA ZAC standards. Similarly to ZBC and ZAC standards, NVMe ZNS defines the zone size as the total number of logical blocks within a zone. A zone capacity is an additional per-zone attribute which indicates the number of usable logical blocks within each zone, starting from the first logical block of each zone. A zone capacity is always smaller or equal to the zone size. This new attribute was introduced to allow for the zone size to remain a power of two number of logical blocks (facilitating easy logical block to zone number conversions) while allowing optimized mapping of a zone storage capacity to the underlying media characteristics. For instance, in the case a flash based device, a zone capacity can be aligned to the size of flash erase blocks without requiring that the device implements a power-of-two sized erased block. The figure below illustrates the zone capacity concept. Zone Size and Zone Capacity As the logical block addresses between the zone capacity and the end of the zone are not mapped to any physical storage blocks, write accesses to these blocks will result in an error. Reading in this area is handled in the same way as when reading unwritted blocks. A zone with a zone capacity smaller than the zone size will be transitioned to a full condition when the number of written blocks equals the zone capacity. Note The total namespace capacity reported by a controller is always equal to the total number of logical blocks defined by the zones. In other words, this reported capacity includes unusable logical blocks of zones with a zone capacity lower than the zone size. The usable capacity of the namespace is equal to the sum of all zones capacities. This usable capacity is always smaller than the reported namespace capacity if the namespace contains zones with a zone capacity lower than the zone size. Active Zones A controller implementation typically requires the allocate of internal resources (e.g. a write buffer) to execute write operations into zones. Limitations on the total amount of resources available to the controller may imply a limit on the total number of zones that can be simultaneously in the implicit open or explicit open conditions. This potential limit on the maximum number of open zones is similarly defined in the NVMe ZNS, SCSI ZBC and ATA ZAC standards. NVMe ZNS however defines an additional limit on the number of zones that can be in the implicit open, explicit open or closed conditions. Any zone with such condition is defined as an active zone and correspond to any zone that is being written or that has been only partially written. A ZNS device controller may impose a limit on the maximum number of zones that can be active. This limit is always equal or larger than the limit on the maximum number of open zones. This new limit imposes new constraints on user applications. While the maximum number of open zones of a namespace only limits the number of zones that an application can simultaneously write, the maximum number of active zones imposes a limit on the number of zones that an application can choose for storing data. If the maximum number of active zones is reached, the application must either reset or finish some active zones before being able to chose other zones for storing data. Similarly to the limit on the maximum number of open zones, a limit on the maximum number of active zones for a namespace does not affect read operations. Any zone that is not offline can always be accessed for reading regardless of the current number of open and active zones. Zone Append The NVMe specifications allow a device controller to execute commands present in the several submission queues available in any order. This has implications for the host IO stack, namely, even if the host submits write commands directed at a zone sequentially, the commands may be reordered before they are processed and violate the sequential write requirement, resulting in errors. Host software can avoid such error by limiting the number of write commands outstanding per zone to one. This can potentially result in poor performance, especially for workloads issuing mostly small write operations. To avoid this problem, the NVMe ZNS specifications introduced the new Zone Append command. This command does not defined by the SCSI ZBC and ATA ZAC standards. A zone append comamnd is a write operation that specifies the first logical block of a zone as the write position. When executing the command, the device controller write the data within the zone indicated, but do so at the current zone write pointer position. This change in the write position is automatic and the effective write position for the data is indicated to the host through the command completion information. This mechanism allows a host to simultaneously submit several zone append operations and let the device process these in any order. The figure below illustrates the differences between regular write operations and zone append write operations. Regular Writes and Zone Append Writes In the eaxmple above, the host must issue to the same zone three different write operations for data A (4KB), B (8KB) and C (16KB). Using regular write commands, this can be done safely only at a write queue depth of 1 per zone,i that is, the host must wait for the completion of an outstanding write operation before issuing the next write request. For each write request, the write position must be equal to the zone write pointer position. This result in the data being stored in the zone in the same order as issued. Using zone append write operations, the write queue depth constraint is removed and the host can issue all three write requests simultaneously. Upon completion of all write requests, the zone write pointer position is identical to the previous case as the total amount of data written is equal. However, the location of the written data within the zone may not correspond to the host command issuing order as the device controller is free to reorder command execution as it sees fit. The host can discover the effective write position of each request through the zone append completion information. Presentations Further information on ZNS SSDs are available here: From Open-Channel SSDs to Zoned Namespaces, OCP 2019 Global Summit. The presentation covers the motivation for ZNS SSDs, the journey, and a general overview of the interface.","title":"Zoned Namespace (ZNS) SSDs"},{"location":"introduction/zns/#zoned-namespaces-zns-ssds","text":"Zoned Namespace (ZNS) SSDs is SSDs that implement the Zoned Namespace Command Set as defined by the NVM Express\u2122 (NVMe\u2122) organization. The specification provides a zoned storage device interface, that allows the SSD and host to collaborate on data placement, such that data can be aligned to the physical media of the SSD, improving the overall performance and increases the capacity that can be exposed to the host. The most reason version, ZNS Command Set 1.0, was released on June 16 th , 2020 to meet industry demands. The ZNS Command Set 1.0 specification addresses requests to improve the data placement capabilities of flash storage and to be exposed through a standardized interface. Note The ZNS Command Set 1.0 specification is currently available as a ratified technical proposal (TP 4053) for the NVMe\u2122 1.4a specification. It is available in the NVM Express 14 Ratified TPs. See the NVMe\u2122 specification page .","title":"Zoned Namespaces (ZNS) SSDs"},{"location":"introduction/zns/#overview","text":"NVMe ZNS SSDs can provide several benefits over conventional NVMe SSDs. Reducing device-side write amplification leading to improved throughput and latencies Reducing device-side media over-provisioning Reducing device-side DRAM utilization Improved amount of drive writes per day NVMe ZNS SSDs achieve these benefits by exposing a set of zones in an NVMe\u2122 Namespace, where each zone requires to be sequentially written and reset explicitly, similarly to SMR hard-disks . The introduction of the zone abstraction allows a ZNS device implementation to optimize logical block mapping for the physical media. In the case of flash-based devices, the media requirement for sequential data writes within certain areas matches the ZNS protocol sequential write zone concept. The device controller no longer has to manage random writes, and therefore can have a more efficient implementation that can provide some or all of the above mentioned benefits. Conventional SSDs and ZNS SSDs internal data placement","title":"Overview"},{"location":"introduction/zns/#nvme-zoned-namespace-zoned-storage-model","text":"The NVMe Zoned Namespace standard was developped based on the Host Managed Zoned Storage Model introduced for SMR hard-disks with the SCSI ZBC (Zoned Block Command) standard and the ATA ZAC (Zoned ATA Commands) standard. A compatible zone state machine was defined, and a similar set of Zone Block Commands was defined. These similarities simplify the implementation of the host storage stack and applications for simultaneously support both host managed SMR hard-disks and NVMe ZNS SSDs. However, the NVMe Zoned Namespace introduces some differences that need special attention.","title":"NVMe Zoned Namespace Zoned Storage Model"},{"location":"introduction/zns/#zone-types","text":"ZBC and ZAC SMR hard-disks can optionally expose a number of conventional zones which accept random write operations. This optional set of random write zones is not defined by the NVMe ZNS specifications. NVMe Zoned Namespaces only support one type of zone: Sequential Write Required zones. The NVMe device may however expose a random writable area as a separate conventional namespace on the same controller.","title":"Zone types"},{"location":"introduction/zns/#zone-capacity-and-zone-size","text":"The NVMe Zoned Namespace specification introduced the concept of a Zone Capacity. This concept is not defined in the SCSI ZBC and ATA ZAC standards. Similarly to ZBC and ZAC standards, NVMe ZNS defines the zone size as the total number of logical blocks within a zone. A zone capacity is an additional per-zone attribute which indicates the number of usable logical blocks within each zone, starting from the first logical block of each zone. A zone capacity is always smaller or equal to the zone size. This new attribute was introduced to allow for the zone size to remain a power of two number of logical blocks (facilitating easy logical block to zone number conversions) while allowing optimized mapping of a zone storage capacity to the underlying media characteristics. For instance, in the case a flash based device, a zone capacity can be aligned to the size of flash erase blocks without requiring that the device implements a power-of-two sized erased block. The figure below illustrates the zone capacity concept. Zone Size and Zone Capacity As the logical block addresses between the zone capacity and the end of the zone are not mapped to any physical storage blocks, write accesses to these blocks will result in an error. Reading in this area is handled in the same way as when reading unwritted blocks. A zone with a zone capacity smaller than the zone size will be transitioned to a full condition when the number of written blocks equals the zone capacity. Note The total namespace capacity reported by a controller is always equal to the total number of logical blocks defined by the zones. In other words, this reported capacity includes unusable logical blocks of zones with a zone capacity lower than the zone size. The usable capacity of the namespace is equal to the sum of all zones capacities. This usable capacity is always smaller than the reported namespace capacity if the namespace contains zones with a zone capacity lower than the zone size.","title":"Zone Capacity and Zone Size"},{"location":"introduction/zns/#active-zones","text":"A controller implementation typically requires the allocate of internal resources (e.g. a write buffer) to execute write operations into zones. Limitations on the total amount of resources available to the controller may imply a limit on the total number of zones that can be simultaneously in the implicit open or explicit open conditions. This potential limit on the maximum number of open zones is similarly defined in the NVMe ZNS, SCSI ZBC and ATA ZAC standards. NVMe ZNS however defines an additional limit on the number of zones that can be in the implicit open, explicit open or closed conditions. Any zone with such condition is defined as an active zone and correspond to any zone that is being written or that has been only partially written. A ZNS device controller may impose a limit on the maximum number of zones that can be active. This limit is always equal or larger than the limit on the maximum number of open zones. This new limit imposes new constraints on user applications. While the maximum number of open zones of a namespace only limits the number of zones that an application can simultaneously write, the maximum number of active zones imposes a limit on the number of zones that an application can choose for storing data. If the maximum number of active zones is reached, the application must either reset or finish some active zones before being able to chose other zones for storing data. Similarly to the limit on the maximum number of open zones, a limit on the maximum number of active zones for a namespace does not affect read operations. Any zone that is not offline can always be accessed for reading regardless of the current number of open and active zones.","title":"Active Zones"},{"location":"introduction/zns/#zone-append","text":"The NVMe specifications allow a device controller to execute commands present in the several submission queues available in any order. This has implications for the host IO stack, namely, even if the host submits write commands directed at a zone sequentially, the commands may be reordered before they are processed and violate the sequential write requirement, resulting in errors. Host software can avoid such error by limiting the number of write commands outstanding per zone to one. This can potentially result in poor performance, especially for workloads issuing mostly small write operations. To avoid this problem, the NVMe ZNS specifications introduced the new Zone Append command. This command does not defined by the SCSI ZBC and ATA ZAC standards. A zone append comamnd is a write operation that specifies the first logical block of a zone as the write position. When executing the command, the device controller write the data within the zone indicated, but do so at the current zone write pointer position. This change in the write position is automatic and the effective write position for the data is indicated to the host through the command completion information. This mechanism allows a host to simultaneously submit several zone append operations and let the device process these in any order. The figure below illustrates the differences between regular write operations and zone append write operations. Regular Writes and Zone Append Writes In the eaxmple above, the host must issue to the same zone three different write operations for data A (4KB), B (8KB) and C (16KB). Using regular write commands, this can be done safely only at a write queue depth of 1 per zone,i that is, the host must wait for the completion of an outstanding write operation before issuing the next write request. For each write request, the write position must be equal to the zone write pointer position. This result in the data being stored in the zone in the same order as issued. Using zone append write operations, the write queue depth constraint is removed and the host can issue all three write requests simultaneously. Upon completion of all write requests, the zone write pointer position is identical to the previous case as the total amount of data written is equal. However, the location of the written data within the zone may not correspond to the host command issuing order as the device controller is free to reorder command execution as it sees fit. The host can discover the effective write position of each request through the zone append completion information.","title":"Zone Append"},{"location":"introduction/zns/#presentations","text":"Further information on ZNS SSDs are available here: From Open-Channel SSDs to Zoned Namespaces, OCP 2019 Global Summit. The presentation covers the motivation for ZNS SSDs, the journey, and a general overview of the interface.","title":"Presentations"},{"location":"introduction/zoned-storage/","text":"Zoned Storage Overview Zoned storage devices are a class of storage devices with an address space that is divided into zones which have write constraints different from regular storage devices. Principle The zones of zoned storage devices must be written sequentially. Each zone of the device address space has a write pointer that keeps track of the position of the next write. Data in a zone cannot be directly overwritten. The zone must first be erased using a special command (zone reset). The figure below illustrates this principle. Zoned Storage Devices Principle Zoned storage devices can be implemented using various recording and media technologies. The most common form of zoned storage today uses the SCSI Zoned Block Commands (ZBC) and Zoned ATA Commands (ZAC) interfaces on Shingled Magnetic Recording (SMR) HDDs. ZBC and ZAC enable a zoned block storage model; SMR technology enables continued areal density growth to meet the demands for expanding data needs, and requires the zoned block access model. Solid State Disks (SSD) storage devices can also implement a zoned interface to reduce write amplification, reduce the device DRAM needs and improve quality of service at scale. The NVMe Zoned NameSpace (ZNS) is a technical proposal of the NVMe standard committee adding a zoned storage interface to the NVMe interface standard. Linux Ecosystem Support Zoned storage devices are not plug-and-play replacement of traditional storage devices due to the sequential write constraints of zones. Special software and drivers are required to ensure compliance of the application operation to the device constraints. Support for zoned storage device was added to the Linux\u00ae kernel with the release of version 4.10.0 and enables support for zoned storage at different levels (disk driver, file system, device mapper drivers), offering a wide range of options for supporting applications. This support is based on the Zoned Block Device (ZBD) abstraction. A zoned block device is a generic representation of a zoned storage device independent of the device access protocol and interface. This abstraction is the basis of Linux kernel support for zoned storage. The interface associated with the ZBD device abstraction is an extension to the traditional Linux block device interface. The ZBD interface, combined with device drivers, provide to kernel subsystems (e.g. File systems) and to user applications a generic zone management interface compatible across all zoned device types and access protocols. A simplified view of the kernel structure including the ZBD interface is shown in the figure below. Linux kernel Zoned Storage Device Support Overview Linux ZBD interface implementation provides functions to discover the zone configuration of a zoned device and functions to manage zones (e.g. Zone reset). Furthermore, the Linux kernel ZBD support also modifies the kernel block I/O stack to ensure that the device access constraints (zone spanning commands, sequential write ordering, etc) are met. Developing for Zoned Storage There are several ways to design a system application for zoned storage depending on one's system structure and ability to modify the application layer and Linux kernel being used. Users that own software applications and do not depend on the operating system and file system to control the device can directly use the device interface protocol to issue zone management commands using a passthrough interface. In such case, the applications will need to be re-written with the new command sets as well as ensuring that all data streams are sequential. The libzbc library provides functions facilitating the implementation of applications using such approach. Managing zoned storage directly from the application layer is a valid approach but can be difficult to implement, particularly for use cases where several device features must be combined (e.g. Using I/O priorities). Application level support can be simplified by relying on the kernel ZBD support which allows accessing and managing zoned devices using regular POSIX system calls. While this does not remove the zoned device access constraints from the application scope, implementation of sequential write streams and zone management can be simplified. If a user lacks the software control at the application level while retaining control over Linux kernel version choices, more advanced kernel support features such as ZBD compliant file systems can be used to hide zoned storage access constraints from the application. Users may also rely on a device mapper driver exposing zoned storage devices as regular block devices. With such solution, existing file systems can be used without any modification. More information on the features provided by the Linux kernel for different versions can be found here . To get started with zoned storage, the Getting Started with SMR section details the first step necessary to setup and verify a system using SMR disks. The Linux Distributions section provides information regarding the availability of the ZBD interface on various Linux distributions. Various tools and open source projects supporting zoned storage are documented in the Applications and Libraries section.","title":"Zoned Storage Overview"},{"location":"introduction/zoned-storage/#zoned-storage-overview","text":"Zoned storage devices are a class of storage devices with an address space that is divided into zones which have write constraints different from regular storage devices.","title":"Zoned Storage Overview"},{"location":"introduction/zoned-storage/#principle","text":"The zones of zoned storage devices must be written sequentially. Each zone of the device address space has a write pointer that keeps track of the position of the next write. Data in a zone cannot be directly overwritten. The zone must first be erased using a special command (zone reset). The figure below illustrates this principle. Zoned Storage Devices Principle Zoned storage devices can be implemented using various recording and media technologies. The most common form of zoned storage today uses the SCSI Zoned Block Commands (ZBC) and Zoned ATA Commands (ZAC) interfaces on Shingled Magnetic Recording (SMR) HDDs. ZBC and ZAC enable a zoned block storage model; SMR technology enables continued areal density growth to meet the demands for expanding data needs, and requires the zoned block access model. Solid State Disks (SSD) storage devices can also implement a zoned interface to reduce write amplification, reduce the device DRAM needs and improve quality of service at scale. The NVMe Zoned NameSpace (ZNS) is a technical proposal of the NVMe standard committee adding a zoned storage interface to the NVMe interface standard.","title":"Principle"},{"location":"introduction/zoned-storage/#linux-ecosystem-support","text":"Zoned storage devices are not plug-and-play replacement of traditional storage devices due to the sequential write constraints of zones. Special software and drivers are required to ensure compliance of the application operation to the device constraints. Support for zoned storage device was added to the Linux\u00ae kernel with the release of version 4.10.0 and enables support for zoned storage at different levels (disk driver, file system, device mapper drivers), offering a wide range of options for supporting applications. This support is based on the Zoned Block Device (ZBD) abstraction. A zoned block device is a generic representation of a zoned storage device independent of the device access protocol and interface. This abstraction is the basis of Linux kernel support for zoned storage. The interface associated with the ZBD device abstraction is an extension to the traditional Linux block device interface. The ZBD interface, combined with device drivers, provide to kernel subsystems (e.g. File systems) and to user applications a generic zone management interface compatible across all zoned device types and access protocols. A simplified view of the kernel structure including the ZBD interface is shown in the figure below. Linux kernel Zoned Storage Device Support Overview Linux ZBD interface implementation provides functions to discover the zone configuration of a zoned device and functions to manage zones (e.g. Zone reset). Furthermore, the Linux kernel ZBD support also modifies the kernel block I/O stack to ensure that the device access constraints (zone spanning commands, sequential write ordering, etc) are met.","title":"Linux Ecosystem Support"},{"location":"introduction/zoned-storage/#developing-for-zoned-storage","text":"There are several ways to design a system application for zoned storage depending on one's system structure and ability to modify the application layer and Linux kernel being used. Users that own software applications and do not depend on the operating system and file system to control the device can directly use the device interface protocol to issue zone management commands using a passthrough interface. In such case, the applications will need to be re-written with the new command sets as well as ensuring that all data streams are sequential. The libzbc library provides functions facilitating the implementation of applications using such approach. Managing zoned storage directly from the application layer is a valid approach but can be difficult to implement, particularly for use cases where several device features must be combined (e.g. Using I/O priorities). Application level support can be simplified by relying on the kernel ZBD support which allows accessing and managing zoned devices using regular POSIX system calls. While this does not remove the zoned device access constraints from the application scope, implementation of sequential write streams and zone management can be simplified. If a user lacks the software control at the application level while retaining control over Linux kernel version choices, more advanced kernel support features such as ZBD compliant file systems can be used to hide zoned storage access constraints from the application. Users may also rely on a device mapper driver exposing zoned storage devices as regular block devices. With such solution, existing file systems can be used without any modification. More information on the features provided by the Linux kernel for different versions can be found here . To get started with zoned storage, the Getting Started with SMR section details the first step necessary to setup and verify a system using SMR disks. The Linux Distributions section provides information regarding the availability of the ZBD interface on various Linux distributions. Various tools and open source projects supporting zoned storage are documented in the Applications and Libraries section.","title":"Developing for Zoned Storage"},{"location":"linux/","text":"Linux Kernel Support This collection of articles describes Linux\u00ae kernel features supporting zoned storage devices. Support Overview : See an overview of Linux kernel and Linux ecosystem support for zoned block devices. Kernel Configuration : Learn how to configure the Linux kernel before compilation to enable zoned block device support. User Interface : Learn about the user application programming interface provided by the kernel for managing and accessing zoned block devices. Write Command Ordering : Learn how the kernel handles sequential write operations to zones to ensure a correct execution on zoned block devices. Partitions : Learn more about the kernel support for partition tables on zoned block devices. Device Mapper : Learn how to use the device mapper infrastructure and setup device mapper targets with zoned block devices as backend storage. File Systems : Learn about Linux native file systems support for zoned block devices.","title":"Linux Kernel Support"},{"location":"linux/#linux-kernel-support","text":"This collection of articles describes Linux\u00ae kernel features supporting zoned storage devices. Support Overview : See an overview of Linux kernel and Linux ecosystem support for zoned block devices. Kernel Configuration : Learn how to configure the Linux kernel before compilation to enable zoned block device support. User Interface : Learn about the user application programming interface provided by the kernel for managing and accessing zoned block devices. Write Command Ordering : Learn how the kernel handles sequential write operations to zones to ensure a correct execution on zoned block devices. Partitions : Learn more about the kernel support for partition tables on zoned block devices. Device Mapper : Learn how to use the device mapper infrastructure and setup device mapper targets with zoned block devices as backend storage. File Systems : Learn about Linux native file systems support for zoned block devices.","title":"Linux Kernel Support"},{"location":"linux/config/","text":"Kernel Compilation Configuration Several kernel compilation configuration options control zoned block device support features. Zoned Block Devices Core Support To allow exposing supported zoned block devices as block device files, the block layer configuration option CONFIG_BLK_DEV_ZONED must be enabled. This option is part of the Enable the block layer top menu of make menuconfig . Block layer zoned block device support option with make menuconfig Without this configuration option set, users will not have access to the ZBD interface and support for zoned block devices will be disabled in all kernel subsystems (I/O schedulers, device mapper and file systems) that include support for these devices. Device Drivers Configuration null_blk Logical Device Support for the zoned block device emulation with the null_blk device driver zoned mode is automatically enabled with the CONFIG_BLK_DEV_ZONED configuration option. ZBC and ZAC Hard-Disks Support The SCSI subsystem support for ZBC and ZAC SMR disks is automatically enabled with the CONFIG_BLK_DEV_ZONED configuration option. NVMe Zoned Namespace Solid State Disks Support NVM Express Zoned Namespace Command Set depends on CONFIG_BLK_DEV_ZONED and CONFIG_NVME_CORE and is automatically built if both configuration options are enabled. The driver requires the device to support the Zone Append command to successfully bind to a zoned namespace, and does not support Zone Excursions. See Zoned Namespace (ZNS) SSDs for more details about these features. Write Ordering Control Write ordering control is achieved through the deadline (legacy single queue block I/O path) and mq-deadline (multi-queue block I/O path) block I/O scheduler (see Write Ordering Control ). deadline and mq-deadline zoned block device support is automatically enabled if the CONFIG_BLK_DEV_ZONED configuration option is set. Enabling this scheduler is mandatory for zoned block devices. This is controlled with the CONFIG_MQ_IOSCHED_DEADLINE option for mq-deadline and with the CONFIG_IOSCHED_DEADLINE option for deadline . Either option be selected from the IO Schedulers top menu. I/O scheduler configuration with make menuconfig With the introduction of kernel version 5.0 and the removal of the block layer legacy single queue I/O path, only the mq-deadline scheduler remains. Since kernel version 5.2, the selection of the CONFIG_MQ_IOSCHED_DEADLINE option is automatic when the CONFIG_BLK_DEV_ZONED configuration option is set. Device Mapper Zoned block device support for the device mapper subsystem is automatically enabled when the CONFIG_BLK_DEV_ZONED option is set. This will enables support for the dm-linear and dm-flakey targets. However, the dm-zoned device mapper target must be enabled to be usable. Enabling the dm-zoned target can be done by selecting the CONFIG_DM_ZONED option from the menu Device Drivers \u2192 Multiple devices driver support (RAID and LVM) \u2192 Device mapper support \u2192 Drive-managed zoned block device target support . dm-zoned device mapper target configuration with make menuconfig zonefs File System Enabling compilation of the zonefs file system is done by selecting the CONFIG_ZONEFS_FS option from the menu File systems -> zonefs filesystem support . This option is available only and only if the CONFIG_BLK_DEV_ZONED option is set to enable zoned block device support. zonefs filesystem configuration with make menuconfig","title":"Kernel Configuration"},{"location":"linux/config/#kernel-compilation-configuration","text":"Several kernel compilation configuration options control zoned block device support features.","title":"Kernel Compilation Configuration"},{"location":"linux/config/#zoned-block-devices-core-support","text":"To allow exposing supported zoned block devices as block device files, the block layer configuration option CONFIG_BLK_DEV_ZONED must be enabled. This option is part of the Enable the block layer top menu of make menuconfig . Block layer zoned block device support option with make menuconfig Without this configuration option set, users will not have access to the ZBD interface and support for zoned block devices will be disabled in all kernel subsystems (I/O schedulers, device mapper and file systems) that include support for these devices.","title":"Zoned Block Devices Core Support"},{"location":"linux/config/#device-drivers-configuration","text":"","title":"Device Drivers Configuration"},{"location":"linux/config/#null_blk-logical-device","text":"Support for the zoned block device emulation with the null_blk device driver zoned mode is automatically enabled with the CONFIG_BLK_DEV_ZONED configuration option.","title":"null_blk Logical Device"},{"location":"linux/config/#zbc-and-zac-hard-disks-support","text":"The SCSI subsystem support for ZBC and ZAC SMR disks is automatically enabled with the CONFIG_BLK_DEV_ZONED configuration option.","title":"ZBC and ZAC Hard-Disks Support"},{"location":"linux/config/#nvme-zoned-namespace-solid-state-disks-support","text":"NVM Express Zoned Namespace Command Set depends on CONFIG_BLK_DEV_ZONED and CONFIG_NVME_CORE and is automatically built if both configuration options are enabled. The driver requires the device to support the Zone Append command to successfully bind to a zoned namespace, and does not support Zone Excursions. See Zoned Namespace (ZNS) SSDs for more details about these features.","title":"NVMe Zoned Namespace Solid State Disks Support"},{"location":"linux/config/#write-ordering-control","text":"Write ordering control is achieved through the deadline (legacy single queue block I/O path) and mq-deadline (multi-queue block I/O path) block I/O scheduler (see Write Ordering Control ). deadline and mq-deadline zoned block device support is automatically enabled if the CONFIG_BLK_DEV_ZONED configuration option is set. Enabling this scheduler is mandatory for zoned block devices. This is controlled with the CONFIG_MQ_IOSCHED_DEADLINE option for mq-deadline and with the CONFIG_IOSCHED_DEADLINE option for deadline . Either option be selected from the IO Schedulers top menu. I/O scheduler configuration with make menuconfig With the introduction of kernel version 5.0 and the removal of the block layer legacy single queue I/O path, only the mq-deadline scheduler remains. Since kernel version 5.2, the selection of the CONFIG_MQ_IOSCHED_DEADLINE option is automatic when the CONFIG_BLK_DEV_ZONED configuration option is set.","title":"Write Ordering Control"},{"location":"linux/config/#device-mapper","text":"Zoned block device support for the device mapper subsystem is automatically enabled when the CONFIG_BLK_DEV_ZONED option is set. This will enables support for the dm-linear and dm-flakey targets. However, the dm-zoned device mapper target must be enabled to be usable. Enabling the dm-zoned target can be done by selecting the CONFIG_DM_ZONED option from the menu Device Drivers \u2192 Multiple devices driver support (RAID and LVM) \u2192 Device mapper support \u2192 Drive-managed zoned block device target support . dm-zoned device mapper target configuration with make menuconfig","title":"Device Mapper"},{"location":"linux/config/#zonefs-file-system","text":"Enabling compilation of the zonefs file system is done by selecting the CONFIG_ZONEFS_FS option from the menu File systems -> zonefs filesystem support . This option is available only and only if the CONFIG_BLK_DEV_ZONED option is set to enable zoned block device support. zonefs filesystem configuration with make menuconfig","title":"zonefs File System"},{"location":"linux/dm/","text":"Device Mapper Zoned block device support was added to the device mapper subsystem with kernel version 4.13. Two existing targets gained support: the dm-linear target and the dm-flakey target. ZBD support also added a new target driver, dm-zoned . dm-linear The dm-linear target maps a linear range of blocks of the device-mapper device onto a linear range of a backend device. dm-linear is the basic building block of logical volume managers such as LVM . Zoned Block Device Restrictions When used with zoned block devices, the dm-linear device created will also be a zoned block device with the same zone size as the underlying device. Several conditions are enforced by the device mapper core management code for the creation of a dm-linear target device. All backend devices used to map different ranges of the target device must have the same zone model. If the backend devices are zoned block devices, all devices must have the same zone size. The mapped ranges must be zone aligned, that is, partial zone mapping is not possible. Example: Creating a Small Host Managed Disk This example illustrates how to create a small host managed disk using zone ranges from a large high capacity host managed disk. The zone information of the backend device used is shown below. # cat /sys/block/sdb/queue/zoned host-managed # cat /sys/block/sdb/queue/chunk_sectors 524288 # blkzone report /dev/sdb start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x6d2300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] To create a dm-linear device named \"small-sdb\" joining the first 5 conventional zones of the backend device with the first 10 sequential zones, the following command can be used. # echo \"0 2621440 linear /dev/sdb 0 2621440 5242880 linear /dev/sdb 274726912\" | dmsetup create small-sdb The resulting device zone model is also host managed and has 15 zones as shown below. # cat /sys/block/dm-0/queue/zoned host-managed # cat /sys/block/dm-0/queue/chunk_sectors 524288 # blkzone report /dev/dm-0 start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000200000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000280000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000400000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000480000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000600000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] The following shows a script facilitating the creation of dm-linear devices using zone ranges from a single zoned block device. Such small zoned block devices are useful for testing applications limits (e.g. Disk full conditions). #!/bin/bash if [ $# != 3 ]; then echo \"Usage: $0 <disk> <num conv zones> <num seq zones>\" exit 1 fi disk=\"$1\" nrconv=$2 nrseq=$3 dname=\"`basename ${disk}`\" # Linear table entries: \"start length linear device offset\" # start: starting block in virtual device # length: length of this segment # device: block device, referenced by the device name or by major:minor # offset: starting offset of the mapping on the device convlen=$(( $nrconv * 524288 )) seqlen=$(( $nrseq * 524288 )) if [ $convlen -eq 0 ] && [ $seqlen -eq 0 ]; then echo \"0 zones...\" exit 1 fi seqofst=`zbc_report_zones $1 | grep \"Sequential-write-required\" | head -n1 | cut -f5 -d',' | cut -f3 -d' '` if [ $convlen -gt $seqofst ]; then nrconv=$(( $seqofst / 524288 )) echo \"Too many conventional zones requested: truncating to $nrconv\" convlen=$seqofst fi if [ $convlen -eq 0 ]; then echo \"0 ${seqlen} linear ${disk} ${seqofst}\" | dmsetup create small-${dname} elif [ $seqlen -eq 0 ]; then echo \"0 ${convlen} linear ${disk} 0\" | dmsetup create small-${dname} else echo \"0 ${convlen} linear ${disk} 0 ${convlen} ${seqlen} linear ${disk} ${seqofst}\" | dmsetup create small-${dname} fi Example: Conventional Zones as a Regular Disk dm-linear can also be used to aggregate a zoned block device conventional zones together into a target device that will be usable as a regular disk (conventional zones can be randomly written). Reusing the previous example backend disk, 524 conventional zones of 524288 sectors (512 B unit) are available. The following command creates a dm-linear device joining all conventional zones together. # echo \"0 274726912 linear /dev/sdb 0\" | dmsetup create small-sdb The target device is again a host managed disk but contains only conventional zones. # cat /sys/block/dm-0/queue/zoned host-managed # cat /sys/block/dm-0/queue/chunk_sectors 524288 # blkzone report /dev/dm-0 start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] This zoned block device being composed of only conventional zones, all sectors are randomly writable and can thus be used directly with any file system. # mkfs.ext4 /dev/dm-0 mke2fs 1.44.6 (5-Mar-2019) Creating filesystem with 34340864 4k blocks and 8585216 inodes Filesystem UUID: 3957429a-5dab-4b30-9797-f9736036a47b Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done # mount /dev/dm-0 /mnt # ls -l /mnt total 16 drwx------ 2 root root 16384 May 21 17:03 lost+found Applications needing frequent random updates to their metadata can use such setup to facilitate implementation of a complex metadata structure. The remaining sequential zones of the disk can be used directly by the application to store data. dm-flakey The dm-flakey target is similar to the dm-linear target except that it exhibits unreliable behavior periodically. This target is useful in simulating failing devices for testing purposes. In the case of zoned block devices, simulating write errors to sequential zones can help in debugging application write pointer management. Starting from the time the table is loaded, the device does not generate errors for some seconds ( up time), then exhibits unreliable behavior for down seconds. This cycle then repeats. Error modes Several error simulation behavior can be configured. drop_writes All write I/Os are silently ignored and dropped. Read I/Os are handled correctly. error_writes All write I/Os are failed with an error signaled. Read I/Os are handled correctly. corrupt_bio_byte During the down time, replace the Nth byte of the data of each read or write block I/O with a specified value. The default error mode is to fail all I/O requests during the down time of the simulation cycle. Zoned Block Device Restrictions The same restrictions as for the dm-linear target apply. Examples dm-linear detailed documentation and usage examples can be found in the kernel source code documentation file Documentation/device-mapper/dm-flakey.txt . dm-zoned The dm-zoned device mapper target provides random write access to zoned block devices (ZBC and ZAC compliant devices). It hides to the device user (a file system or an application doing raw block device accesses) the sequential write constraint of host-managed zoned block devices, allowing the use of applications and file systems that do not have native zoned block device support. File systems or applications that can natively support host-managed zoned block devices (e.g. the f2fs file system since kernel 4.10) do not need to use the dm-zoned device mapper target. Design Overview dm-zoned implements an on-disk write buffering scheme to handle random write accesses to sequential write required zones of a zoned block device. Conventional zones of the backend device are used for buffering random accesses, as well as for storing internal metadata. The figure below illustrates dm-zoned zone usage principle. Zone mapping overview of the dm-zoned device mapper target Optionally, since Linux kernel version 5.8.0, an additional regular block device can also be used to provide randomly writable storage used in place of the conventional zones of the backend zoned block device for write buffering. With this new version of dm-zoned , multiple zoned block devices can also be used to increase performance. All zones of the device(s) used to back a dm-zoned target are separated into 2 types: Metadata zones These are randomly writable zones used to store metadata. Randomly writable zones may be conventional zones or sequential write preferred zones (host-aware devices only). Metadata zones are not reported as usable capacity to the user. If an additional regular block device is used for write buffering, metadata zones are stored on this cache device. Data zones All remaining zones of the device. The majority of these zones will be sequential zones which are used used exclusively for storing user data. The conventional zones (or part of the sequential write preferred zones on a host-aware device) may be used also for buffering user random writes. User data may thus be stored either in conventional zone or in a sequential zone. As shown in the above figure, the target device is divided into chunks that have the same size as the zones of the backing zoned devices. A logical chunk can be mapped to zones of the backing device in different ways. Conventional or cache zone mapping This is the case for chunk A in the figure which is mapped to the conventional zone C A . This is the default mapping initialized when the first write command is issued to an empty (unwritten) chunk. As long as a chunk is mapped to a conventional zone, any incoming write request can be directly executed using the mapped conventional zone. Sequential zone mapping A chunk initially can also be mapped to a sequential zone as shown for the chunk C mapped to the sequential zone S C in the figure. With such mapping, a already written block of the chunk cannot be modified directly. To handle this case, the next mapping type is used. Dual conventional-sequential zone mapping To process data updates to written blocks of a chunk mapped to a sequential zone, a conventional zone may be temporarily added to the chunk mapping. Any write targeting a written block will be processed using the conventional zone rather than the sequential zone. dm-zoned metadata include a set of bitmaps to track the validity state of blocks in the backing device zones. Any write operation execution is always followed by an update to the bitmaps to mark the written blocks as valid. In the case of the dual conventional-sequential chunk mapping, the bitmap for the blocks of the sequential zone is also updated to clear the bits representing the blocks updated but written to the conventional zone. Doing so, incoming reads always gain access to the latest version of the block data by simply inspecting the block validity bitmaps. On-Disk Format dm-zoned exposes a logical device with a sector size of 4096 bytes, irrespectively of the physical sector size of the backend zoned block device being used. This allows reducing the amount of metadata needed to manage valid blocks (blocks written). The on-disk metadata format is as follows: The first block of the first randomly writable zone found contains the super block which describes the amount and position on disk of metadata blocks. Following the super block, a set of blocks is used to describe the mapping of the logical chunks of the target logical device to data zones. The mapping is indexed by logical chunk number and each mapping entry indicates the data zone storing the chunk data and optionally the zone number of a random zone used to buffer random modification to the chunk data. A set of blocks used to store bitmaps indicating the validity of blocks in the data zones follows the mapping table blocks. A valid block is a block that was writen and not discarded. For a buffered data zone, a block can be valid only in the data zone or in the buffer zone. To protect internal metadata against corruption in case of sudden power loss or system crash, two sets of metadata zones are used. One set, the primary set, is used as the main metadata set, while the secondary set is used as a log. Modified metadata are first written to the secondary set and the log so created validated by writing an updated super block in the secondary set. Once this log operation completes, updates in place of metadata blocks can be done in the primary metadata set, ensuring that one of the set is always correct. Flush operations are used as a commit point: upon reception of a flush operation, metadata activity is temporarily stopped, all dirty metadata logged and updated and normal operation resumed. This only temporarily delays write and discard requests. Read requests can be processed while metadata logging is executed. Read-Write Processing For a logical chunk mapped to a random data zone, all write operations are processed by directly writing to the data zone. If the mapping zone is to a sequential zone, the write operation is processed directly only and only if the write offset within the logical chunk is equal to the write pointer offset within of the sequential data zone (i.e. the write operation is aligned on the zone write pointer). Otherwise, write operations are processed indirectly using a buffer zone: a randomly writable free data zone is allocated and assigned to the chunk being accessed in addition to the already mapped sequential data zone. Writing block to the buffer zone will invalidate the same blocks in the sequential data zone. Read operations are processed according to the block validity information provided by the bitmaps: valid blocks are read either from the data zone or, if the data zone is buffered, from the buffer zone assigned to the data zone. Random Zone Reclaim After some time, the limited number of random zones available may be exhausted and unaligned writes to unbuffered zones become impossible. To avoid such situation, a reclaim process regularly scans used random zones and try to \"reclaim\" them by copying (sequentially) the valid blocks of the buffer zone to a free sequential zone. Once the copy completes, the chunk mapping is updated to point to the sequential zone and the buffer zone freed for reuse. To protect internal metadata against corruption in case of sudden power loss or system crash, 2 sets of metadata zones are used. One set, the primary set, is used as the main metadata repository, while the secondary set is used as a log. Modified metadata are first written to the secondary set and the log so created validated by writing an updated super block in the secondary set. Once this log operation completes, updates in place of metadata blocks can be done in the primary metadata set, ensuring that one of the set is always correct. Flush operations are used as a commit point: upon reception of a flush operation, metadata activity is temporarily stopped, all dirty metadata logged and updated and normal operation resumed. This only temporarily delays write and discard requests. Read requests can be processed while metadata logging is executed. Userspace Tool The dmzadm command line utility is used to format backend zoned devices for use with the dm-zoned device mapper target. This utility will verify the device zone model and will prepare and write on-disk dm-zoned metadata according to the device capacity, zone size, etc. The source code for the dmzadm utility is available as part of the dm-zoned-tools project hosted on GitHub . The project README file provides instructions on how to compile and install the utility. Note The dm-zoned-tools project was formerly hosted on GitHub as part of the HGST organization . dm-zoned-tools repository has since then moved to the Western Digital Corporation organization on GitHub . dmzadm detailed usage is as follows. # dmzadm --help dmzadm allows formatting, checking and repairing a zoned block device for use with the dm-zoned device mapper. Usage: dmzadm <operation> <device(s)> [options] Operations --help | -h : General help message --format : Format a block device metadata --check : Check a block device metadata --repair : Repair a block device metadata --start : Start the device-mapper target --stop : Stop the device-mapper target Devices For a single device target, a zoned block device must be specified. For a multi-device target, a a list of block devices must be specified, with a regular block device as the first device specified, followed by one or more zoned block devices General options --verbose : Verbose output --vverbose : Very verbose output Format operation options --force : Force overwrite of existing content --label=<str> : Set the target label name to <str> --seq=<num> : Number of sequential zones reserved for reclaim. The minimum is 1 and the default is 16 Formatting a Target Device Formatting a single device target is done using the command. # dmzadm --format /dev/<disk name> where /dev/<disk name> identifies the backend zoned block device to use. An example execution using a SMR hard-disk is shown below. # dmzadm --format /dev/sdi /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 0 55880 zones of 524288 512-byte sectors (256 MiB) 65536 4KB data blocks per zone Resetting sequential zones Writing primary metadata set Writing mapping table Writing bitmap blocks Writing super block to sdi block 0 Writing secondary metadata set Writing mapping table Writing bitmap blocks Writing super block to sdi block 131072 Syncing disk Done. Starting with Linux kernel v5.8.0, regular block devices such as SSDs can also be used together with zoned block devices with dm-zoned . In this case, conventional zones are emulated for the regular block device to hold dm-zoned metadata and for buffering data. When a regular block device is used, the zone reclaim process operates by copying data from emulated conventional zones on the regular block device to zones of the zoned block device. This dual-drive configuration can significantly increase performance of the target device under write-intensive workloads. To format a dm-zoned target device using an additional regular block device and optionally several zoned block devices, the following commands can be used. # dmzadm --format /dev/nvme2n1 /dev/sdi /dev/nvme2n1: 976773168 512-byte sectors (465 GiB) Regular block device 1864 zones, offset 0 /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 122159104 57743 zones of 524288 512-byte sectors (256 MiB) 1 runt zone of 24624 512-byte sectors (12 MiB) 65536 4KB data blocks per zone Resetting sequential zones Writing primary metadata set Writing mapping table Writing bitmap blocks Writing super block to nvme2n1 block 0 Writing secondary metadata set Writing mapping table Writing bitmap blocks Writing super block to nvme2n1 block 131072 Writing tertiary metadata Writing super block to sdi block 0 Syncing disk Syncing disk Done. Where /dev/nvme2n1 is in this example a NVMe SSD and /dev/sdi is a host managed SMR hard-disk. Activating a Target Device The dm-zoned target device using a formatted zoned device or set of devices can be started by executing dmzadm with the --start command. # dmzadm --start /dev/sdi /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 0 55880 zones of 524288 512-byte sectors (256 MiB) 65536 4KB data blocks per zone sdi: starting dmz-sdi uuid 8c505b4b-d1e9-47a7-8e3a-8b1c00317eaf The target start can be confirmed by looking at the kernel messages. # dmesg ... device-mapper: zoned metadata: (dmz-sdi): DM-Zoned metadata version 2 device-mapper: zoned metadata: (sdi): Host-managed zoned block device device-mapper: zoned metadata: (sdi): 29297213440 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (sdi): 55880 zones of 524288 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (dmz-sdi): 55880 zones of 524288 512-byte logical sectors device-mapper: zoned: (dmz-sdi): Target device: 29286727680 512-byte logical sectors (3660840960 blocks) The target device created is a regular disk that can be used with any file system. # cat /sys/block/dm-0/queue/zoned none # mkfs.ext4 /dev/dm-0 mke2fs 1.45.5 (07-Jan-2020) Discarding device blocks: done Creating filesystem with 3660840960 4k blocks and 457605120 inodes Filesystem UUID: d49ed278-5bca-46c4-8ce2-cec263dd060c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544, 1934917632, 2560000000 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done # mount /dev/dm-0 /mnt # ls -l /mnt total 16 drwx------ 2 root root 16384 Aug 27 15:14 lost+found For a multi-device target, the same list of devices as used for format must be specified. # dmzadm --start /dev/nvmen2p1 /dev/sdi /dev/nvme2n1: 976773168 512-byte sectors (465 GiB) Regular block device 1864 zones, offset 0 /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 122159104 57743 zones of 524288 512-byte sectors (256 MiB) 1 runt zone of 24624 512-byte sectors (12 MiB) 65536 4KB data blocks per zone nvme2n1: starting dmz-nvme2n1 uuid ffbd1a3a-d79b-4d7f-bc13-e475a157bc39 Similarly to the single device case, kernel messages notify the target device activation. device-mapper: zoned metadata: (dmz-nvme2n1): DM-Zoned metadata version 2 device-mapper: zoned metadata: (nvme2n1): Regular block device device-mapper: zoned metadata: (nvme2n1): 976773168 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (nvme2n1): 1864 zones of 524288 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (sdi): Host-managed zoned block device device-mapper: zoned metadata: (sdi): 29297213440 512-byte logical sectors (offset 977272832) device-mapper: zoned metadata: (sdi): 55880 zones of 524288 512-byte logical sectors (offset 1864) device-mapper: zoned metadata: (dmz-nvme2n1): 57744 zones of 524288 512-byte logical sectors device-mapper: zoned: (dmz-nvme2n1): Target device: 30264000512 512-byte logical sectors (3783000064 blocks) Stopping a Target Device A dm-zoned target device can be disabled using the --stop operation. # dmzadm --stop /dev/sdX For a multi-device target, the same list of devices as used for format must be specified.","title":"Device Mapper"},{"location":"linux/dm/#device-mapper","text":"Zoned block device support was added to the device mapper subsystem with kernel version 4.13. Two existing targets gained support: the dm-linear target and the dm-flakey target. ZBD support also added a new target driver, dm-zoned .","title":"Device Mapper"},{"location":"linux/dm/#dm-linear","text":"The dm-linear target maps a linear range of blocks of the device-mapper device onto a linear range of a backend device. dm-linear is the basic building block of logical volume managers such as LVM .","title":"dm-linear"},{"location":"linux/dm/#zoned-block-device-restrictions","text":"When used with zoned block devices, the dm-linear device created will also be a zoned block device with the same zone size as the underlying device. Several conditions are enforced by the device mapper core management code for the creation of a dm-linear target device. All backend devices used to map different ranges of the target device must have the same zone model. If the backend devices are zoned block devices, all devices must have the same zone size. The mapped ranges must be zone aligned, that is, partial zone mapping is not possible.","title":"Zoned Block Device Restrictions"},{"location":"linux/dm/#example-creating-a-small-host-managed-disk","text":"This example illustrates how to create a small host managed disk using zone ranges from a large high capacity host managed disk. The zone information of the backend device used is shown below. # cat /sys/block/sdb/queue/zoned host-managed # cat /sys/block/sdb/queue/chunk_sectors 524288 # blkzone report /dev/sdb start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... start: 0x6d2300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x6d2380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] To create a dm-linear device named \"small-sdb\" joining the first 5 conventional zones of the backend device with the first 10 sequential zones, the following command can be used. # echo \"0 2621440 linear /dev/sdb 0 2621440 5242880 linear /dev/sdb 274726912\" | dmsetup create small-sdb The resulting device zone model is also host managed and has 15 zones as shown below. # cat /sys/block/dm-0/queue/zoned host-managed # cat /sys/block/dm-0/queue/chunk_sectors 524288 # blkzone report /dev/dm-0 start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000200000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000280000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000300000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000380000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000400000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000480000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000600000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000680000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x000700000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] The following shows a script facilitating the creation of dm-linear devices using zone ranges from a single zoned block device. Such small zoned block devices are useful for testing applications limits (e.g. Disk full conditions). #!/bin/bash if [ $# != 3 ]; then echo \"Usage: $0 <disk> <num conv zones> <num seq zones>\" exit 1 fi disk=\"$1\" nrconv=$2 nrseq=$3 dname=\"`basename ${disk}`\" # Linear table entries: \"start length linear device offset\" # start: starting block in virtual device # length: length of this segment # device: block device, referenced by the device name or by major:minor # offset: starting offset of the mapping on the device convlen=$(( $nrconv * 524288 )) seqlen=$(( $nrseq * 524288 )) if [ $convlen -eq 0 ] && [ $seqlen -eq 0 ]; then echo \"0 zones...\" exit 1 fi seqofst=`zbc_report_zones $1 | grep \"Sequential-write-required\" | head -n1 | cut -f5 -d',' | cut -f3 -d' '` if [ $convlen -gt $seqofst ]; then nrconv=$(( $seqofst / 524288 )) echo \"Too many conventional zones requested: truncating to $nrconv\" convlen=$seqofst fi if [ $convlen -eq 0 ]; then echo \"0 ${seqlen} linear ${disk} ${seqofst}\" | dmsetup create small-${dname} elif [ $seqlen -eq 0 ]; then echo \"0 ${convlen} linear ${disk} 0\" | dmsetup create small-${dname} else echo \"0 ${convlen} linear ${disk} 0 ${convlen} ${seqlen} linear ${disk} ${seqofst}\" | dmsetup create small-${dname} fi","title":"Example: Creating a Small Host Managed Disk"},{"location":"linux/dm/#example-conventional-zones-as-a-regular-disk","text":"dm-linear can also be used to aggregate a zoned block device conventional zones together into a target device that will be usable as a regular disk (conventional zones can be randomly written). Reusing the previous example backend disk, 524 conventional zones of 524288 sectors (512 B unit) are available. The following command creates a dm-linear device joining all conventional zones together. # echo \"0 274726912 linear /dev/sdb 0\" | dmsetup create small-sdb The target device is again a host managed disk but contains only conventional zones. # cat /sys/block/dm-0/queue/zoned host-managed # cat /sys/block/dm-0/queue/chunk_sectors 524288 # blkzone report /dev/dm-0 start: 0x000000000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010500000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] This zoned block device being composed of only conventional zones, all sectors are randomly writable and can thus be used directly with any file system. # mkfs.ext4 /dev/dm-0 mke2fs 1.44.6 (5-Mar-2019) Creating filesystem with 34340864 4k blocks and 8585216 inodes Filesystem UUID: 3957429a-5dab-4b30-9797-f9736036a47b Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done # mount /dev/dm-0 /mnt # ls -l /mnt total 16 drwx------ 2 root root 16384 May 21 17:03 lost+found Applications needing frequent random updates to their metadata can use such setup to facilitate implementation of a complex metadata structure. The remaining sequential zones of the disk can be used directly by the application to store data.","title":"Example: Conventional Zones as a Regular Disk"},{"location":"linux/dm/#dm-flakey","text":"The dm-flakey target is similar to the dm-linear target except that it exhibits unreliable behavior periodically. This target is useful in simulating failing devices for testing purposes. In the case of zoned block devices, simulating write errors to sequential zones can help in debugging application write pointer management. Starting from the time the table is loaded, the device does not generate errors for some seconds ( up time), then exhibits unreliable behavior for down seconds. This cycle then repeats.","title":"dm-flakey"},{"location":"linux/dm/#error-modes","text":"Several error simulation behavior can be configured. drop_writes All write I/Os are silently ignored and dropped. Read I/Os are handled correctly. error_writes All write I/Os are failed with an error signaled. Read I/Os are handled correctly. corrupt_bio_byte During the down time, replace the Nth byte of the data of each read or write block I/O with a specified value. The default error mode is to fail all I/O requests during the down time of the simulation cycle.","title":"Error modes"},{"location":"linux/dm/#zoned-block-device-restrictions_1","text":"The same restrictions as for the dm-linear target apply.","title":"Zoned Block Device Restrictions"},{"location":"linux/dm/#examples","text":"dm-linear detailed documentation and usage examples can be found in the kernel source code documentation file Documentation/device-mapper/dm-flakey.txt .","title":"Examples"},{"location":"linux/dm/#dm-zoned","text":"The dm-zoned device mapper target provides random write access to zoned block devices (ZBC and ZAC compliant devices). It hides to the device user (a file system or an application doing raw block device accesses) the sequential write constraint of host-managed zoned block devices, allowing the use of applications and file systems that do not have native zoned block device support. File systems or applications that can natively support host-managed zoned block devices (e.g. the f2fs file system since kernel 4.10) do not need to use the dm-zoned device mapper target.","title":"dm-zoned"},{"location":"linux/dm/#design-overview","text":"dm-zoned implements an on-disk write buffering scheme to handle random write accesses to sequential write required zones of a zoned block device. Conventional zones of the backend device are used for buffering random accesses, as well as for storing internal metadata. The figure below illustrates dm-zoned zone usage principle. Zone mapping overview of the dm-zoned device mapper target Optionally, since Linux kernel version 5.8.0, an additional regular block device can also be used to provide randomly writable storage used in place of the conventional zones of the backend zoned block device for write buffering. With this new version of dm-zoned , multiple zoned block devices can also be used to increase performance. All zones of the device(s) used to back a dm-zoned target are separated into 2 types: Metadata zones These are randomly writable zones used to store metadata. Randomly writable zones may be conventional zones or sequential write preferred zones (host-aware devices only). Metadata zones are not reported as usable capacity to the user. If an additional regular block device is used for write buffering, metadata zones are stored on this cache device. Data zones All remaining zones of the device. The majority of these zones will be sequential zones which are used used exclusively for storing user data. The conventional zones (or part of the sequential write preferred zones on a host-aware device) may be used also for buffering user random writes. User data may thus be stored either in conventional zone or in a sequential zone. As shown in the above figure, the target device is divided into chunks that have the same size as the zones of the backing zoned devices. A logical chunk can be mapped to zones of the backing device in different ways. Conventional or cache zone mapping This is the case for chunk A in the figure which is mapped to the conventional zone C A . This is the default mapping initialized when the first write command is issued to an empty (unwritten) chunk. As long as a chunk is mapped to a conventional zone, any incoming write request can be directly executed using the mapped conventional zone. Sequential zone mapping A chunk initially can also be mapped to a sequential zone as shown for the chunk C mapped to the sequential zone S C in the figure. With such mapping, a already written block of the chunk cannot be modified directly. To handle this case, the next mapping type is used. Dual conventional-sequential zone mapping To process data updates to written blocks of a chunk mapped to a sequential zone, a conventional zone may be temporarily added to the chunk mapping. Any write targeting a written block will be processed using the conventional zone rather than the sequential zone. dm-zoned metadata include a set of bitmaps to track the validity state of blocks in the backing device zones. Any write operation execution is always followed by an update to the bitmaps to mark the written blocks as valid. In the case of the dual conventional-sequential chunk mapping, the bitmap for the blocks of the sequential zone is also updated to clear the bits representing the blocks updated but written to the conventional zone. Doing so, incoming reads always gain access to the latest version of the block data by simply inspecting the block validity bitmaps.","title":"Design Overview"},{"location":"linux/dm/#on-disk-format","text":"dm-zoned exposes a logical device with a sector size of 4096 bytes, irrespectively of the physical sector size of the backend zoned block device being used. This allows reducing the amount of metadata needed to manage valid blocks (blocks written). The on-disk metadata format is as follows: The first block of the first randomly writable zone found contains the super block which describes the amount and position on disk of metadata blocks. Following the super block, a set of blocks is used to describe the mapping of the logical chunks of the target logical device to data zones. The mapping is indexed by logical chunk number and each mapping entry indicates the data zone storing the chunk data and optionally the zone number of a random zone used to buffer random modification to the chunk data. A set of blocks used to store bitmaps indicating the validity of blocks in the data zones follows the mapping table blocks. A valid block is a block that was writen and not discarded. For a buffered data zone, a block can be valid only in the data zone or in the buffer zone. To protect internal metadata against corruption in case of sudden power loss or system crash, two sets of metadata zones are used. One set, the primary set, is used as the main metadata set, while the secondary set is used as a log. Modified metadata are first written to the secondary set and the log so created validated by writing an updated super block in the secondary set. Once this log operation completes, updates in place of metadata blocks can be done in the primary metadata set, ensuring that one of the set is always correct. Flush operations are used as a commit point: upon reception of a flush operation, metadata activity is temporarily stopped, all dirty metadata logged and updated and normal operation resumed. This only temporarily delays write and discard requests. Read requests can be processed while metadata logging is executed.","title":"On-Disk Format"},{"location":"linux/dm/#read-write-processing","text":"For a logical chunk mapped to a random data zone, all write operations are processed by directly writing to the data zone. If the mapping zone is to a sequential zone, the write operation is processed directly only and only if the write offset within the logical chunk is equal to the write pointer offset within of the sequential data zone (i.e. the write operation is aligned on the zone write pointer). Otherwise, write operations are processed indirectly using a buffer zone: a randomly writable free data zone is allocated and assigned to the chunk being accessed in addition to the already mapped sequential data zone. Writing block to the buffer zone will invalidate the same blocks in the sequential data zone. Read operations are processed according to the block validity information provided by the bitmaps: valid blocks are read either from the data zone or, if the data zone is buffered, from the buffer zone assigned to the data zone.","title":"Read-Write Processing"},{"location":"linux/dm/#random-zone-reclaim","text":"After some time, the limited number of random zones available may be exhausted and unaligned writes to unbuffered zones become impossible. To avoid such situation, a reclaim process regularly scans used random zones and try to \"reclaim\" them by copying (sequentially) the valid blocks of the buffer zone to a free sequential zone. Once the copy completes, the chunk mapping is updated to point to the sequential zone and the buffer zone freed for reuse. To protect internal metadata against corruption in case of sudden power loss or system crash, 2 sets of metadata zones are used. One set, the primary set, is used as the main metadata repository, while the secondary set is used as a log. Modified metadata are first written to the secondary set and the log so created validated by writing an updated super block in the secondary set. Once this log operation completes, updates in place of metadata blocks can be done in the primary metadata set, ensuring that one of the set is always correct. Flush operations are used as a commit point: upon reception of a flush operation, metadata activity is temporarily stopped, all dirty metadata logged and updated and normal operation resumed. This only temporarily delays write and discard requests. Read requests can be processed while metadata logging is executed.","title":"Random Zone Reclaim"},{"location":"linux/dm/#userspace-tool","text":"The dmzadm command line utility is used to format backend zoned devices for use with the dm-zoned device mapper target. This utility will verify the device zone model and will prepare and write on-disk dm-zoned metadata according to the device capacity, zone size, etc. The source code for the dmzadm utility is available as part of the dm-zoned-tools project hosted on GitHub . The project README file provides instructions on how to compile and install the utility. Note The dm-zoned-tools project was formerly hosted on GitHub as part of the HGST organization . dm-zoned-tools repository has since then moved to the Western Digital Corporation organization on GitHub . dmzadm detailed usage is as follows. # dmzadm --help dmzadm allows formatting, checking and repairing a zoned block device for use with the dm-zoned device mapper. Usage: dmzadm <operation> <device(s)> [options] Operations --help | -h : General help message --format : Format a block device metadata --check : Check a block device metadata --repair : Repair a block device metadata --start : Start the device-mapper target --stop : Stop the device-mapper target Devices For a single device target, a zoned block device must be specified. For a multi-device target, a a list of block devices must be specified, with a regular block device as the first device specified, followed by one or more zoned block devices General options --verbose : Verbose output --vverbose : Very verbose output Format operation options --force : Force overwrite of existing content --label=<str> : Set the target label name to <str> --seq=<num> : Number of sequential zones reserved for reclaim. The minimum is 1 and the default is 16","title":"Userspace Tool"},{"location":"linux/dm/#formatting-a-target-device","text":"Formatting a single device target is done using the command. # dmzadm --format /dev/<disk name> where /dev/<disk name> identifies the backend zoned block device to use. An example execution using a SMR hard-disk is shown below. # dmzadm --format /dev/sdi /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 0 55880 zones of 524288 512-byte sectors (256 MiB) 65536 4KB data blocks per zone Resetting sequential zones Writing primary metadata set Writing mapping table Writing bitmap blocks Writing super block to sdi block 0 Writing secondary metadata set Writing mapping table Writing bitmap blocks Writing super block to sdi block 131072 Syncing disk Done. Starting with Linux kernel v5.8.0, regular block devices such as SSDs can also be used together with zoned block devices with dm-zoned . In this case, conventional zones are emulated for the regular block device to hold dm-zoned metadata and for buffering data. When a regular block device is used, the zone reclaim process operates by copying data from emulated conventional zones on the regular block device to zones of the zoned block device. This dual-drive configuration can significantly increase performance of the target device under write-intensive workloads. To format a dm-zoned target device using an additional regular block device and optionally several zoned block devices, the following commands can be used. # dmzadm --format /dev/nvme2n1 /dev/sdi /dev/nvme2n1: 976773168 512-byte sectors (465 GiB) Regular block device 1864 zones, offset 0 /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 122159104 57743 zones of 524288 512-byte sectors (256 MiB) 1 runt zone of 24624 512-byte sectors (12 MiB) 65536 4KB data blocks per zone Resetting sequential zones Writing primary metadata set Writing mapping table Writing bitmap blocks Writing super block to nvme2n1 block 0 Writing secondary metadata set Writing mapping table Writing bitmap blocks Writing super block to nvme2n1 block 131072 Writing tertiary metadata Writing super block to sdi block 0 Syncing disk Syncing disk Done. Where /dev/nvme2n1 is in this example a NVMe SSD and /dev/sdi is a host managed SMR hard-disk.","title":"Formatting a Target Device"},{"location":"linux/dm/#activating-a-target-device","text":"The dm-zoned target device using a formatted zoned device or set of devices can be started by executing dmzadm with the --start command. # dmzadm --start /dev/sdi /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 0 55880 zones of 524288 512-byte sectors (256 MiB) 65536 4KB data blocks per zone sdi: starting dmz-sdi uuid 8c505b4b-d1e9-47a7-8e3a-8b1c00317eaf The target start can be confirmed by looking at the kernel messages. # dmesg ... device-mapper: zoned metadata: (dmz-sdi): DM-Zoned metadata version 2 device-mapper: zoned metadata: (sdi): Host-managed zoned block device device-mapper: zoned metadata: (sdi): 29297213440 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (sdi): 55880 zones of 524288 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (dmz-sdi): 55880 zones of 524288 512-byte logical sectors device-mapper: zoned: (dmz-sdi): Target device: 29286727680 512-byte logical sectors (3660840960 blocks) The target device created is a regular disk that can be used with any file system. # cat /sys/block/dm-0/queue/zoned none # mkfs.ext4 /dev/dm-0 mke2fs 1.45.5 (07-Jan-2020) Discarding device blocks: done Creating filesystem with 3660840960 4k blocks and 457605120 inodes Filesystem UUID: d49ed278-5bca-46c4-8ce2-cec263dd060c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544, 1934917632, 2560000000 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done # mount /dev/dm-0 /mnt # ls -l /mnt total 16 drwx------ 2 root root 16384 Aug 27 15:14 lost+found For a multi-device target, the same list of devices as used for format must be specified. # dmzadm --start /dev/nvmen2p1 /dev/sdi /dev/nvme2n1: 976773168 512-byte sectors (465 GiB) Regular block device 1864 zones, offset 0 /dev/sdi: 29297213440 512-byte sectors (13970 GiB) Host-managed device 55880 zones, offset 122159104 57743 zones of 524288 512-byte sectors (256 MiB) 1 runt zone of 24624 512-byte sectors (12 MiB) 65536 4KB data blocks per zone nvme2n1: starting dmz-nvme2n1 uuid ffbd1a3a-d79b-4d7f-bc13-e475a157bc39 Similarly to the single device case, kernel messages notify the target device activation. device-mapper: zoned metadata: (dmz-nvme2n1): DM-Zoned metadata version 2 device-mapper: zoned metadata: (nvme2n1): Regular block device device-mapper: zoned metadata: (nvme2n1): 976773168 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (nvme2n1): 1864 zones of 524288 512-byte logical sectors (offset 0) device-mapper: zoned metadata: (sdi): Host-managed zoned block device device-mapper: zoned metadata: (sdi): 29297213440 512-byte logical sectors (offset 977272832) device-mapper: zoned metadata: (sdi): 55880 zones of 524288 512-byte logical sectors (offset 1864) device-mapper: zoned metadata: (dmz-nvme2n1): 57744 zones of 524288 512-byte logical sectors device-mapper: zoned: (dmz-nvme2n1): Target device: 30264000512 512-byte logical sectors (3783000064 blocks)","title":"Activating a Target Device"},{"location":"linux/dm/#stopping-a-target-device","text":"A dm-zoned target device can be disabled using the --stop operation. # dmzadm --stop /dev/sdX For a multi-device target, the same list of devices as used for format must be specified.","title":"Stopping a Target Device"},{"location":"linux/fs/","text":"File Systems and Zoned Block Devices The dm-zoned device mapper target allows using any file system with host managed zoned block devices by hiding the device sequential write constraints. This is a simple solution to enable a file system use but not necessarily the most efficient due to the potentially high overhead of a block based zone reclaim process. Supporting zoned block devices directly in a file system implementation can lead to a more efficient zone reclaim processing as the file system metadata and file abstraction provide more information on the usage and validity status of storage blocks compared to the raw block device based approach. Furthermore, a file system design may lend itself well to the sequential write constraint of host managed zoned block devices. This is the case for log-structured file systems such as f2fs and copy-on-write (CoW) file systems such as Btrfs . zonefs zonefs is a very simple file system exposing each zone of a zoned block device as a file. zonefs is included with the upstream Linux kernel since version 5.6.0. Overview Unlike a regular POSIX-compliant file system with native zoned block device support (e.g. f2fs ), zonefs does not hide the sequential write constraint of zoned block devices to the user. Files representing sequential write zones of the device must be written sequentially starting from the end of the file (append only writes). As such, zonefs is in essence closer to a raw block device access interface than to a full-featured POSIX file system. The goal of zonefs is to simplify the implementation of zoned block device support in applications by replacing raw block device file accesses with the richer regular file API, avoiding relying on direct block device file ioctls which may be more obscure to developers. One example of this approach is the implementation of LSM (log-structured merge) tree structures (such as used in RocksDB and LevelDB) on zoned block devices by allowing SSTables to be stored in a zone file similarly to a regular file system rather than as a range of sectors of the entire disk. The introduction of the higher level construct \"one file is one zone\" can help reducing the amount of changes needed in the application as well as introducing support for different application programming languages. The files representing zones are grouped by zone type, which are themselves represented by sub-directories. This file structure is built entirely using zone information provided by the device and so does not require any complex on-disk metadata structure. On-Disk Metadata zonefs on-disk metadata is composed only of an immutable super block which persistently stores a magic number and optional feature flags and values. On mount, zonefs uses the block layer API function blkdev_report_zones() to obtain the device zone configuration and populates the mount point with a static file tree solely based on this information. File sizes come from the device zone type and write pointer position managed by the device itself. The super block is always written on disk at sector 0. The first zone of the device storing the super block is never exposed as a zone file by zonefs . If the zone containing the super block is a sequential zone, the mkzonefs format tool always \"finishes\" the zone, that is, it transitions the zone to a full state to make it read-only, preventing any data write. Zone Type Sub-Directories Files representing zones of the same type are grouped together under the same sub-directory automatically created on mount. For conventional zones, the sub-directory \"cnv\" is used. This directory is however created if and only if the device has usable conventional zones. If the device only has a single conventional zone at sector 0, the zone will not be exposed as a file as it will be used to store the zonefs super block. For such devices, the \"cnv\" sub-directory will not be created. For sequential write zones, the sub-directory \"seq\" is used. These two directories are the only directories that exist in zonefs . Users cannot create other directories and cannot rename nor delete the \"cnv\" and \"seq\" sub-directories. The size of the directories indicated by the st_size field of struct stat , obtained with the stat() or fstat() system calls, indicates the number of files existing under the directory. Zone files Zone files are named using the number of the zone they represent within the set of zones of a particular type. That is, both the \"cnv\" and \"seq\" directories contain files named \"0\", \"1\", \"2\", ... The file numbers also represent increasing zone start sector on the device. All read and write operations to zone files are not allowed beyond the file maximum size, that is, beyond the zone size. Any access exceeding the zone size is failed with the -EFBIG error. Creating, deleting, renaming or modifying any attribute of files is not allowed. The number of blocks of a file as reported by stat() and fstat() indicates the size of the file zone, or in other words, the maximum file size. Conventional Zone Files The size of conventional zone files is fixed to the size of the zone they represent. Conventional zone files cannot be truncated. These files can be randomly read and written using any type of I/O operation: buffered I/Os, direct I/Os, memory mapped I/Os (mmap), etc. There are no I/O constraint for these files beyond the file size limit mentioned above. Sequential zone files The size of sequential zone files grouped in the \"seq\" sub-directory represents the file's zone write pointer position relative to the zone start sector. Sequential zone files can only be written sequentially, starting from the file end, that is, write operations can only be append writes. Zonefs makes no attempt at accepting random writes and will fail any write request that has a start offset not corresponding to the end of the file, or to the end of the last write issued and still in-flight (for asynchronous I/O operations). Since dirty page writeback by the page cache does not guarantee a sequential write pattern, zonefs prevents buffered writes and writeable shared mappings on sequential files. Only direct I/O writes are accepted for these files. zonefs relies on the sequential delivery of write I/O requests to the device implemented by the block layer elevator (See Write Command Ordering ). There are no restrictions on the type of I/O used for read operations in sequential zone files. Buffered I/Os, direct I/Os and shared read mappings are all accepted. Truncating sequential zone files is allowed only down to 0, in which case, the zone is reset to rewind the file zone write pointer position to the start of the zone, or up to the zone size, in which case the file's zone is transitioned to the FULL state (finish zone operation). Format options Several optional features of zonefs can be enabled at format time. Conventional zone aggregation: ranges of contiguous conventional zones can be aggregated into a single larger file instead of the default one file per zone. File ownership: The owner UID and GID of zone files is by default 0 (root) but can be changed to any valid UID/GID. File access permissions: the default 640 access permissions can be changed. IO error handling Zoned block devices may fail I/O requests for reasons similar to regular block devices, e.g. due to bad sectors. However, in addition to such known I/O failure pattern, the standards governing zoned block devices behavior define additional conditions that can result in I/O errors. A zone may transition to the read-only condition: While the data already written in the zone is still readable, the zone can no longer be written. No user action on the zone (zone management command or read/write access) can change the zone condition back to a normal read/write state. While the reasons for the device to transition a zone to read-only state are not defined by the standards, a typical cause for such transition would be a defective write head on an HDD (all zones under this head are changed to read-only). A zone may transition to the offline condition: An offline zone cannot be read nor written. No user action can transition an offline zone back to an operational good state. Similarly to zone read-only transitions, the reasons for a drive to transition a zone to the offline condition are undefined. A typical cause would be a defective read-write head on an HDD causing all zones on the platter under the broken head to be inaccessible. Unaligned write errors: These errors result from the host issuing write requests with a start sector that does not correspond to a zone write pointer position when the write request is executed by the device. Even though zonefs enforces sequential file write for sequential zones, unaligned write errors may still happen in the case of a partial failure of a very large direct I/O operation split into multiple BIOs/requests or asynchronous I/O operations. If one of the write request within the set of sequential write requests issued to the device fails, all write requests queued after it will become unaligned and fail. Delayed write errors: Similarly to regular block devices, if the device side write cache is enabled, write errors may occur in ranges of previously completed writes when the device write cache is flushed, e.g. on fsync() . Similarly to the previous immediate unaligned write error case, delayed write errors can propagate through a stream of cached sequential data for a zone causing all data to be dropped after the sector that caused the error. All I/O errors detected by zonefs are notified to the user with an error code return for the system call that triggered or detected the error. The recovery actions taken by zonefs in response to I/O errors depend on the I/O type (read vs write) and on the reason for the error (bad sector, unaligned writes or zone condition change). For read I/O errors, zonefs does not execute any particular recovery action, but only if the file zone is still in a good condition and there is no inconsistency between the file inode size and its zone write pointer position. If a problem is detected, I/O error recovery is executed (see below table). For write I/O errors, zonefs I/O error recovery is always executed. A zone condition change to read-only or offline also always triggers zonefs I/O error recovery. zonefs minimal I/O error recovery may change a file size and file access permissions. File size changes: Immediate or delayed write errors in a sequential zone file may cause the file inode size to be inconsistent with the amount of data successfully written in the file zone. For instance, the partial failure of a multi-BIO large write operation will cause the zone write pointer to advance partially, even though the entire write operation will be reported as failed to the user. In such case, the file inode size must be advanced to reflect the zone write pointer change and eventually allow the user to restart writing at the end of the file. A file size may also be reduced to reflect a delayed write error detected on fsync(): in this case, the amount of data effectively written in the zone may be less than originally indicated by the file inode size. After such I/O error, zonefs always fixes the file inode size to reflect the amount of data persistently stored in the file zone. Access permission changes: A zone condition change to read-only is indicated with a change in the file access permissions to render the file read-only. This disables changes to the file attributes and data modification. For offline zones, all permissions (read and write) to the file are disabled. Further action taken by zonefs I/O error recovery can be controlled by the user with the \"errors=xxx\" mount option. The table below summarizes the result of zonefs I/O error processing depending on the mount option and on the zone conditions. \"errors=xxx\" mount option device zone condition file size file read file write device read device write remount-ro good fixed yes no yes yes remount-ro read-only as is yes no yes no remount-ro offline 0 no no no no zone-ro good fixed yes no yes yes zone-ro read-only as is yes no yes no zone-ro offline 0 no no no no zone-offline good 0 no no yes yes zone-offline read-only 0 no no yes no zone-offline offline 0 no no no no repair good fixed yes yes yes yes repair read-only as is yes no yes no repair offline 0 no no no no Further notes: The \"errors=remount-ro\" mount option is the default behavior of zonefs I/O error processing if no errors mount option is specified. With the \"errors=remount-ro\" mount option, the change of the file access permissions to read-only applies to all files. The file system is remounted read-only. Access permission and file size changes due to the device transitioning zones to the offline condition are permanent. Remounting or reformatting the device with mkfs.zonefs (mkzonefs) will not change back offline zone files to a good state. File access permission changes to read-only due to the device transitioning zones to the read-only condition are permanent. Remounting or reformatting the device will not re-enable file write access. File access permission changes implied by the remount-ro, zone-ro and zone-offline mount options are temporary for zones in a good condition. Unmounting and remounting the file system will restore the previous default (format time values) access rights to the files affected. The repair mount option triggers only the minimal set of I/O error recovery actions, that is, file size fixes for zones in a good condition. Zones indicated as being read-only or offline by the device still imply changes to the zone file access permissions as noted in the table above. Mount options zonefs define the \"errors= \" mount option to allow the user to specify zonefs behavior in response to I/O errors, inode size inconsistencies or zone condition changes. The defined behaviors are as follow: remount-ro (default) zone-ro zone-offline repair The run-time I/O error actions defined for each behavior are detailed in the previous section. Mount time I/O errors will cause the mount operation to fail. The handling of read-only zones also differs between mount-time and run-time. If a read-only zone is found at mount time, the zone is always treated in the same manner as offline zones, that is, all accesses are disabled and the zone file size set to 0. This is necessary as the write pointer of read-only zones is defined as invalib by the ZBC and ZAC standards, making it impossible to discover the amount of data that has been written to the zone. In the case of a read-only zone discovered at run-time, as indicated in the previous section. the size of the zone file is left unchanged from its last updated value. Zonefs User Space Tools The mkzonefs tool is used to format zoned block devices for use with zonefs . This tool is available on GitHub . zonefs-tools also includes a test suite which can be run against any zoned block device, including nullblk block device created with zoned mode . Examples The following formats a 15TB host-managed SMR HDD with 256 MB zones with the conventional zones aggregation feature enabled:: # mkzonefs -o aggr_cnv /dev/sdX # mount -t zonefs /dev/sdX /mnt # ls -l /mnt/ total 0 dr-xr-xr-x 2 root root 1 Nov 25 13:23 cnv dr-xr-xr-x 2 root root 55356 Nov 25 13:23 seq The size of the zone files sub-directories indicate the number of files existing for each type of zones. In this example, there is only one conventional zone file (all conventional zones are aggregated under a single file). # ls -l /mnt/cnv total 137101312 -rw-r----- 1 root root 140391743488 Nov 25 13:23 0 This aggregated conventional zone file can be used as a regular file:: # mkfs.ext4 /mnt/cnv/0 # mount -o loop /mnt/cnv/0 /data The \"seq\" sub-directory grouping files for sequential write zones has in this example 55356 zones:: # ls -lv /mnt/seq total 14511243264 -rw-r----- 1 root root 0 Nov 25 13:23 0 -rw-r----- 1 root root 0 Nov 25 13:23 1 -rw-r----- 1 root root 0 Nov 25 13:23 2 ... -rw-r----- 1 root root 0 Nov 25 13:23 55354 -rw-r----- 1 root root 0 Nov 25 13:23 55355 For sequential write zone files, the file size changes as data is appended at the end of the file, similarly to any regular file system:: # dd if=/dev/zero of=/mnt/seq/0 bs=4096 count=1 conv=notrunc oflag=direct 1+0 records in 1+0 records out 4096 bytes (4.1 kB, 4.0 KiB) copied, 0.00044121 s, 9.3 MB/s # ls -l /mnt/seq/0 -rw-r----- 1 root root 4096 Nov 25 13:23 /mnt/seq/0 The written file can be truncated to the zone size, preventing any further write operation:: # truncate -s 268435456 /mnt/seq/0 # ls -l /mnt/seq/0 -rw-r----- 1 root root 268435456 Nov 25 13:49 /mnt/seq/0 Truncation to 0 size allows freeing the file zone storage space and restart append-writes to the file:: # truncate -s 0 /mnt/seq/0 # ls -l /mnt/seq/0 -rw-r----- 1 root root 0 Nov 25 13:49 /mnt/seq/0 Since files are statically mapped to zones on the disk, the number of blocks of a file as reported by stat() and fstat() indicates the size of the file zone:: # stat /mnt/seq/0 File: /mnt/seq/0 Size: 0 Blocks: 524288 IO Block: 4096 regular empty file Device: 870h/2160d Inode: 50431 Links: 1 Access: (0640/-rw-r-----) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-11-25 13:23:57.048971997 +0900 Modify: 2019-11-25 13:52:25.553805765 +0900 Change: 2019-11-25 13:52:25.553805765 +0900 Birth: - The number of blocks of the file (\"Blocks\") in units of 512B blocks gives the maximum file size of 524288 * 512 B = 256 MB, corresponding to the device zone size in this example. Of note is that the \"IO block\" field always indicates the minimum I/O size for writes and corresponds to the device physical sector size. f2fs The Flash-Friendly File System ( f2fs ) was designed on a basis of a log-structured file system approach but modified to avoid the classical problems of the traditional log-structured approach (e.g. The snowball effect of wandering trees and the high cleaning overhead). f2fs supports various parameters not only for configuring on-disk layout but also for selecting allocation and cleaning algorithms. Zoned Block Device Support Zoned block device support was added to f2fs with kernel 4.10. Since f2fs uses a metadata block on-disk format with fixed block location, only zoned block devices which include conventional zones can be supported. Zoned devices composed entirely of sequential zones cannot be used with f2fs as a standalone device and require a multi-device setup to place metadata blocks on a randomly writable storage. f2fs supports multi-device setup where multiple block device address spaces are linearly concatenated to form a logically larger block device. The dm-linear device mapper target can also be used to create a logical device composed of conventional zones and sequential zones suitable for f2fs . f2fs zoned block device support was achieved using the following principles. Section Alignment In f2fs , a section is a group of fixed size segments (2 MB). The number of segments in a section is determined to match the zoned device zone size. For instance, with a 256 MB zone size, a section contains 128 segments of 2MB. Forced LFS mode By default, f2fs tries to optimize block allocation to avoid excessive append write by allowing some random writes within segments. The LFS mode forces sequential writes to segments and the sequential use of segments within sections, resulting in full compliance with zoned block devices write constraint. Zone reset as discard operation Block discard (or trim ) used to indicate to a device that a block or range of blocks are no longer in use is replaced with execution of a zone write pointer reset command when all blocks of all segments of a section are free, allowing the section to be reused. Limitations f2fs uses 32 bits block number with a block size of 4 KB. This results in a maximum volume size of 16 TB. Any device with a total capacity larger than 16 TB cannot be used with f2fs . To overcome this limit, the dm-linear device mapper target can be used to partition a zoned block device into serviceable smaller logical devices. This configuration must ensure that each logical device created is assigned a sufficient amount of conventional zones to store f2fs fixed location metadata blocks. Usage Example To format a zoned block device with mkfs.f2fs , the option -m must be specified. # mkfs.f2fs -m /dev/sdb f2fs-tools: mkfs.f2fs Ver: 1.12.0 (2018-11-12) Info: Disable heap-based policy Info: Debug level = 0 Info: Trim is enabled Info: [/dev/sdb] Disk Model: HGST HSH721415AL Info: Host-managed zoned block device: 55880 zones, 524 randomly writeable zones 65536 blocks per zone Info: Segments per section = 128 Info: Sections per zone = 1 Info: sector size = 4096 Info: total sectors = 3662151680 (14305280 MB) Info: zone aligned segment0 blkaddr: 65536 Info: format version with \"Linux version 5.0.16-300.fc30.x86_64 (mockbuild@bkernel03.phx2.fedoraproject.org) (gcc version 9.1.1 20190503 (Red Hat 9.1.1-1) (GCC)) #1 SMP Tue May 14 19:33:09 UTC 2019\" Info: [/dev/sdb] Discarding device Info: Discarded 14305280 MB Info: Overprovision ratio = 0.600% Info: Overprovision segments = 86254 (GC reserved = 43690) Info: format successful The formatted zoned block device can now be directly mounted without any other setup necessary. # mount /dev/sdb /mnt Compared to the dm-zoned device mapper target solution, performance of f2fs does not suffer from zone reclaim overhead as writes are always sequential and do not require on-disk temporary buffering. f2fs garbage collection (segment cleanup) will generate performance overhead only for workloads frequently deleting file or modifying files data. Btrfs Btrfs is a file system based on the copy-on-write (CoW) principle resulting in any block update to never be written in-place. Work is ongoing to add native ZBD support by changing the block allocation algorithm and block IO issuing code. Block Allocation Changes Btrfs block management relies on grouping of blocks into block groups , with each group composed of one or more device extent . The device extents of a block group may belong to different devices (e.g. In the case of a RAID volume). ZBD support changes the default device extent size to the size of the device zones so that all device extents are always aligned to a zone. Allocation of blocks within a block group is changed so that the allocation is always sequential from the beginning of the block group. To do so, an allocation pointer is added to block groups and used as the allocation hint. The changes also ensure that block freed below the allocation pointer are ignored, resulting in sequential block allocation within each group regardless of the block group usage. I/O Management While the introduction of the allocation pointer ensures that blocks are allocated sequentially within groups, so sequentially within zones, I/Os to write out newly allocated blocks may be issued out of order causing errors when writing to sequential zones. This problem is solved by introducing a write I/O request staging list to each block group. This list is used to delay the execution of unaligned write requests within a block group. The zones of a block group are reset to allow rewriting only when the block group is being freed, that is, when all the blocks within the block group are unused. For Btrfs volumes composed of multiple disks, restrictions are added to ensure that all disks have the same zone model and in the case of zoned block devices, the same zone size. This matches the existing Btrfs constraint that all device extents in a block group must have the same size. Upstream Contribution Btrfs zoned block device support is still in development and will be available in stable releases after the usual upstream review process completes. XFS XFS currently does not support zoned block devices. The dm-zoned device mapper target must be used to enable zoned device use with XFS . An early design document discussed the development work necessary to support host aware and host managed disks with XFS . Parts of this design have already been implemented and included into the kernel stable releases (e.g. Per inode reverse block mapping b-trees feature). However, more work is necessary to fully support zoned block devices. ext4 This article describes attempts at improving ext4 performance with host aware zoned block devices using changes to the file system journal management. The changes are small and succeed in maintaining good performance. However, support for host managed zoned block devices is not provided as some fundamental ext4 design aspects cannot be easily changed to match host managed device constraints. These optimizations for host aware zoned block devices is a research work and is not included in ext4 stable kernel releases. ext4 also does not support host managed disks. Similarly to XFS , the ext4 file system can however be used together with the dm-zoned device mapper target.","title":"File Systems"},{"location":"linux/fs/#file-systems-and-zoned-block-devices","text":"The dm-zoned device mapper target allows using any file system with host managed zoned block devices by hiding the device sequential write constraints. This is a simple solution to enable a file system use but not necessarily the most efficient due to the potentially high overhead of a block based zone reclaim process. Supporting zoned block devices directly in a file system implementation can lead to a more efficient zone reclaim processing as the file system metadata and file abstraction provide more information on the usage and validity status of storage blocks compared to the raw block device based approach. Furthermore, a file system design may lend itself well to the sequential write constraint of host managed zoned block devices. This is the case for log-structured file systems such as f2fs and copy-on-write (CoW) file systems such as Btrfs .","title":"File Systems and Zoned Block Devices"},{"location":"linux/fs/#zonefs","text":"zonefs is a very simple file system exposing each zone of a zoned block device as a file. zonefs is included with the upstream Linux kernel since version 5.6.0.","title":"zonefs"},{"location":"linux/fs/#overview","text":"Unlike a regular POSIX-compliant file system with native zoned block device support (e.g. f2fs ), zonefs does not hide the sequential write constraint of zoned block devices to the user. Files representing sequential write zones of the device must be written sequentially starting from the end of the file (append only writes). As such, zonefs is in essence closer to a raw block device access interface than to a full-featured POSIX file system. The goal of zonefs is to simplify the implementation of zoned block device support in applications by replacing raw block device file accesses with the richer regular file API, avoiding relying on direct block device file ioctls which may be more obscure to developers. One example of this approach is the implementation of LSM (log-structured merge) tree structures (such as used in RocksDB and LevelDB) on zoned block devices by allowing SSTables to be stored in a zone file similarly to a regular file system rather than as a range of sectors of the entire disk. The introduction of the higher level construct \"one file is one zone\" can help reducing the amount of changes needed in the application as well as introducing support for different application programming languages. The files representing zones are grouped by zone type, which are themselves represented by sub-directories. This file structure is built entirely using zone information provided by the device and so does not require any complex on-disk metadata structure.","title":"Overview"},{"location":"linux/fs/#on-disk-metadata","text":"zonefs on-disk metadata is composed only of an immutable super block which persistently stores a magic number and optional feature flags and values. On mount, zonefs uses the block layer API function blkdev_report_zones() to obtain the device zone configuration and populates the mount point with a static file tree solely based on this information. File sizes come from the device zone type and write pointer position managed by the device itself. The super block is always written on disk at sector 0. The first zone of the device storing the super block is never exposed as a zone file by zonefs . If the zone containing the super block is a sequential zone, the mkzonefs format tool always \"finishes\" the zone, that is, it transitions the zone to a full state to make it read-only, preventing any data write.","title":"On-Disk Metadata"},{"location":"linux/fs/#zone-type-sub-directories","text":"Files representing zones of the same type are grouped together under the same sub-directory automatically created on mount. For conventional zones, the sub-directory \"cnv\" is used. This directory is however created if and only if the device has usable conventional zones. If the device only has a single conventional zone at sector 0, the zone will not be exposed as a file as it will be used to store the zonefs super block. For such devices, the \"cnv\" sub-directory will not be created. For sequential write zones, the sub-directory \"seq\" is used. These two directories are the only directories that exist in zonefs . Users cannot create other directories and cannot rename nor delete the \"cnv\" and \"seq\" sub-directories. The size of the directories indicated by the st_size field of struct stat , obtained with the stat() or fstat() system calls, indicates the number of files existing under the directory.","title":"Zone Type Sub-Directories"},{"location":"linux/fs/#zone-files","text":"Zone files are named using the number of the zone they represent within the set of zones of a particular type. That is, both the \"cnv\" and \"seq\" directories contain files named \"0\", \"1\", \"2\", ... The file numbers also represent increasing zone start sector on the device. All read and write operations to zone files are not allowed beyond the file maximum size, that is, beyond the zone size. Any access exceeding the zone size is failed with the -EFBIG error. Creating, deleting, renaming or modifying any attribute of files is not allowed. The number of blocks of a file as reported by stat() and fstat() indicates the size of the file zone, or in other words, the maximum file size.","title":"Zone files"},{"location":"linux/fs/#conventional-zone-files","text":"The size of conventional zone files is fixed to the size of the zone they represent. Conventional zone files cannot be truncated. These files can be randomly read and written using any type of I/O operation: buffered I/Os, direct I/Os, memory mapped I/Os (mmap), etc. There are no I/O constraint for these files beyond the file size limit mentioned above.","title":"Conventional Zone Files"},{"location":"linux/fs/#sequential-zone-files","text":"The size of sequential zone files grouped in the \"seq\" sub-directory represents the file's zone write pointer position relative to the zone start sector. Sequential zone files can only be written sequentially, starting from the file end, that is, write operations can only be append writes. Zonefs makes no attempt at accepting random writes and will fail any write request that has a start offset not corresponding to the end of the file, or to the end of the last write issued and still in-flight (for asynchronous I/O operations). Since dirty page writeback by the page cache does not guarantee a sequential write pattern, zonefs prevents buffered writes and writeable shared mappings on sequential files. Only direct I/O writes are accepted for these files. zonefs relies on the sequential delivery of write I/O requests to the device implemented by the block layer elevator (See Write Command Ordering ). There are no restrictions on the type of I/O used for read operations in sequential zone files. Buffered I/Os, direct I/Os and shared read mappings are all accepted. Truncating sequential zone files is allowed only down to 0, in which case, the zone is reset to rewind the file zone write pointer position to the start of the zone, or up to the zone size, in which case the file's zone is transitioned to the FULL state (finish zone operation).","title":"Sequential zone files"},{"location":"linux/fs/#format-options","text":"Several optional features of zonefs can be enabled at format time. Conventional zone aggregation: ranges of contiguous conventional zones can be aggregated into a single larger file instead of the default one file per zone. File ownership: The owner UID and GID of zone files is by default 0 (root) but can be changed to any valid UID/GID. File access permissions: the default 640 access permissions can be changed.","title":"Format options"},{"location":"linux/fs/#io-error-handling","text":"Zoned block devices may fail I/O requests for reasons similar to regular block devices, e.g. due to bad sectors. However, in addition to such known I/O failure pattern, the standards governing zoned block devices behavior define additional conditions that can result in I/O errors. A zone may transition to the read-only condition: While the data already written in the zone is still readable, the zone can no longer be written. No user action on the zone (zone management command or read/write access) can change the zone condition back to a normal read/write state. While the reasons for the device to transition a zone to read-only state are not defined by the standards, a typical cause for such transition would be a defective write head on an HDD (all zones under this head are changed to read-only). A zone may transition to the offline condition: An offline zone cannot be read nor written. No user action can transition an offline zone back to an operational good state. Similarly to zone read-only transitions, the reasons for a drive to transition a zone to the offline condition are undefined. A typical cause would be a defective read-write head on an HDD causing all zones on the platter under the broken head to be inaccessible. Unaligned write errors: These errors result from the host issuing write requests with a start sector that does not correspond to a zone write pointer position when the write request is executed by the device. Even though zonefs enforces sequential file write for sequential zones, unaligned write errors may still happen in the case of a partial failure of a very large direct I/O operation split into multiple BIOs/requests or asynchronous I/O operations. If one of the write request within the set of sequential write requests issued to the device fails, all write requests queued after it will become unaligned and fail. Delayed write errors: Similarly to regular block devices, if the device side write cache is enabled, write errors may occur in ranges of previously completed writes when the device write cache is flushed, e.g. on fsync() . Similarly to the previous immediate unaligned write error case, delayed write errors can propagate through a stream of cached sequential data for a zone causing all data to be dropped after the sector that caused the error. All I/O errors detected by zonefs are notified to the user with an error code return for the system call that triggered or detected the error. The recovery actions taken by zonefs in response to I/O errors depend on the I/O type (read vs write) and on the reason for the error (bad sector, unaligned writes or zone condition change). For read I/O errors, zonefs does not execute any particular recovery action, but only if the file zone is still in a good condition and there is no inconsistency between the file inode size and its zone write pointer position. If a problem is detected, I/O error recovery is executed (see below table). For write I/O errors, zonefs I/O error recovery is always executed. A zone condition change to read-only or offline also always triggers zonefs I/O error recovery. zonefs minimal I/O error recovery may change a file size and file access permissions. File size changes: Immediate or delayed write errors in a sequential zone file may cause the file inode size to be inconsistent with the amount of data successfully written in the file zone. For instance, the partial failure of a multi-BIO large write operation will cause the zone write pointer to advance partially, even though the entire write operation will be reported as failed to the user. In such case, the file inode size must be advanced to reflect the zone write pointer change and eventually allow the user to restart writing at the end of the file. A file size may also be reduced to reflect a delayed write error detected on fsync(): in this case, the amount of data effectively written in the zone may be less than originally indicated by the file inode size. After such I/O error, zonefs always fixes the file inode size to reflect the amount of data persistently stored in the file zone. Access permission changes: A zone condition change to read-only is indicated with a change in the file access permissions to render the file read-only. This disables changes to the file attributes and data modification. For offline zones, all permissions (read and write) to the file are disabled. Further action taken by zonefs I/O error recovery can be controlled by the user with the \"errors=xxx\" mount option. The table below summarizes the result of zonefs I/O error processing depending on the mount option and on the zone conditions. \"errors=xxx\" mount option device zone condition file size file read file write device read device write remount-ro good fixed yes no yes yes remount-ro read-only as is yes no yes no remount-ro offline 0 no no no no zone-ro good fixed yes no yes yes zone-ro read-only as is yes no yes no zone-ro offline 0 no no no no zone-offline good 0 no no yes yes zone-offline read-only 0 no no yes no zone-offline offline 0 no no no no repair good fixed yes yes yes yes repair read-only as is yes no yes no repair offline 0 no no no no Further notes: The \"errors=remount-ro\" mount option is the default behavior of zonefs I/O error processing if no errors mount option is specified. With the \"errors=remount-ro\" mount option, the change of the file access permissions to read-only applies to all files. The file system is remounted read-only. Access permission and file size changes due to the device transitioning zones to the offline condition are permanent. Remounting or reformatting the device with mkfs.zonefs (mkzonefs) will not change back offline zone files to a good state. File access permission changes to read-only due to the device transitioning zones to the read-only condition are permanent. Remounting or reformatting the device will not re-enable file write access. File access permission changes implied by the remount-ro, zone-ro and zone-offline mount options are temporary for zones in a good condition. Unmounting and remounting the file system will restore the previous default (format time values) access rights to the files affected. The repair mount option triggers only the minimal set of I/O error recovery actions, that is, file size fixes for zones in a good condition. Zones indicated as being read-only or offline by the device still imply changes to the zone file access permissions as noted in the table above.","title":"IO error handling"},{"location":"linux/fs/#mount-options","text":"zonefs define the \"errors= \" mount option to allow the user to specify zonefs behavior in response to I/O errors, inode size inconsistencies or zone condition changes. The defined behaviors are as follow: remount-ro (default) zone-ro zone-offline repair The run-time I/O error actions defined for each behavior are detailed in the previous section. Mount time I/O errors will cause the mount operation to fail. The handling of read-only zones also differs between mount-time and run-time. If a read-only zone is found at mount time, the zone is always treated in the same manner as offline zones, that is, all accesses are disabled and the zone file size set to 0. This is necessary as the write pointer of read-only zones is defined as invalib by the ZBC and ZAC standards, making it impossible to discover the amount of data that has been written to the zone. In the case of a read-only zone discovered at run-time, as indicated in the previous section. the size of the zone file is left unchanged from its last updated value.","title":"Mount options"},{"location":"linux/fs/#zonefs-user-space-tools","text":"The mkzonefs tool is used to format zoned block devices for use with zonefs . This tool is available on GitHub . zonefs-tools also includes a test suite which can be run against any zoned block device, including nullblk block device created with zoned mode .","title":"Zonefs User Space Tools"},{"location":"linux/fs/#examples","text":"The following formats a 15TB host-managed SMR HDD with 256 MB zones with the conventional zones aggregation feature enabled:: # mkzonefs -o aggr_cnv /dev/sdX # mount -t zonefs /dev/sdX /mnt # ls -l /mnt/ total 0 dr-xr-xr-x 2 root root 1 Nov 25 13:23 cnv dr-xr-xr-x 2 root root 55356 Nov 25 13:23 seq The size of the zone files sub-directories indicate the number of files existing for each type of zones. In this example, there is only one conventional zone file (all conventional zones are aggregated under a single file). # ls -l /mnt/cnv total 137101312 -rw-r----- 1 root root 140391743488 Nov 25 13:23 0 This aggregated conventional zone file can be used as a regular file:: # mkfs.ext4 /mnt/cnv/0 # mount -o loop /mnt/cnv/0 /data The \"seq\" sub-directory grouping files for sequential write zones has in this example 55356 zones:: # ls -lv /mnt/seq total 14511243264 -rw-r----- 1 root root 0 Nov 25 13:23 0 -rw-r----- 1 root root 0 Nov 25 13:23 1 -rw-r----- 1 root root 0 Nov 25 13:23 2 ... -rw-r----- 1 root root 0 Nov 25 13:23 55354 -rw-r----- 1 root root 0 Nov 25 13:23 55355 For sequential write zone files, the file size changes as data is appended at the end of the file, similarly to any regular file system:: # dd if=/dev/zero of=/mnt/seq/0 bs=4096 count=1 conv=notrunc oflag=direct 1+0 records in 1+0 records out 4096 bytes (4.1 kB, 4.0 KiB) copied, 0.00044121 s, 9.3 MB/s # ls -l /mnt/seq/0 -rw-r----- 1 root root 4096 Nov 25 13:23 /mnt/seq/0 The written file can be truncated to the zone size, preventing any further write operation:: # truncate -s 268435456 /mnt/seq/0 # ls -l /mnt/seq/0 -rw-r----- 1 root root 268435456 Nov 25 13:49 /mnt/seq/0 Truncation to 0 size allows freeing the file zone storage space and restart append-writes to the file:: # truncate -s 0 /mnt/seq/0 # ls -l /mnt/seq/0 -rw-r----- 1 root root 0 Nov 25 13:49 /mnt/seq/0 Since files are statically mapped to zones on the disk, the number of blocks of a file as reported by stat() and fstat() indicates the size of the file zone:: # stat /mnt/seq/0 File: /mnt/seq/0 Size: 0 Blocks: 524288 IO Block: 4096 regular empty file Device: 870h/2160d Inode: 50431 Links: 1 Access: (0640/-rw-r-----) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-11-25 13:23:57.048971997 +0900 Modify: 2019-11-25 13:52:25.553805765 +0900 Change: 2019-11-25 13:52:25.553805765 +0900 Birth: - The number of blocks of the file (\"Blocks\") in units of 512B blocks gives the maximum file size of 524288 * 512 B = 256 MB, corresponding to the device zone size in this example. Of note is that the \"IO block\" field always indicates the minimum I/O size for writes and corresponds to the device physical sector size.","title":"Examples"},{"location":"linux/fs/#f2fs","text":"The Flash-Friendly File System ( f2fs ) was designed on a basis of a log-structured file system approach but modified to avoid the classical problems of the traditional log-structured approach (e.g. The snowball effect of wandering trees and the high cleaning overhead). f2fs supports various parameters not only for configuring on-disk layout but also for selecting allocation and cleaning algorithms.","title":"f2fs"},{"location":"linux/fs/#zoned-block-device-support","text":"Zoned block device support was added to f2fs with kernel 4.10. Since f2fs uses a metadata block on-disk format with fixed block location, only zoned block devices which include conventional zones can be supported. Zoned devices composed entirely of sequential zones cannot be used with f2fs as a standalone device and require a multi-device setup to place metadata blocks on a randomly writable storage. f2fs supports multi-device setup where multiple block device address spaces are linearly concatenated to form a logically larger block device. The dm-linear device mapper target can also be used to create a logical device composed of conventional zones and sequential zones suitable for f2fs . f2fs zoned block device support was achieved using the following principles. Section Alignment In f2fs , a section is a group of fixed size segments (2 MB). The number of segments in a section is determined to match the zoned device zone size. For instance, with a 256 MB zone size, a section contains 128 segments of 2MB. Forced LFS mode By default, f2fs tries to optimize block allocation to avoid excessive append write by allowing some random writes within segments. The LFS mode forces sequential writes to segments and the sequential use of segments within sections, resulting in full compliance with zoned block devices write constraint. Zone reset as discard operation Block discard (or trim ) used to indicate to a device that a block or range of blocks are no longer in use is replaced with execution of a zone write pointer reset command when all blocks of all segments of a section are free, allowing the section to be reused.","title":"Zoned Block Device Support"},{"location":"linux/fs/#limitations","text":"f2fs uses 32 bits block number with a block size of 4 KB. This results in a maximum volume size of 16 TB. Any device with a total capacity larger than 16 TB cannot be used with f2fs . To overcome this limit, the dm-linear device mapper target can be used to partition a zoned block device into serviceable smaller logical devices. This configuration must ensure that each logical device created is assigned a sufficient amount of conventional zones to store f2fs fixed location metadata blocks.","title":"Limitations"},{"location":"linux/fs/#usage-example","text":"To format a zoned block device with mkfs.f2fs , the option -m must be specified. # mkfs.f2fs -m /dev/sdb f2fs-tools: mkfs.f2fs Ver: 1.12.0 (2018-11-12) Info: Disable heap-based policy Info: Debug level = 0 Info: Trim is enabled Info: [/dev/sdb] Disk Model: HGST HSH721415AL Info: Host-managed zoned block device: 55880 zones, 524 randomly writeable zones 65536 blocks per zone Info: Segments per section = 128 Info: Sections per zone = 1 Info: sector size = 4096 Info: total sectors = 3662151680 (14305280 MB) Info: zone aligned segment0 blkaddr: 65536 Info: format version with \"Linux version 5.0.16-300.fc30.x86_64 (mockbuild@bkernel03.phx2.fedoraproject.org) (gcc version 9.1.1 20190503 (Red Hat 9.1.1-1) (GCC)) #1 SMP Tue May 14 19:33:09 UTC 2019\" Info: [/dev/sdb] Discarding device Info: Discarded 14305280 MB Info: Overprovision ratio = 0.600% Info: Overprovision segments = 86254 (GC reserved = 43690) Info: format successful The formatted zoned block device can now be directly mounted without any other setup necessary. # mount /dev/sdb /mnt Compared to the dm-zoned device mapper target solution, performance of f2fs does not suffer from zone reclaim overhead as writes are always sequential and do not require on-disk temporary buffering. f2fs garbage collection (segment cleanup) will generate performance overhead only for workloads frequently deleting file or modifying files data.","title":"Usage Example"},{"location":"linux/fs/#btrfs","text":"Btrfs is a file system based on the copy-on-write (CoW) principle resulting in any block update to never be written in-place. Work is ongoing to add native ZBD support by changing the block allocation algorithm and block IO issuing code.","title":"Btrfs"},{"location":"linux/fs/#block-allocation-changes","text":"Btrfs block management relies on grouping of blocks into block groups , with each group composed of one or more device extent . The device extents of a block group may belong to different devices (e.g. In the case of a RAID volume). ZBD support changes the default device extent size to the size of the device zones so that all device extents are always aligned to a zone. Allocation of blocks within a block group is changed so that the allocation is always sequential from the beginning of the block group. To do so, an allocation pointer is added to block groups and used as the allocation hint. The changes also ensure that block freed below the allocation pointer are ignored, resulting in sequential block allocation within each group regardless of the block group usage.","title":"Block Allocation Changes"},{"location":"linux/fs/#io-management","text":"While the introduction of the allocation pointer ensures that blocks are allocated sequentially within groups, so sequentially within zones, I/Os to write out newly allocated blocks may be issued out of order causing errors when writing to sequential zones. This problem is solved by introducing a write I/O request staging list to each block group. This list is used to delay the execution of unaligned write requests within a block group. The zones of a block group are reset to allow rewriting only when the block group is being freed, that is, when all the blocks within the block group are unused. For Btrfs volumes composed of multiple disks, restrictions are added to ensure that all disks have the same zone model and in the case of zoned block devices, the same zone size. This matches the existing Btrfs constraint that all device extents in a block group must have the same size.","title":"I/O Management"},{"location":"linux/fs/#upstream-contribution","text":"Btrfs zoned block device support is still in development and will be available in stable releases after the usual upstream review process completes.","title":"Upstream Contribution"},{"location":"linux/fs/#xfs","text":"XFS currently does not support zoned block devices. The dm-zoned device mapper target must be used to enable zoned device use with XFS . An early design document discussed the development work necessary to support host aware and host managed disks with XFS . Parts of this design have already been implemented and included into the kernel stable releases (e.g. Per inode reverse block mapping b-trees feature). However, more work is necessary to fully support zoned block devices.","title":"XFS"},{"location":"linux/fs/#ext4","text":"This article describes attempts at improving ext4 performance with host aware zoned block devices using changes to the file system journal management. The changes are small and succeed in maintaining good performance. However, support for host managed zoned block devices is not provided as some fundamental ext4 design aspects cannot be easily changed to match host managed device constraints. These optimizations for host aware zoned block devices is a research work and is not included in ext4 stable kernel releases. ext4 also does not support host managed disks. Similarly to XFS , the ext4 file system can however be used together with the dm-zoned device mapper target.","title":"ext4"},{"location":"linux/overview/","text":"Linux Zoned Storage Support Overview Zoned block device support was initially released with the Linux\u00ae kernel version 4.10. Following versions improved this support and added new features beyond the raw block device access interface. More advanced features such as device mapper support and ZBD aware file systems are now available. Overview Applications developers can use zoned block devices through various I/O paths controlled with different programming interfaces and exposing the device in different ways. A simplistic representation of the different possible access paths is shown in the figure below. Linux Zoned Block Device Support Overview Three different I/O path implement two POSIX compatible interfaces that completely hide the write constraints of zoned block devices sequential zones. These three I/O path are suitable to execute legacy applications, that is, applications that have not been modified to implement fully sequential write streams. File Access Interface This is the interface implemented by a file system allowing an application to organize its data in files and directories. Two different implementations are available. ZBD Compliant File System With this implementation, the file system is modified to directly handle the sequential write constraint of zoned block devices. Random writes to files by applications are transformed into sequential write streams by the file system, consealing the device constraints from the application. An example of this is the F2FS file system. Legacy File System In this case, an unmodified file system is used and the device sequential write constraint is handled by a device mapper target driver exposing the zoned block device as a regular block device. This device mapper is dm-zoned . Its characteristics and use are discussed in detail in this article . Raw Block Access Interface This is the raw block device file access interface that can be used by applications to directly access data stored on the device. Similarly to the legacy file system case, this interface is also implemented using the dm-zoned device mapper target driver to hide the sequential write constraints from the application. Three additional interfaces are available to applications that have been written or modified to comply with the sequential write constraint of zoned block devices. These interfaces directly expose the device constraints to applications which must ensure that data is written using sequential streams starting from zones write pointer positions. File Access Interface This special interface is implemented by the zonefs file system. zonefs is a very simple file system that exposes each zone of a zoned block device as a file. However, unlike regular POSIX file systems, the sequential write constraint of the device is not automatically handled by zonefs. It is the responsibility of the application to sequentially write files representing zones. Zoned Raw Block Access Interface This is the counterpart of the Raw Block Access Interface without any intermediate driver to handle the device constraints. An application can use this interface by directly opening the device file representing the zoned block device to gain access to zone information and management operations provided by the block layer. As an example, Linux System Utilities use this interface. Physical zoned block devices as well as logically created zoned block devices (e.g. zoned block devices created with the dm-linear device mapper target) support this interface. The libzbd user library and tools can simplify the implementation of applications using this interface. Passthrough Device Access Interface This is the interface provided by the SCSI generic driver (SG) and NVMe driver which allows an application to send SCSI or NVMe commands directly to a device. The kernel interfere minimally with the commands sent by applications, resulting in the need for application to handle all device constraints itself (e.g. Logical and physical sector size, zone boundaries, command timeout, command retry count, etc). User level libraries such as libzbc and libnvme can greatly simplify the implementation of applications using this interface. Kernel Versions The initial release of the zoned block device support with kernel 4.10 was limited to the block layer ZBD user interface, SCSI layer sequential write ordering control and native support for the F2FS file system. Following kernel versions added more feature such as device mapper drivers and support for the block multi-queue infrastructure. The figure below summarizes the evolution of zoned block device support with kernel versions. Kernel kernel versions and ZBD features Passthrough Access Support ( SG Access ) Support for exposing host managed ZBC/ZAC hard-disks as SCSI generic (SG) nodes was officially added to kernel 3.18 with the definition of the device type TYPE_SCSI for SCSI devices and with the definition of the device class ATA_DEV_ZAC for ATA devices. For kernels older than version 3.18, SATA host managed ZAC disks will not be exposed to the users as SG nodes nor as block device files. These older kernels will simply ignore SATA devices reporting a host managed ZAC device signature and the devices will not be usable in any way. For SCSI disks or SATA disks connected to a compatible SAS HBA, host managed disk will be accessible by the user through the node file created by the SG driver to represent these disks. Zoned Block Device Access and F2FS Support The block I/O layer zoned block device support added to kernel version 4.10 enables exposing host managed ZBC and ZAC disks as block device files, similarly to regular disks. This support also includes changes to the kernel libata command translation to enable SCSI ZBC zone block commands translation to ZAC zone ATA commands. For applications relying on SCSI generic direct access, this enables handling both ZBC (SCSI) and ZAC (ATA) disks with the same code (e.g. ATA commands do not need to be issued). Access to zoned block devices is also possible using the disk block device file (e.g. /dev/sdX device file) with regular POSIX system calls. However, compared to regular disks, some restrictions still apply (see Kernel ZBD Support Restrictions ). Device Mapper and dm-zoned Support With kernel version 4.13.0, support for zoned block devices was added to the device mapper infrastructure. This support allows using the dm-linear and dm-flakey device mapper targets on top of zoned block devices. Additionally, the dm-zoned device mapper target driver was also added. Block multi-queue and SCSI multi-queue Support With kernel version 4.16.0, support for the block multi-queue infrastructure was added. This improvement enables using host managed ZBC and ZAC disks with the SCSI multiqueue ( scsi-mq ) support enabled while retaining support for the legacy single queue block I/O path. The block multi queue and scsi-mq I/O path are the default since kernel version 5.0 with the removal of the legacy single queue block I/O path support. zonefs Kernel 5.6.0 first introduced the zonefs file system which exposes zones of a zoned block device as regular files. zonefs being implemented using the kernel internal zoned block device interface, all types of zoned block devices are supported (SCSI ZBC, ATA ZAC and NVMe ZNS). Zone Append Operation Support Kernel version 5.8.0 introduced a generic block layer interface for supporting zone append write operations . This release also modifies the SCSI layer to emulate these operations using regular write commands. With the introduction of NVMe ZNS support (see below), this emulation unifies the interface and capabilities of all zoned block devices, simplifying the implementation of other features such as file systems. NVM Express Zoned Namespaces With kernel version 5.9, support for the NVMe ZNS command set was added. This enables the nvme driver with the command set enhancements required to discover zoned namespaces, and registers these with the block layer as host managed zone block devices. This kernel release only support devices that do not implement any of the zone optional characteristics (ZOC), and also requires that the device implements the optional Zone Append command. Improvements to the kernel zoned block device support are still ongoing. Support for new file systems (e.g. btrfs ) will be released in the coming months. Recommended Kernel Versions All kernel versions since 4.10 include zoned block device support. However, as shown in the figure Kernel versions and features , some versions are recommended over others. Long Term Stable (LTS) Versions Kernel versions 4.14, 4.19 and 5.4 are long term kernel stable versions which will see bug fix back-ports from fixes in the mainline (development) kernel. These versions thus benefit from stability improvements developed for higher versions. Fixes to the zoned block device support infrastructure are also back-ported to these versions. Latest Stable Version While not necessarily marked as a long term stable version, the latest stable kernel version receives all bug fixes being developed with the mainline development kernel version following it. Except if the version is tagged as a long term support version, back-port of fixes to a stable kernel version stops with the switch of the following version from mainline to stable. Using a particular kernel stable version for a long time is thus not recommended. For any stable or long term stable kernel version, it is recommended that system administrators use the latest available release within that version to ensure that all known problem fixes are applied. ZBD Support Restrictions In order to minimize the amount of changes to the block layer code, various existing features were reused. Furthermore, other kernel components that are not compatible with zoned block devices behavior and are too complex to change were left unmodified. This approach led to a set of constraints that all zoned block devices must meet to be usable with Linux. Zone size While the ZBC, ZAC, and ZNS standards do not impose any constraint on the zone layout of a device, that is, zones can be of any size, the kernel ZBD support is restricted to zoned devices with all zones of equal size. The zone size must also be equal to a power of 2 number of logical blocks. Only the last zone of a device may optionally have a smaller size (a so called runt zone). This zone size restriction allows the kernel code to use the block layer \"chunked\" space management normally used for software RAID devices. The chunked space management uses power of two arithmetic (bit shift operations) to determine which chunk (i.e. which zone) is being accessed and ensures that block I/O operations do not cross zone boundaries. Unrestricted Reads The ZBC and ZAC standards define the URSWRZ bit indicating if a device will return an error when a read operation is directed at unwritten sectors of a sequential write required zone, that is, for a read command accessing sectors that are after the write pointer position of a zone. Linux only supports ZBC and ZAC host managed hard disks allowing unrestricted read commands, or in other words, SMR hard disks reporting the URSWRZ bit as not set. This restriction has been added to ensure that the block layer disk partition scanning process does not result in read commands failing whenever the disk partition table is checked. Direct IO Writes The kernel page cache does not guarantee that cached dirty pages will be flushed to a block device in sequential sector order. This can lead to unaligned write errors if an application uses buffered writes to write sequential write required zones of a device. To avoid this pitfall, applications directly using a zoned block device without a file system should always write to host managed disk sequential write required zones using direct I/O operations, that is, issue write() system calls with a block device file open using the O_DIRECT flag. Zone Append The ZNS specifications define the optional Zone Append command. This command may be used instead of regular write commands when the host needs to write data, but wants the device to tell it where the data was placed. This allows an efficient host implementation as it does not need to track the write pointer or order write commands in any special way. The Linux IO stack has been enabled to use this with kernel version 5.8, and the NVMe driver requires the device to support this optional command in order for the namespace to be usable through the kernel. All known ZBC and ZAC host-managed hard disks available on the market today have characteristics compatible with these requirements and can operate with a ZBD compatible Linux kernel.","title":"Support Overview"},{"location":"linux/overview/#linux-zoned-storage-support-overview","text":"Zoned block device support was initially released with the Linux\u00ae kernel version 4.10. Following versions improved this support and added new features beyond the raw block device access interface. More advanced features such as device mapper support and ZBD aware file systems are now available.","title":"Linux Zoned Storage Support Overview"},{"location":"linux/overview/#overview","text":"Applications developers can use zoned block devices through various I/O paths controlled with different programming interfaces and exposing the device in different ways. A simplistic representation of the different possible access paths is shown in the figure below. Linux Zoned Block Device Support Overview Three different I/O path implement two POSIX compatible interfaces that completely hide the write constraints of zoned block devices sequential zones. These three I/O path are suitable to execute legacy applications, that is, applications that have not been modified to implement fully sequential write streams. File Access Interface This is the interface implemented by a file system allowing an application to organize its data in files and directories. Two different implementations are available. ZBD Compliant File System With this implementation, the file system is modified to directly handle the sequential write constraint of zoned block devices. Random writes to files by applications are transformed into sequential write streams by the file system, consealing the device constraints from the application. An example of this is the F2FS file system. Legacy File System In this case, an unmodified file system is used and the device sequential write constraint is handled by a device mapper target driver exposing the zoned block device as a regular block device. This device mapper is dm-zoned . Its characteristics and use are discussed in detail in this article . Raw Block Access Interface This is the raw block device file access interface that can be used by applications to directly access data stored on the device. Similarly to the legacy file system case, this interface is also implemented using the dm-zoned device mapper target driver to hide the sequential write constraints from the application. Three additional interfaces are available to applications that have been written or modified to comply with the sequential write constraint of zoned block devices. These interfaces directly expose the device constraints to applications which must ensure that data is written using sequential streams starting from zones write pointer positions. File Access Interface This special interface is implemented by the zonefs file system. zonefs is a very simple file system that exposes each zone of a zoned block device as a file. However, unlike regular POSIX file systems, the sequential write constraint of the device is not automatically handled by zonefs. It is the responsibility of the application to sequentially write files representing zones. Zoned Raw Block Access Interface This is the counterpart of the Raw Block Access Interface without any intermediate driver to handle the device constraints. An application can use this interface by directly opening the device file representing the zoned block device to gain access to zone information and management operations provided by the block layer. As an example, Linux System Utilities use this interface. Physical zoned block devices as well as logically created zoned block devices (e.g. zoned block devices created with the dm-linear device mapper target) support this interface. The libzbd user library and tools can simplify the implementation of applications using this interface. Passthrough Device Access Interface This is the interface provided by the SCSI generic driver (SG) and NVMe driver which allows an application to send SCSI or NVMe commands directly to a device. The kernel interfere minimally with the commands sent by applications, resulting in the need for application to handle all device constraints itself (e.g. Logical and physical sector size, zone boundaries, command timeout, command retry count, etc). User level libraries such as libzbc and libnvme can greatly simplify the implementation of applications using this interface.","title":"Overview"},{"location":"linux/overview/#kernel-versions","text":"The initial release of the zoned block device support with kernel 4.10 was limited to the block layer ZBD user interface, SCSI layer sequential write ordering control and native support for the F2FS file system. Following kernel versions added more feature such as device mapper drivers and support for the block multi-queue infrastructure. The figure below summarizes the evolution of zoned block device support with kernel versions. Kernel kernel versions and ZBD features Passthrough Access Support ( SG Access ) Support for exposing host managed ZBC/ZAC hard-disks as SCSI generic (SG) nodes was officially added to kernel 3.18 with the definition of the device type TYPE_SCSI for SCSI devices and with the definition of the device class ATA_DEV_ZAC for ATA devices. For kernels older than version 3.18, SATA host managed ZAC disks will not be exposed to the users as SG nodes nor as block device files. These older kernels will simply ignore SATA devices reporting a host managed ZAC device signature and the devices will not be usable in any way. For SCSI disks or SATA disks connected to a compatible SAS HBA, host managed disk will be accessible by the user through the node file created by the SG driver to represent these disks. Zoned Block Device Access and F2FS Support The block I/O layer zoned block device support added to kernel version 4.10 enables exposing host managed ZBC and ZAC disks as block device files, similarly to regular disks. This support also includes changes to the kernel libata command translation to enable SCSI ZBC zone block commands translation to ZAC zone ATA commands. For applications relying on SCSI generic direct access, this enables handling both ZBC (SCSI) and ZAC (ATA) disks with the same code (e.g. ATA commands do not need to be issued). Access to zoned block devices is also possible using the disk block device file (e.g. /dev/sdX device file) with regular POSIX system calls. However, compared to regular disks, some restrictions still apply (see Kernel ZBD Support Restrictions ). Device Mapper and dm-zoned Support With kernel version 4.13.0, support for zoned block devices was added to the device mapper infrastructure. This support allows using the dm-linear and dm-flakey device mapper targets on top of zoned block devices. Additionally, the dm-zoned device mapper target driver was also added. Block multi-queue and SCSI multi-queue Support With kernel version 4.16.0, support for the block multi-queue infrastructure was added. This improvement enables using host managed ZBC and ZAC disks with the SCSI multiqueue ( scsi-mq ) support enabled while retaining support for the legacy single queue block I/O path. The block multi queue and scsi-mq I/O path are the default since kernel version 5.0 with the removal of the legacy single queue block I/O path support. zonefs Kernel 5.6.0 first introduced the zonefs file system which exposes zones of a zoned block device as regular files. zonefs being implemented using the kernel internal zoned block device interface, all types of zoned block devices are supported (SCSI ZBC, ATA ZAC and NVMe ZNS). Zone Append Operation Support Kernel version 5.8.0 introduced a generic block layer interface for supporting zone append write operations . This release also modifies the SCSI layer to emulate these operations using regular write commands. With the introduction of NVMe ZNS support (see below), this emulation unifies the interface and capabilities of all zoned block devices, simplifying the implementation of other features such as file systems. NVM Express Zoned Namespaces With kernel version 5.9, support for the NVMe ZNS command set was added. This enables the nvme driver with the command set enhancements required to discover zoned namespaces, and registers these with the block layer as host managed zone block devices. This kernel release only support devices that do not implement any of the zone optional characteristics (ZOC), and also requires that the device implements the optional Zone Append command. Improvements to the kernel zoned block device support are still ongoing. Support for new file systems (e.g. btrfs ) will be released in the coming months.","title":"Kernel Versions"},{"location":"linux/overview/#recommended-kernel-versions","text":"All kernel versions since 4.10 include zoned block device support. However, as shown in the figure Kernel versions and features , some versions are recommended over others. Long Term Stable (LTS) Versions Kernel versions 4.14, 4.19 and 5.4 are long term kernel stable versions which will see bug fix back-ports from fixes in the mainline (development) kernel. These versions thus benefit from stability improvements developed for higher versions. Fixes to the zoned block device support infrastructure are also back-ported to these versions. Latest Stable Version While not necessarily marked as a long term stable version, the latest stable kernel version receives all bug fixes being developed with the mainline development kernel version following it. Except if the version is tagged as a long term support version, back-port of fixes to a stable kernel version stops with the switch of the following version from mainline to stable. Using a particular kernel stable version for a long time is thus not recommended. For any stable or long term stable kernel version, it is recommended that system administrators use the latest available release within that version to ensure that all known problem fixes are applied.","title":"Recommended Kernel Versions"},{"location":"linux/overview/#zbd-support-restrictions","text":"In order to minimize the amount of changes to the block layer code, various existing features were reused. Furthermore, other kernel components that are not compatible with zoned block devices behavior and are too complex to change were left unmodified. This approach led to a set of constraints that all zoned block devices must meet to be usable with Linux. Zone size While the ZBC, ZAC, and ZNS standards do not impose any constraint on the zone layout of a device, that is, zones can be of any size, the kernel ZBD support is restricted to zoned devices with all zones of equal size. The zone size must also be equal to a power of 2 number of logical blocks. Only the last zone of a device may optionally have a smaller size (a so called runt zone). This zone size restriction allows the kernel code to use the block layer \"chunked\" space management normally used for software RAID devices. The chunked space management uses power of two arithmetic (bit shift operations) to determine which chunk (i.e. which zone) is being accessed and ensures that block I/O operations do not cross zone boundaries. Unrestricted Reads The ZBC and ZAC standards define the URSWRZ bit indicating if a device will return an error when a read operation is directed at unwritten sectors of a sequential write required zone, that is, for a read command accessing sectors that are after the write pointer position of a zone. Linux only supports ZBC and ZAC host managed hard disks allowing unrestricted read commands, or in other words, SMR hard disks reporting the URSWRZ bit as not set. This restriction has been added to ensure that the block layer disk partition scanning process does not result in read commands failing whenever the disk partition table is checked. Direct IO Writes The kernel page cache does not guarantee that cached dirty pages will be flushed to a block device in sequential sector order. This can lead to unaligned write errors if an application uses buffered writes to write sequential write required zones of a device. To avoid this pitfall, applications directly using a zoned block device without a file system should always write to host managed disk sequential write required zones using direct I/O operations, that is, issue write() system calls with a block device file open using the O_DIRECT flag. Zone Append The ZNS specifications define the optional Zone Append command. This command may be used instead of regular write commands when the host needs to write data, but wants the device to tell it where the data was placed. This allows an efficient host implementation as it does not need to track the write pointer or order write commands in any special way. The Linux IO stack has been enabled to use this with kernel version 5.8, and the NVMe driver requires the device to support this optional command in order for the namespace to be usable through the kernel. All known ZBC and ZAC host-managed hard disks available on the market today have characteristics compatible with these requirements and can operate with a ZBD compatible Linux kernel.","title":"ZBD Support Restrictions"},{"location":"linux/part/","text":"Zoned Block Device Partition Support Support for partition tables on zoned block devices depends on the kernel version being used. Kernel Versions 4.10 to 5.4 All kernels starting with version 4.10.0 (first kernel including zoned block device support) up to version 5.4 support partition tables on zoned block devices. For these kernels, The start sector and size of partitions of a zoned block device must be aligned to zone boundaries of the device. That is, a partition must start with the first sector of a zone and ends on the last sector of a zone. A zone report operation on a partition device (as opposed to a zone report for the entire container device) will report zone sector information (zone start sector and write pointer position) relative to the partition start sector. Similarly, a zone reset operation must specify a target zone range relative to the partition start sector. In effect, a zoned block device partition can be treated exactly like a regular disk, using the partition zoned block device with a start sector of 0. All zone and IO operations will be executed correctly taking into account the partition start sector and size. For host aware zoned block device models, creating partitions using a standard tools such as gparted to create a GUID partition table will work as expected. Since well-known partition tools do not have zoned block device support implemented, users must manually align the partitions to zone boundaries to satisfy the kernel constraint. This alignement will not be automatically done by the partitioning tool. However, the lack of zoned block device support for partition management tools can result in write IO errors for host managed zoned disk models. If the zones to be used to store the partition table data are sequential write required zones, then the partition tool must be able to correctly write align the partition table information on disk with the write pointer position of the zones that will be written. For instance, with the GUID partition table format, if the last zone of the disk is a sequential write required zone, the secondary GPT header and table entries will not necessarily be writable at the end of the disk LBA space, that is, at the end of the last zone of the disk. While manually performing this alignment is possible, such a procedure would be very difficult and unreliable. As a result, Using the dm-linear device mapper target to isolate smaller portions of a large host managed device is a better solution than partitions. Users should assume that partitions are not supported for host managed zoned block devices. Kernel Versions 5.5 and later Starting with the kernel version 5.5.0, partition support for host managed zoned block devices is ino longer provided. If a well formatted partition table is detected on a host managed zoned block device, the kernel will ignore it and will not create the block device files representing the partitions. For host aware zoned block devices, partitions are still supported. However, the kernel behavior differs from previosu versions. If a valid partition table is detected on a host aware zoned device, the device zone model is changed to none , completely disabling the use of the device and of its partitions as zoned block devices. In other words, a partitioned host aware device is always turned into a logical regular block device. Deleting the device partition table will enable again the use of the host aware device as a zoned block device.","title":"Zoned Block Device Partition Support"},{"location":"linux/part/#zoned-block-device-partition-support","text":"Support for partition tables on zoned block devices depends on the kernel version being used.","title":"Zoned Block Device Partition Support"},{"location":"linux/part/#kernel-versions-410-to-54","text":"All kernels starting with version 4.10.0 (first kernel including zoned block device support) up to version 5.4 support partition tables on zoned block devices. For these kernels, The start sector and size of partitions of a zoned block device must be aligned to zone boundaries of the device. That is, a partition must start with the first sector of a zone and ends on the last sector of a zone. A zone report operation on a partition device (as opposed to a zone report for the entire container device) will report zone sector information (zone start sector and write pointer position) relative to the partition start sector. Similarly, a zone reset operation must specify a target zone range relative to the partition start sector. In effect, a zoned block device partition can be treated exactly like a regular disk, using the partition zoned block device with a start sector of 0. All zone and IO operations will be executed correctly taking into account the partition start sector and size. For host aware zoned block device models, creating partitions using a standard tools such as gparted to create a GUID partition table will work as expected. Since well-known partition tools do not have zoned block device support implemented, users must manually align the partitions to zone boundaries to satisfy the kernel constraint. This alignement will not be automatically done by the partitioning tool. However, the lack of zoned block device support for partition management tools can result in write IO errors for host managed zoned disk models. If the zones to be used to store the partition table data are sequential write required zones, then the partition tool must be able to correctly write align the partition table information on disk with the write pointer position of the zones that will be written. For instance, with the GUID partition table format, if the last zone of the disk is a sequential write required zone, the secondary GPT header and table entries will not necessarily be writable at the end of the disk LBA space, that is, at the end of the last zone of the disk. While manually performing this alignment is possible, such a procedure would be very difficult and unreliable. As a result, Using the dm-linear device mapper target to isolate smaller portions of a large host managed device is a better solution than partitions. Users should assume that partitions are not supported for host managed zoned block devices.","title":"Kernel Versions 4.10 to 5.4"},{"location":"linux/part/#kernel-versions-55-and-later","text":"Starting with the kernel version 5.5.0, partition support for host managed zoned block devices is ino longer provided. If a well formatted partition table is detected on a host managed zoned block device, the kernel will ignore it and will not create the block device files representing the partitions. For host aware zoned block devices, partitions are still supported. However, the kernel behavior differs from previosu versions. If a valid partition table is detected on a host aware zoned device, the device zone model is changed to none , completely disabling the use of the device and of its partitions as zoned block devices. In other words, a partitioned host aware device is always turned into a logical regular block device. Deleting the device partition table will enable again the use of the host aware device as a zoned block device.","title":"Kernel Versions 5.5 and later"},{"location":"linux/sched/","text":"Write Ordering Control For Zoned Block Devices Historically, Linux\u00ae kernel block I/O stack (block layer and SCSI layer) has never provided guarantees regarding the exact execution order of block I/O requests. The main reasons for this are the asynchronous nature of block I/O request execution in the kernel and a fine granularity lock model for a device request queue to minimize lock contention overhead when multiple contexts are issuing I/O requests to a block device. A direct result of this design is the inability to give guarantees to a well behaving ZBD compliant application that write commands for a zone will be delivered in increasing LBA order matching the zone sequential write constraint. To address this problem, the kernel ZBD support adds zone write locking to ensure that write requests are processed in order per zone. Zone Write Locking As its name indicates, zone write locking implements a per-zone write lock to serialize the execution of write request targeting the same zone. Considering a target zone, this feature does not guarantee that write commands will always be issued at the zone write pointer location. This is the responsibility of the write I/O issuer. Zone write locking will only guarantee that the order in which write commands are issued by an application, file system or device mapper target will be respected by the block I/O stack. A well behaved user of zoned block devices will thus avoid unaligned write command failures. Zone write locking does not affect read commands in any way. Read requests are not serialized and processed in exactly the same manner as with regular block devices. Initial Implementation The initial implementation of zone write locking in kernel 4.10 was done in the SCSI disk driver, below the block layer, operating on requests already passed to the device dispatch queue by the block I/O scheduler. This early implementation was relying on the fact that the SCSI layer may delay issuing any request to the device. By maintaining a bitmap with one bit per zone, the SCSI disk driver marked a zone as locked whenever a write command was seen. In more details, the algorithm is as follows. If the next command to dispatch to the device is not a write command, the command is dispatched immediately. If the next command to dispatch is a write command, the zone write lock bit for the target zone of the command is inspected. If the target zone of the write command is not write locked (bit not set), then the zone is locked and the write command issued to the device. Both operations are atomically executed under the device request queue spinlock. If the target zone is already locked (bit set), the SCSI disk driver temporarily delays the command issuing to the device until the zone write lock is released. When a write command completes, the zone write lock for the target zone of the command is released and the dispatch process resumed. This means that if the command at the head of the dispatch queue is targeting the same zone, this command can now be issued (step 2.a). Note Zone write locking implemented as shown above also prevents unintended command reordering by the SAS HBAs or SATA adapters. The AHCI specifications are unclear regarding the definition of command issuing order to a device and as a result, many chipsets suffer from an inability to guarantee the proper command ordering. While providing write ordering guarantees for the legacy single queue block I/O path without any dependence on the HBA being used, this implementation suffered from several limitations. Potential performance degradation Any write command to any zone result in the command dispatch processing to be stalled, preventing the dispatching of all other commands, including read commands. This can limit performance benefits that can be obtained with device level command reordering when operating the device at high queue depth. The extreme case is an application issuing a write stream to a zone with asynchronous I/O system calls (e.g. io_submit() ). In this case, the sequential write commands would be queued in sequence in the device dispatch queue, resulting in the drive being operated at a queue depth of one, one write command at a time. No support for the block multi-queue I/O path Unlike the legacy single queue block I/O interface, the multi-queue block I/O implementation does not heavily rely on the device queue spin-lock for processing block I/O requests issued by the disk users (applications or kernel components). This results in potential block I/O request reordering happening before requests are passed on to the device dispatch queue and the ineffectiveness of zone write locking. These limitations led to a new implementation of zone write locking at a higher level in the I/O stack, using the block layer I/O schedulers. Improved Implementation: Block I/O Scheduler By moving zone write locking implementation higher up in the I/O stack, the block multi-queue (and SCSI multi-queue) infrastructure can also be supported. This improvement was added with kernel version 4.16 and the SCSI layer implementation of zone write locking removed. This new implementation of zone write locking relies on the block layer I/O deadline and mq-deadline I/O scheduler and addresses also the performance limitations of the previous implementation. In details, the new algorithm is as follows. Note The deadline and mq-deadline schedulers operate by grouping commands per type (reads vs writes) and always process these two groups of commands separately, e.g. first issuing many reads, then many writes. This allows for an increased performance by benefiting from hardware features such as device level read-ahead. If the scheduler is processing read commands... ...the first command queued in the list of read commands is allowed to proceed and submitted to the device dispatch queue. When no read commands are available, activate write processing (step 2). If read command processing time limit is reached, write command processing (step 2) is activated to avoid write command starvation. If read commands are still available, restart at step 1. When processing write commands, the list of write commands queued in the scheduler is scanned in order, starting with the command at the head of the LBA ordered list, or the first command in the arrival time ordered list (when there is a risk of starving commands). If the target zone of the first write command is not write locked (zone bitmap bit not set), then the zone is locked and the write command issued to the device. Both operations are atomically executed under a spinlock maintained by the scheduler. If the target zone is already locked (bit set), the command is skipped and the first write command targetting a different zone is searched for in the queue write command. If a command is found, step 2 is executed again. If all queued write commands are targeting locked zones, the scheduler operation mode (batch mode) is switched to read and step 1 called. When a write command completes, the zone write lock for the target zone of the command is released and the scheduler is activated, resuming operation at step 1 or 2 depending on the current batch mode. From this algorithm, it is clear that the device can now be operated at higher queue depth and that only sequential writes targeting the same zone will be throttled. All read commands can proceed, always, and write commands targeting different zones are not impacting each other. Note This new implementation does not guarantee overall command ordering. Guarantees are given only for write commands targetting the same zone. The dispatch order of write commands targetting different zones may be changed by the scheduler. For any single sequential zone, at any time, there is always at most a single write command in-flight being executed. Overall disk operation at high queue depth is possible in the presence of read accesses and if multiple zones are being written simultaneously. Block I/O Scheduler Configuration The deadline and mq-deadline schedulers need to be enabled in the kernel compilation configuration. Refer to the Write Ordering Control section for details. Note The legacy single queue block I/O path was removed from the kernel with version 5.0. Starting with the version, the deadline scheduler cannot be enabled. The mq-deadline scheduler is the only ZBD compliant scheduler. Manual Configuration A system may define a default I/O scheduler other than deadline or mq-deadline . The block I/O scheduler for a zoned block device can be checked with the following command. # cat /sys/block/sdb/queue/scheduler [none] mq-deadline kyber bfq If the block I/O scheduler selected is not deadline nor mq-deadline as in the example above, the scheduler can be changed with the following command. # echo mq-deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none deadline is also an alias name for the mq-deadline scheduler. The following command can thus be used with equal results in environements using the legacy single queue I/O path (kernels 4.16 to 4.20) as well as using the block multi-queue infrastructure (sole possibility starting with kernel version 5.0. # echo deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none Automatic Persistent Configuration Automatically configuring the deadline scheduler at system boot time can also be done using a udev rule. The procedure to follow to define a new udev rule slightly varies between distributions. Refer to your distribution documentation for details. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}==\"host-managed\", ATTR{queue/scheduler}=\"deadline\" This rule will setup the deadline scheduler for any host-managed zoned block device found in the system. Host aware zoned block disk can accept random writes and thus tolerate occasional write reordering within a zone sequential write stream. Nevertheless, write ordering can be maintained for these disks too by using the deadline scheduler. The above udev rule modified will automatically enable this. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}==\"host-aware\", ATTR{queue/scheduler}=\"deadline\" The following single rule will enable the deadline scheduler for any zoned block device, regardless of the device zone model. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}!=\"none\", ATTR{queue/scheduler}=\"deadline\"","title":"Write Command Ordering"},{"location":"linux/sched/#write-ordering-control-for-zoned-block-devices","text":"Historically, Linux\u00ae kernel block I/O stack (block layer and SCSI layer) has never provided guarantees regarding the exact execution order of block I/O requests. The main reasons for this are the asynchronous nature of block I/O request execution in the kernel and a fine granularity lock model for a device request queue to minimize lock contention overhead when multiple contexts are issuing I/O requests to a block device. A direct result of this design is the inability to give guarantees to a well behaving ZBD compliant application that write commands for a zone will be delivered in increasing LBA order matching the zone sequential write constraint. To address this problem, the kernel ZBD support adds zone write locking to ensure that write requests are processed in order per zone.","title":"Write Ordering Control For Zoned Block Devices"},{"location":"linux/sched/#zone-write-locking","text":"As its name indicates, zone write locking implements a per-zone write lock to serialize the execution of write request targeting the same zone. Considering a target zone, this feature does not guarantee that write commands will always be issued at the zone write pointer location. This is the responsibility of the write I/O issuer. Zone write locking will only guarantee that the order in which write commands are issued by an application, file system or device mapper target will be respected by the block I/O stack. A well behaved user of zoned block devices will thus avoid unaligned write command failures. Zone write locking does not affect read commands in any way. Read requests are not serialized and processed in exactly the same manner as with regular block devices.","title":"Zone Write Locking"},{"location":"linux/sched/#initial-implementation","text":"The initial implementation of zone write locking in kernel 4.10 was done in the SCSI disk driver, below the block layer, operating on requests already passed to the device dispatch queue by the block I/O scheduler. This early implementation was relying on the fact that the SCSI layer may delay issuing any request to the device. By maintaining a bitmap with one bit per zone, the SCSI disk driver marked a zone as locked whenever a write command was seen. In more details, the algorithm is as follows. If the next command to dispatch to the device is not a write command, the command is dispatched immediately. If the next command to dispatch is a write command, the zone write lock bit for the target zone of the command is inspected. If the target zone of the write command is not write locked (bit not set), then the zone is locked and the write command issued to the device. Both operations are atomically executed under the device request queue spinlock. If the target zone is already locked (bit set), the SCSI disk driver temporarily delays the command issuing to the device until the zone write lock is released. When a write command completes, the zone write lock for the target zone of the command is released and the dispatch process resumed. This means that if the command at the head of the dispatch queue is targeting the same zone, this command can now be issued (step 2.a). Note Zone write locking implemented as shown above also prevents unintended command reordering by the SAS HBAs or SATA adapters. The AHCI specifications are unclear regarding the definition of command issuing order to a device and as a result, many chipsets suffer from an inability to guarantee the proper command ordering. While providing write ordering guarantees for the legacy single queue block I/O path without any dependence on the HBA being used, this implementation suffered from several limitations. Potential performance degradation Any write command to any zone result in the command dispatch processing to be stalled, preventing the dispatching of all other commands, including read commands. This can limit performance benefits that can be obtained with device level command reordering when operating the device at high queue depth. The extreme case is an application issuing a write stream to a zone with asynchronous I/O system calls (e.g. io_submit() ). In this case, the sequential write commands would be queued in sequence in the device dispatch queue, resulting in the drive being operated at a queue depth of one, one write command at a time. No support for the block multi-queue I/O path Unlike the legacy single queue block I/O interface, the multi-queue block I/O implementation does not heavily rely on the device queue spin-lock for processing block I/O requests issued by the disk users (applications or kernel components). This results in potential block I/O request reordering happening before requests are passed on to the device dispatch queue and the ineffectiveness of zone write locking. These limitations led to a new implementation of zone write locking at a higher level in the I/O stack, using the block layer I/O schedulers.","title":"Initial Implementation"},{"location":"linux/sched/#improved-implementation-block-io-scheduler","text":"By moving zone write locking implementation higher up in the I/O stack, the block multi-queue (and SCSI multi-queue) infrastructure can also be supported. This improvement was added with kernel version 4.16 and the SCSI layer implementation of zone write locking removed. This new implementation of zone write locking relies on the block layer I/O deadline and mq-deadline I/O scheduler and addresses also the performance limitations of the previous implementation. In details, the new algorithm is as follows. Note The deadline and mq-deadline schedulers operate by grouping commands per type (reads vs writes) and always process these two groups of commands separately, e.g. first issuing many reads, then many writes. This allows for an increased performance by benefiting from hardware features such as device level read-ahead. If the scheduler is processing read commands... ...the first command queued in the list of read commands is allowed to proceed and submitted to the device dispatch queue. When no read commands are available, activate write processing (step 2). If read command processing time limit is reached, write command processing (step 2) is activated to avoid write command starvation. If read commands are still available, restart at step 1. When processing write commands, the list of write commands queued in the scheduler is scanned in order, starting with the command at the head of the LBA ordered list, or the first command in the arrival time ordered list (when there is a risk of starving commands). If the target zone of the first write command is not write locked (zone bitmap bit not set), then the zone is locked and the write command issued to the device. Both operations are atomically executed under a spinlock maintained by the scheduler. If the target zone is already locked (bit set), the command is skipped and the first write command targetting a different zone is searched for in the queue write command. If a command is found, step 2 is executed again. If all queued write commands are targeting locked zones, the scheduler operation mode (batch mode) is switched to read and step 1 called. When a write command completes, the zone write lock for the target zone of the command is released and the scheduler is activated, resuming operation at step 1 or 2 depending on the current batch mode. From this algorithm, it is clear that the device can now be operated at higher queue depth and that only sequential writes targeting the same zone will be throttled. All read commands can proceed, always, and write commands targeting different zones are not impacting each other. Note This new implementation does not guarantee overall command ordering. Guarantees are given only for write commands targetting the same zone. The dispatch order of write commands targetting different zones may be changed by the scheduler. For any single sequential zone, at any time, there is always at most a single write command in-flight being executed. Overall disk operation at high queue depth is possible in the presence of read accesses and if multiple zones are being written simultaneously.","title":"Improved Implementation: Block I/O Scheduler"},{"location":"linux/sched/#block-io-scheduler-configuration","text":"The deadline and mq-deadline schedulers need to be enabled in the kernel compilation configuration. Refer to the Write Ordering Control section for details. Note The legacy single queue block I/O path was removed from the kernel with version 5.0. Starting with the version, the deadline scheduler cannot be enabled. The mq-deadline scheduler is the only ZBD compliant scheduler.","title":"Block I/O Scheduler Configuration"},{"location":"linux/sched/#manual-configuration","text":"A system may define a default I/O scheduler other than deadline or mq-deadline . The block I/O scheduler for a zoned block device can be checked with the following command. # cat /sys/block/sdb/queue/scheduler [none] mq-deadline kyber bfq If the block I/O scheduler selected is not deadline nor mq-deadline as in the example above, the scheduler can be changed with the following command. # echo mq-deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none deadline is also an alias name for the mq-deadline scheduler. The following command can thus be used with equal results in environements using the legacy single queue I/O path (kernels 4.16 to 4.20) as well as using the block multi-queue infrastructure (sole possibility starting with kernel version 5.0. # echo deadline > /sys/block/sdb/queue/scheduler # cat sys/block/sdb/queue/scheduler [mq-deadline] kyber bfq none","title":"Manual Configuration"},{"location":"linux/sched/#automatic-persistent-configuration","text":"Automatically configuring the deadline scheduler at system boot time can also be done using a udev rule. The procedure to follow to define a new udev rule slightly varies between distributions. Refer to your distribution documentation for details. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}==\"host-managed\", ATTR{queue/scheduler}=\"deadline\" This rule will setup the deadline scheduler for any host-managed zoned block device found in the system. Host aware zoned block disk can accept random writes and thus tolerate occasional write reordering within a zone sequential write stream. Nevertheless, write ordering can be maintained for these disks too by using the deadline scheduler. The above udev rule modified will automatically enable this. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}==\"host-aware\", ATTR{queue/scheduler}=\"deadline\" The following single rule will enable the deadline scheduler for any zoned block device, regardless of the device zone model. ACTION==\"add|change\", KERNEL==\"sd*[!0-9]\", ATTRS{queue/zoned}!=\"none\", ATTR{queue/scheduler}=\"deadline\"","title":"Automatic Persistent Configuration"},{"location":"linux/zbd-api/","text":"Zoned Block Device User Interface User applications can access zone information and manage zones of a zoned block device using two types of interfaces. sysfs attribute files accessible either directly from applications as regular files or form scripted languages (shell scripts, python, etc). ioctl() system calls suitable for use from C programs or other programming languages with an equivalent system call binding. The sysfs files and ioctl() commands available to applications have evolved since the introduction of zoned block device support in Kernel 4.10. The availability of files and commands per kernel version is detailed in the following sections. Sysfs Interface Programs using script languages (e.g. bash scripts) can access a zoned device information through sysfs attribute files. The attribute files provided are shown in the following table. File Kernel version Description /sys/block/ dev name /queue/zoned 4.10.0 Device zoned model /sys/block/ dev name /queue/chunk_sectors 4.10.0 Device zone size /sys/block/ dev name /queue/nr_zones 4.20.0 Total number of zones /sys/block/ dev name /queue/zone_append_max_bytes 5.8.0 Maximum size in bytes of a zone append write operation /sys/block/ dev name /queue/max_open_zones 5.9.0 Maximum number of open zones /sys/block/ dev name /queue/max_active_zones 5.9.0 Maximum number of active zones Device Zoned Model The zone model of a zoned device can be discovered using the zoned device queue attribute file. For instance, for a zoned block device named sdb , the following shell command displays the device zoned model. # cat /sys/block/sdb/queue/zoned host-managed The possible values of the zoned attribute file are shown in the table below. Value Description none Regular block device, including drive managed SMR disks host-aware Host aware device model host-managed Host managed device model Device Zone Size The device zone size can be read from the sysfs queue attribute file named chunk_sectors . For the same device named sdb as in the previous example, the following command gives the device zone size. # cat /sys/block/sdb/queue/chunk_sectors 524288 The value is displayed as a number of 512B sectors, regardless of the actual logical and physical block size of the device. In this example, the device zone size is 524288 x 512 = 256 MiB . Number of Zones Starting with Linux kernel version 4.20.0, the sysfs queue attribute file nr_zones is available to obtain the total number of zones of a zoned device. # cat /sys/block/sdb/queue/nr_zones 55880 This attribute value is always 0 for a regular block device. ioctl() Application Programming Interface The C header file /usr/include/linux/blkzoned.h contains macro definitions and data structure definitions allowing an application developer to obtain information on a zoned block device and to manage the zones of the device. Zone Information Data Structures The data structure struct blk_zone defines a zone descriptor structure which completely describes a zone information: the zone location on the device, the zone type, its condition (state), and the position of the zone write pointer for sequential zones. Up to kernel version 5.8, this data structure is as shown below. /** * struct blk_zone - Zone descriptor for BLKREPORTZONE ioctl. * * @start: Zone start in 512 B sector units * @len: Zone length in 512 B sector units * @wp: Zone write pointer location in 512 B sector units * @type: see enum blk_zone_type for possible values * @cond: see enum blk_zone_cond for possible values * @non_seq: Flag indicating that the zone is using non-sequential resources * (for host-aware zoned block devices only). * @reset: Flag indicating that a zone reset is recommended. * @reserved: Padding to 64 B to match the ZBC/ZAC defined zone descriptor size. * * start, len, capacity and wp use the regular 512 B sector unit, regardless * of the device logical block size. The overall structure size is 64 B to * match the ZBC, ZAC and ZNS defined zone descriptor and allow support for * future additional zone information. */ struct blk_zone { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 reserved[36]; }; As indicated in the comments to this data structure definition, the unit that is used to indicate the zone start position, size and write pointer position is 512B sector size, regardless of the actual logical block size of the device. Even for a device with a 4KB physical sector, the above zone descriptor fields use a 512-byte sector size unit. The capacity field was added to struct blk_zone starting from kernel version 5.9. With this change, the data structure is as follows. /** * struct blk_zone - Zone descriptor for BLKREPORTZONE ioctl. * * @start: Zone start in 512 B sector units * @len: Zone length in 512 B sector units * @wp: Zone write pointer location in 512 B sector units * @type: see enum blk_zone_type for possible values * @cond: see enum blk_zone_cond for possible values * @non_seq: Flag indicating that the zone is using non-sequential resources * (for host-aware zoned block devices only). * @reset: Flag indicating that a zone reset is recommended. * @resv: Padding for 8B alignment. * @capacity: Zone usable capacity in 512 B sector units * @reserved: Padding to 64 B to match the ZBC, ZAC and ZNS defined zone * descriptor size. * * start, len, capacity and wp use the regular 512 B sector unit, regardless * of the device logical block size. The overall structure size is 64 B to * match the ZBC, ZAC and ZNS defined zone descriptor and allow support for * future additional zone information. */ struct blk_zone { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 resv[4]; __u64 capacity; /* Zone capacity in number of sectors */ __u8 reserved[24]; }; The capacity filed indicate the usable zone capacity of a zone in unit of 512B sectors. The presence, or validity, of this field within the structure is indicated using a zone report flag. See Obtaining Zone Information below for details. Zone Type The type field of a zone descriptor can only have one of the values defined by the enumeration enum blk_zone_type . /** * enum blk_zone_type - Types of zones allowed in a zoned device. * * @BLK_ZONE_TYPE_CONVENTIONAL: The zone has no write pointer and can be writen * randomly. Zone reset has no effect on the zone. * @BLK_ZONE_TYPE_SEQWRITE_REQ: The zone must be written sequentially * @BLK_ZONE_TYPE_SEQWRITE_PREF: The zone can be written non-sequentially * * Any other value not defined is reserved and must be considered as invalid. */ enum blk_zone_type { BLK_ZONE_TYPE_CONVENTIONAL = 0x1, BLK_ZONE_TYPE_SEQWRITE_REQ = 0x2, BLK_ZONE_TYPE_SEQWRITE_PREF = 0x3, }; Zone Condition The cond field of the struct blkzone data structure defines the current condition of a zone. The possible condition (state) values of this field are defined by the blk_zone_cond enumeration. /** * enum blk_zone_cond - Condition [state] of a zone in a zoned device. * * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional. * @BLK_ZONE_COND_EMPTY: The zone is empty. * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened. * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an * OPEN ZONE command. * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing. * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone * FINISH ZONE command. * @BLK_ZONE_COND_READONLY: The zone is read-only. * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written). * * The Zone Condition state machine in the ZBC/ZAC standards maps the above * deinitions as: * - ZC1: Empty | BLK_ZONE_EMPTY * - ZC2: Implicit Open | BLK_ZONE_COND_IMP_OPEN * - ZC3: Explicit Open | BLK_ZONE_COND_EXP_OPEN * - ZC4: Closed | BLK_ZONE_CLOSED * - ZC5: Full | BLK_ZONE_FULL * - ZC6: Read Only | BLK_ZONE_READONLY * - ZC7: Offline | BLK_ZONE_OFFLINE * * Conditions 0x5 to 0xC are reserved by the current ZBC/ZAC spec and should * be considered invalid. */ enum blk_zone_cond { BLK_ZONE_COND_NOT_WP = 0x0, BLK_ZONE_COND_EMPTY = 0x1, BLK_ZONE_COND_IMP_OPEN = 0x2, BLK_ZONE_COND_EXP_OPEN = 0x3, BLK_ZONE_COND_CLOSED = 0x4, BLK_ZONE_COND_READONLY = 0xD, BLK_ZONE_COND_FULL = 0xE, BLK_ZONE_COND_OFFLINE = 0xF, }; Under a device normal operation, some of the conditions defined cannot result directly from a host initiated operation. These conditions are BLK_ZONE_COND_OFFLINE and BLK_ZONE_COND_READONLY and can only be set by the device itself to indicate zones with capabilities limited by a hardware defect. The condition BLK_ZONE_COND_EXP_OPEN , or explicit open , is the result of a successful execution of an OPEN ZONE command (see Zone Block Commands . Since the OPEN ZONE command is not supported by the kernel ZBD interface, a zone can be transitioned in the explicit open zone condition only by using direct device access, that is, issuing the SCSI OPEN ZONE command through the SG_IO interface (using libzbc , libzbc zbc_open_zone utility or the sg_zone utility). ioctl() Commands Several ioctl() commands are defined to manipulate obtain information and manipulate zones of a zoned block device. All commands supported are shown below. /** * Zoned block device ioctl's: * * @BLKREPORTZONE: Get zone information. Takes a zone report as argument. * The zone report will start from the zone containing the * sector specified in the report request structure. * @BLKRESETZONE: Reset the write pointer of the zones in the specified * sector range. The sector range must be zone aligned. * @BLKGETZONESZ: Get the device zone size in number of 512 B sectors. * @BLKGETNRZONES: Get the total number of zones of the device. * @BLKOPENZONE: Open the zones in the specified sector range. * The 512 B sector range must be zone aligned. * @BLKCLOSEZONE: Close the zones in the specified sector range. * The 512 B sector range must be zone aligned. * @BLKFINISHZONE: Mark the zones as full in the specified sector range. * The 512 B sector range must be zone aligned. */ #define BLKREPORTZONE _IOWR(0x12, 130, struct blk_zone_report) #define BLKRESETZONE _IOW(0x12, 131, struct blk_zone_range) #define BLKGETZONESZ _IOR(0x12, 132, __u32) #define BLKGETNRZONES _IOR(0x12, 133, __u32) #define BLKOPENZONE _IOW(0x12, 134, struct blk_zone_range) #define BLKCLOSEZONE _IOW(0x12, 135, struct blk_zone_range) #define BLKFINISHZONE _IOW(0x12, 136, struct blk_zone_range) Not all commands are available on all kernel versions. The following table shows the kernel version that introduced each command. Command Kernel version Description BLKREPORTZONE 4.10.0 Get zone information BLKRESETZONE 4.10.0 Reset a zone write pointer BLKGETZONESZ 4.20.0 Get a device zone size BLKGETNRZONES 4.20.0 Get the total number of zones of a device BLKOPENZONE 5.5.0 Explicitly open a zone BLKCLOSEZONE 5.5.0 Close a zone BLKFINISHZONE 5.5.0 Finish a zone Obtaining Zone Information The BLKREPORTZONE command allows an application to obtain a device zone information in the form of an array of zone descriptors. The data argument passed to the ioctl() must be the address of a memory area large enough to store one struct blk_zone_report header structure followed by an array of zone descriptors. The zone report header structure blk_zone_report is as shown below. /** * struct blk_zone_report - BLKREPORTZONE ioctl request/reply * * @sector: starting sector of report * @nr_zones: IN maximum / OUT actual * @reserved: padding to 16 byte alignment * @zones: Space to hold @nr_zones @zones entries on reply. * * The array of at most @nr_zones must follow this structure in memory. */ struct blk_zone_report { __u64 sector; __u32 nr_zones; __u8 reserved[4]; struct blk_zone zones[0]; }; The header indicates the 512-byte sector from which the report should start and the number of zone descriptors in the array following the header. A typical use of the BLKREPORTZONE command to obtain information on all the zones of a device is as shown below. #include <stdlib.h> #include <linux/blkzoned.h> unsigned long long start_sector = 0; struct blk_zone_report *hdr; size_t hdr_len; int nr_zones = 256; hdr_len = sizeof(struct blk_zone_report) + nr_zones * sizeof(struct blkzone); hdr = malloc(hdr_len); if (!hdr) return -1; while (1) { hdr->sector = start_sector; hdr->nr_zones = nr_zones; ret = ioctl(fd, BLKREPORTZONE, hdr); if (ret) goto error; if (!hdr->nr_zones) { /* Done */ break; } printf(\"Got %u zone descriptors\\n\", hdr->nr_zones); ... /* The next report must start after the last zone reported */ start_sector = hdr->zones[hdr->nr_zones - 1].start + hdr->zones[hdr->nr_zones - 1].len; } The number of zone descriptors obtained is returned to the user using the nr_zones field of the report header structure blk_zone_report . With the introduction of zone capacity support for NVMe Zoned Namepsaces in kernel version 5.9, zone descriptors gained the capacity field. The presence of this field is indicated using the new flag field added to struct blk_zone_report /** * enum blk_zone_report_flags - Feature flags of reported zone descriptors. * * @BLK_ZONE_REP_CAPACITY: Zone descriptor has capacity field. */ enum blk_zone_report_flags { BLK_ZONE_REP_CAPACITY = (1 << 0), }; /** * struct blk_zone_report - BLKREPORTZONE ioctl request/reply * * @sector: starting sector of report * @nr_zones: IN maximum / OUT actual * @flags: one or more flags as defined by enum blk_zone_report_flags. * @zones: Space to hold @nr_zones @zones entries on reply. * * The array of at most @nr_zones must follow this structure in memory. */ struct blk_zone_report { __u64 sector; __u32 nr_zones; __u32 flags; struct blk_zone zones[0]; }; If the flags field of struct blk_zone_report has the flag BLK_ZONE_REP_CAPACITY set, then zone descriptors structure will have a valid value set in the capacity field of sturct blk_zone . Otherwise, this field can be ignored as it will always indicate a value of 0. The example code below, extracted from the code of the libzbd library, illustrates how application can implement backward compatible support for zone capacity information using the autotools build environment. # less configure.ac ... AC_CHECK_HEADER(linux/blkzoned.h, [], [AC_MSG_ERROR([Couldn't find linux/blkzoned.h. Kernel too old ?])], [[#include <linux/blkzoned.h>]]) AC_CHECK_MEMBER([struct blk_zone.capacity], [AC_DEFINE(HAVE_BLK_ZONE_REP_V2, [1], [report zones includes zone capacity])], [], [[#include <linux/blkzoned.h>]]) ... # less lib/zbd.h ... /* * Handle kernel zone capacity support */ #ifndef HAVE_BLK_ZONE_REP_V2 #define BLK_ZONE_REP_CAPACITY (1 << 0) struct blk_zone_v2 { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 resv[4]; __u64 capacity; /* Zone capacity in number of sectors */ __u8 reserved[24]; }; #define blk_zone blk_zone_v2 struct blk_zone_report_v2 { __u64 sector; __u32 nr_zones; __u32 flags; struct blk_zone zones[0]; }; #define blk_zone_report blk_zone_report_v2 #endif /* HAVE_BLK_ZONE_REP_V2 */ ... With this method, the main code responsible for issuing and parsing zone reports always has access to the capacity field of struct blk_zone , regardless of the kernel version the code is executed on. For kernels preceding kernel version 5.9, the zone capacity field will always be equal to 0, meaning that the zone capacity should be ignored and the zone size used in place. Different coding techniques can also be used to always return a zone capacity equal to the zone size for kernels lacking support for this field. The command line utility blkzone , part of the util-linux project, uses the BLKREPORTZONE command to implement its report function. Its code was modified similarly to the above method to allow its correct compilation and execution regardless of the version of the kernel being used. Resetting a Zone Write Pointer The write pointer of a single sequential zone or of a range of contiguous sequential zones can be reset using the BLKRESETZONE command. Resetting a sequential zone write pointer position will also transition the zone to the Empty condition ( BLK_ZONE_COND_EMPTY ). The range of zones to reset is defined using the data structure blk_zone_range shown below. /** * struct blk_zone_range - BLKRESETZONE/BLKOPENZONE/ * BLKCLOSEZONE/BLKFINISHZONE ioctl * requests * @sector: Starting sector of the first zone to operate on. * @nr_sectors: Total number of sectors of all zones to operate on. */ struct blk_zone_range { __u64 sector; __u64 nr_sectors; }; The sector field must specify the start sector of the first zone to reset. The nr_sectors field specifies the total length of the range of zones to reset. This length must be at least as large as one zone. As indicated in comments describing the blk_zone_range structure, the commands BLKOPENZONE , BLKCLOSEZONE and BLKFINISHZONE also use this data structure to define the range of zones on which the command will operate. The following code shows an example use of the BLKRESETZONE command to reset a single zone starting at sector 274726912 with a zone size of 256 MiB (524288 sectors of 512B). #include <linux/blkzoned.h> struct blk_zone_range zrange; int ret; zrange.sector = 274726912; zrange.nr_sectors = 524288; ret = ioctl(fd, BLKRESETZONE, &zrange); if (ret) goto error; ... The device file descriptor fd must be open for writing for this command to succeed. The command line utility blkzone uses the BLKRESETZONE command to implement its reset functionality. Opening, Closing and Finishing Zones Explicitely opening a zone or a range of zones can be done using the BLKOPENZONE command. This command uses the same arguments as the BLKRESETZONE command. It takes a pointer to a data structure blk_zone_range which must specifies the range of zones to operate on. Closing a zone is done using the command BLKCLOSEZONE . Finishing a zone, that is transitioning the zone to the full condition ( BLK_ZONE_COND_FULL ), is done using the BLKFINISHZONE command. Both of these commands also take as an argument a pointer to a blk_zone_range data structure to specify the range of zones to operate on. The BLKOPENZONE , BLKCLOSEZONE and BLKFINISHZONE commands were introduced with kernel version 5.5.0. Zone Size and Number of Zones Linux\u00ae kernel version 4.20 introduced two new additional commands to obtain a zoned device zone size ( BLKGETZONESZ ) and the total number of zones of the device ( BLKGETNRZONES ). Both commands take a pointer to an unsigned 32-bits integer variable as argument where the zone size value or the number of zones will be returned. The following sample C code illustrates the use of these commands. #include <linux/blkzoned.h> #include <stdio.h> unsigned int nr_zones, zone_size; int ret; ret = ioctl(fd, ,BLKGETZONESZ, &zone_size); if (ret) goto error; ret = ioctl(fd, ,BLKGETNRZONES, &nr_zones); if (ret) goto error; printf(\"Device has %u zones of %u 512-Bytes sectors\\n\", nr_zones, zone_size); ... The command BLKGETNRZONES is especially useful to allocate an array of zone descriptors large enough for a zone report of all zones of a device.","title":"User Interface"},{"location":"linux/zbd-api/#zoned-block-device-user-interface","text":"User applications can access zone information and manage zones of a zoned block device using two types of interfaces. sysfs attribute files accessible either directly from applications as regular files or form scripted languages (shell scripts, python, etc). ioctl() system calls suitable for use from C programs or other programming languages with an equivalent system call binding. The sysfs files and ioctl() commands available to applications have evolved since the introduction of zoned block device support in Kernel 4.10. The availability of files and commands per kernel version is detailed in the following sections.","title":"Zoned Block Device User Interface"},{"location":"linux/zbd-api/#sysfs-interface","text":"Programs using script languages (e.g. bash scripts) can access a zoned device information through sysfs attribute files. The attribute files provided are shown in the following table. File Kernel version Description /sys/block/ dev name /queue/zoned 4.10.0 Device zoned model /sys/block/ dev name /queue/chunk_sectors 4.10.0 Device zone size /sys/block/ dev name /queue/nr_zones 4.20.0 Total number of zones /sys/block/ dev name /queue/zone_append_max_bytes 5.8.0 Maximum size in bytes of a zone append write operation /sys/block/ dev name /queue/max_open_zones 5.9.0 Maximum number of open zones /sys/block/ dev name /queue/max_active_zones 5.9.0 Maximum number of active zones","title":"Sysfs Interface"},{"location":"linux/zbd-api/#device-zoned-model","text":"The zone model of a zoned device can be discovered using the zoned device queue attribute file. For instance, for a zoned block device named sdb , the following shell command displays the device zoned model. # cat /sys/block/sdb/queue/zoned host-managed The possible values of the zoned attribute file are shown in the table below. Value Description none Regular block device, including drive managed SMR disks host-aware Host aware device model host-managed Host managed device model","title":"Device Zoned Model"},{"location":"linux/zbd-api/#device-zone-size","text":"The device zone size can be read from the sysfs queue attribute file named chunk_sectors . For the same device named sdb as in the previous example, the following command gives the device zone size. # cat /sys/block/sdb/queue/chunk_sectors 524288 The value is displayed as a number of 512B sectors, regardless of the actual logical and physical block size of the device. In this example, the device zone size is 524288 x 512 = 256 MiB .","title":"Device Zone Size"},{"location":"linux/zbd-api/#number-of-zones","text":"Starting with Linux kernel version 4.20.0, the sysfs queue attribute file nr_zones is available to obtain the total number of zones of a zoned device. # cat /sys/block/sdb/queue/nr_zones 55880 This attribute value is always 0 for a regular block device.","title":"Number of Zones"},{"location":"linux/zbd-api/#ioctl-application-programming-interface","text":"The C header file /usr/include/linux/blkzoned.h contains macro definitions and data structure definitions allowing an application developer to obtain information on a zoned block device and to manage the zones of the device.","title":"ioctl() Application Programming Interface"},{"location":"linux/zbd-api/#zone-information-data-structures","text":"The data structure struct blk_zone defines a zone descriptor structure which completely describes a zone information: the zone location on the device, the zone type, its condition (state), and the position of the zone write pointer for sequential zones. Up to kernel version 5.8, this data structure is as shown below. /** * struct blk_zone - Zone descriptor for BLKREPORTZONE ioctl. * * @start: Zone start in 512 B sector units * @len: Zone length in 512 B sector units * @wp: Zone write pointer location in 512 B sector units * @type: see enum blk_zone_type for possible values * @cond: see enum blk_zone_cond for possible values * @non_seq: Flag indicating that the zone is using non-sequential resources * (for host-aware zoned block devices only). * @reset: Flag indicating that a zone reset is recommended. * @reserved: Padding to 64 B to match the ZBC/ZAC defined zone descriptor size. * * start, len, capacity and wp use the regular 512 B sector unit, regardless * of the device logical block size. The overall structure size is 64 B to * match the ZBC, ZAC and ZNS defined zone descriptor and allow support for * future additional zone information. */ struct blk_zone { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 reserved[36]; }; As indicated in the comments to this data structure definition, the unit that is used to indicate the zone start position, size and write pointer position is 512B sector size, regardless of the actual logical block size of the device. Even for a device with a 4KB physical sector, the above zone descriptor fields use a 512-byte sector size unit. The capacity field was added to struct blk_zone starting from kernel version 5.9. With this change, the data structure is as follows. /** * struct blk_zone - Zone descriptor for BLKREPORTZONE ioctl. * * @start: Zone start in 512 B sector units * @len: Zone length in 512 B sector units * @wp: Zone write pointer location in 512 B sector units * @type: see enum blk_zone_type for possible values * @cond: see enum blk_zone_cond for possible values * @non_seq: Flag indicating that the zone is using non-sequential resources * (for host-aware zoned block devices only). * @reset: Flag indicating that a zone reset is recommended. * @resv: Padding for 8B alignment. * @capacity: Zone usable capacity in 512 B sector units * @reserved: Padding to 64 B to match the ZBC, ZAC and ZNS defined zone * descriptor size. * * start, len, capacity and wp use the regular 512 B sector unit, regardless * of the device logical block size. The overall structure size is 64 B to * match the ZBC, ZAC and ZNS defined zone descriptor and allow support for * future additional zone information. */ struct blk_zone { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 resv[4]; __u64 capacity; /* Zone capacity in number of sectors */ __u8 reserved[24]; }; The capacity filed indicate the usable zone capacity of a zone in unit of 512B sectors. The presence, or validity, of this field within the structure is indicated using a zone report flag. See Obtaining Zone Information below for details.","title":"Zone Information Data Structures"},{"location":"linux/zbd-api/#zone-type","text":"The type field of a zone descriptor can only have one of the values defined by the enumeration enum blk_zone_type . /** * enum blk_zone_type - Types of zones allowed in a zoned device. * * @BLK_ZONE_TYPE_CONVENTIONAL: The zone has no write pointer and can be writen * randomly. Zone reset has no effect on the zone. * @BLK_ZONE_TYPE_SEQWRITE_REQ: The zone must be written sequentially * @BLK_ZONE_TYPE_SEQWRITE_PREF: The zone can be written non-sequentially * * Any other value not defined is reserved and must be considered as invalid. */ enum blk_zone_type { BLK_ZONE_TYPE_CONVENTIONAL = 0x1, BLK_ZONE_TYPE_SEQWRITE_REQ = 0x2, BLK_ZONE_TYPE_SEQWRITE_PREF = 0x3, };","title":"Zone Type"},{"location":"linux/zbd-api/#zone-condition","text":"The cond field of the struct blkzone data structure defines the current condition of a zone. The possible condition (state) values of this field are defined by the blk_zone_cond enumeration. /** * enum blk_zone_cond - Condition [state] of a zone in a zoned device. * * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional. * @BLK_ZONE_COND_EMPTY: The zone is empty. * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened. * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an * OPEN ZONE command. * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing. * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone * FINISH ZONE command. * @BLK_ZONE_COND_READONLY: The zone is read-only. * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written). * * The Zone Condition state machine in the ZBC/ZAC standards maps the above * deinitions as: * - ZC1: Empty | BLK_ZONE_EMPTY * - ZC2: Implicit Open | BLK_ZONE_COND_IMP_OPEN * - ZC3: Explicit Open | BLK_ZONE_COND_EXP_OPEN * - ZC4: Closed | BLK_ZONE_CLOSED * - ZC5: Full | BLK_ZONE_FULL * - ZC6: Read Only | BLK_ZONE_READONLY * - ZC7: Offline | BLK_ZONE_OFFLINE * * Conditions 0x5 to 0xC are reserved by the current ZBC/ZAC spec and should * be considered invalid. */ enum blk_zone_cond { BLK_ZONE_COND_NOT_WP = 0x0, BLK_ZONE_COND_EMPTY = 0x1, BLK_ZONE_COND_IMP_OPEN = 0x2, BLK_ZONE_COND_EXP_OPEN = 0x3, BLK_ZONE_COND_CLOSED = 0x4, BLK_ZONE_COND_READONLY = 0xD, BLK_ZONE_COND_FULL = 0xE, BLK_ZONE_COND_OFFLINE = 0xF, }; Under a device normal operation, some of the conditions defined cannot result directly from a host initiated operation. These conditions are BLK_ZONE_COND_OFFLINE and BLK_ZONE_COND_READONLY and can only be set by the device itself to indicate zones with capabilities limited by a hardware defect. The condition BLK_ZONE_COND_EXP_OPEN , or explicit open , is the result of a successful execution of an OPEN ZONE command (see Zone Block Commands . Since the OPEN ZONE command is not supported by the kernel ZBD interface, a zone can be transitioned in the explicit open zone condition only by using direct device access, that is, issuing the SCSI OPEN ZONE command through the SG_IO interface (using libzbc , libzbc zbc_open_zone utility or the sg_zone utility).","title":"Zone Condition"},{"location":"linux/zbd-api/#ioctl-commands","text":"Several ioctl() commands are defined to manipulate obtain information and manipulate zones of a zoned block device. All commands supported are shown below. /** * Zoned block device ioctl's: * * @BLKREPORTZONE: Get zone information. Takes a zone report as argument. * The zone report will start from the zone containing the * sector specified in the report request structure. * @BLKRESETZONE: Reset the write pointer of the zones in the specified * sector range. The sector range must be zone aligned. * @BLKGETZONESZ: Get the device zone size in number of 512 B sectors. * @BLKGETNRZONES: Get the total number of zones of the device. * @BLKOPENZONE: Open the zones in the specified sector range. * The 512 B sector range must be zone aligned. * @BLKCLOSEZONE: Close the zones in the specified sector range. * The 512 B sector range must be zone aligned. * @BLKFINISHZONE: Mark the zones as full in the specified sector range. * The 512 B sector range must be zone aligned. */ #define BLKREPORTZONE _IOWR(0x12, 130, struct blk_zone_report) #define BLKRESETZONE _IOW(0x12, 131, struct blk_zone_range) #define BLKGETZONESZ _IOR(0x12, 132, __u32) #define BLKGETNRZONES _IOR(0x12, 133, __u32) #define BLKOPENZONE _IOW(0x12, 134, struct blk_zone_range) #define BLKCLOSEZONE _IOW(0x12, 135, struct blk_zone_range) #define BLKFINISHZONE _IOW(0x12, 136, struct blk_zone_range) Not all commands are available on all kernel versions. The following table shows the kernel version that introduced each command. Command Kernel version Description BLKREPORTZONE 4.10.0 Get zone information BLKRESETZONE 4.10.0 Reset a zone write pointer BLKGETZONESZ 4.20.0 Get a device zone size BLKGETNRZONES 4.20.0 Get the total number of zones of a device BLKOPENZONE 5.5.0 Explicitly open a zone BLKCLOSEZONE 5.5.0 Close a zone BLKFINISHZONE 5.5.0 Finish a zone","title":"ioctl() Commands"},{"location":"linux/zbd-api/#obtaining-zone-information","text":"The BLKREPORTZONE command allows an application to obtain a device zone information in the form of an array of zone descriptors. The data argument passed to the ioctl() must be the address of a memory area large enough to store one struct blk_zone_report header structure followed by an array of zone descriptors. The zone report header structure blk_zone_report is as shown below. /** * struct blk_zone_report - BLKREPORTZONE ioctl request/reply * * @sector: starting sector of report * @nr_zones: IN maximum / OUT actual * @reserved: padding to 16 byte alignment * @zones: Space to hold @nr_zones @zones entries on reply. * * The array of at most @nr_zones must follow this structure in memory. */ struct blk_zone_report { __u64 sector; __u32 nr_zones; __u8 reserved[4]; struct blk_zone zones[0]; }; The header indicates the 512-byte sector from which the report should start and the number of zone descriptors in the array following the header. A typical use of the BLKREPORTZONE command to obtain information on all the zones of a device is as shown below. #include <stdlib.h> #include <linux/blkzoned.h> unsigned long long start_sector = 0; struct blk_zone_report *hdr; size_t hdr_len; int nr_zones = 256; hdr_len = sizeof(struct blk_zone_report) + nr_zones * sizeof(struct blkzone); hdr = malloc(hdr_len); if (!hdr) return -1; while (1) { hdr->sector = start_sector; hdr->nr_zones = nr_zones; ret = ioctl(fd, BLKREPORTZONE, hdr); if (ret) goto error; if (!hdr->nr_zones) { /* Done */ break; } printf(\"Got %u zone descriptors\\n\", hdr->nr_zones); ... /* The next report must start after the last zone reported */ start_sector = hdr->zones[hdr->nr_zones - 1].start + hdr->zones[hdr->nr_zones - 1].len; } The number of zone descriptors obtained is returned to the user using the nr_zones field of the report header structure blk_zone_report . With the introduction of zone capacity support for NVMe Zoned Namepsaces in kernel version 5.9, zone descriptors gained the capacity field. The presence of this field is indicated using the new flag field added to struct blk_zone_report /** * enum blk_zone_report_flags - Feature flags of reported zone descriptors. * * @BLK_ZONE_REP_CAPACITY: Zone descriptor has capacity field. */ enum blk_zone_report_flags { BLK_ZONE_REP_CAPACITY = (1 << 0), }; /** * struct blk_zone_report - BLKREPORTZONE ioctl request/reply * * @sector: starting sector of report * @nr_zones: IN maximum / OUT actual * @flags: one or more flags as defined by enum blk_zone_report_flags. * @zones: Space to hold @nr_zones @zones entries on reply. * * The array of at most @nr_zones must follow this structure in memory. */ struct blk_zone_report { __u64 sector; __u32 nr_zones; __u32 flags; struct blk_zone zones[0]; }; If the flags field of struct blk_zone_report has the flag BLK_ZONE_REP_CAPACITY set, then zone descriptors structure will have a valid value set in the capacity field of sturct blk_zone . Otherwise, this field can be ignored as it will always indicate a value of 0. The example code below, extracted from the code of the libzbd library, illustrates how application can implement backward compatible support for zone capacity information using the autotools build environment. # less configure.ac ... AC_CHECK_HEADER(linux/blkzoned.h, [], [AC_MSG_ERROR([Couldn't find linux/blkzoned.h. Kernel too old ?])], [[#include <linux/blkzoned.h>]]) AC_CHECK_MEMBER([struct blk_zone.capacity], [AC_DEFINE(HAVE_BLK_ZONE_REP_V2, [1], [report zones includes zone capacity])], [], [[#include <linux/blkzoned.h>]]) ... # less lib/zbd.h ... /* * Handle kernel zone capacity support */ #ifndef HAVE_BLK_ZONE_REP_V2 #define BLK_ZONE_REP_CAPACITY (1 << 0) struct blk_zone_v2 { __u64 start; /* Zone start sector */ __u64 len; /* Zone length in number of sectors */ __u64 wp; /* Zone write pointer position */ __u8 type; /* Zone type */ __u8 cond; /* Zone condition */ __u8 non_seq; /* Non-sequential write resources active */ __u8 reset; /* Reset write pointer recommended */ __u8 resv[4]; __u64 capacity; /* Zone capacity in number of sectors */ __u8 reserved[24]; }; #define blk_zone blk_zone_v2 struct blk_zone_report_v2 { __u64 sector; __u32 nr_zones; __u32 flags; struct blk_zone zones[0]; }; #define blk_zone_report blk_zone_report_v2 #endif /* HAVE_BLK_ZONE_REP_V2 */ ... With this method, the main code responsible for issuing and parsing zone reports always has access to the capacity field of struct blk_zone , regardless of the kernel version the code is executed on. For kernels preceding kernel version 5.9, the zone capacity field will always be equal to 0, meaning that the zone capacity should be ignored and the zone size used in place. Different coding techniques can also be used to always return a zone capacity equal to the zone size for kernels lacking support for this field. The command line utility blkzone , part of the util-linux project, uses the BLKREPORTZONE command to implement its report function. Its code was modified similarly to the above method to allow its correct compilation and execution regardless of the version of the kernel being used.","title":"Obtaining Zone Information"},{"location":"linux/zbd-api/#resetting-a-zone-write-pointer","text":"The write pointer of a single sequential zone or of a range of contiguous sequential zones can be reset using the BLKRESETZONE command. Resetting a sequential zone write pointer position will also transition the zone to the Empty condition ( BLK_ZONE_COND_EMPTY ). The range of zones to reset is defined using the data structure blk_zone_range shown below. /** * struct blk_zone_range - BLKRESETZONE/BLKOPENZONE/ * BLKCLOSEZONE/BLKFINISHZONE ioctl * requests * @sector: Starting sector of the first zone to operate on. * @nr_sectors: Total number of sectors of all zones to operate on. */ struct blk_zone_range { __u64 sector; __u64 nr_sectors; }; The sector field must specify the start sector of the first zone to reset. The nr_sectors field specifies the total length of the range of zones to reset. This length must be at least as large as one zone. As indicated in comments describing the blk_zone_range structure, the commands BLKOPENZONE , BLKCLOSEZONE and BLKFINISHZONE also use this data structure to define the range of zones on which the command will operate. The following code shows an example use of the BLKRESETZONE command to reset a single zone starting at sector 274726912 with a zone size of 256 MiB (524288 sectors of 512B). #include <linux/blkzoned.h> struct blk_zone_range zrange; int ret; zrange.sector = 274726912; zrange.nr_sectors = 524288; ret = ioctl(fd, BLKRESETZONE, &zrange); if (ret) goto error; ... The device file descriptor fd must be open for writing for this command to succeed. The command line utility blkzone uses the BLKRESETZONE command to implement its reset functionality.","title":"Resetting a Zone Write Pointer"},{"location":"linux/zbd-api/#opening-closing-and-finishing-zones","text":"Explicitely opening a zone or a range of zones can be done using the BLKOPENZONE command. This command uses the same arguments as the BLKRESETZONE command. It takes a pointer to a data structure blk_zone_range which must specifies the range of zones to operate on. Closing a zone is done using the command BLKCLOSEZONE . Finishing a zone, that is transitioning the zone to the full condition ( BLK_ZONE_COND_FULL ), is done using the BLKFINISHZONE command. Both of these commands also take as an argument a pointer to a blk_zone_range data structure to specify the range of zones to operate on. The BLKOPENZONE , BLKCLOSEZONE and BLKFINISHZONE commands were introduced with kernel version 5.5.0.","title":"Opening, Closing and Finishing Zones"},{"location":"linux/zbd-api/#zone-size-and-number-of-zones","text":"Linux\u00ae kernel version 4.20 introduced two new additional commands to obtain a zoned device zone size ( BLKGETZONESZ ) and the total number of zones of the device ( BLKGETNRZONES ). Both commands take a pointer to an unsigned 32-bits integer variable as argument where the zone size value or the number of zones will be returned. The following sample C code illustrates the use of these commands. #include <linux/blkzoned.h> #include <stdio.h> unsigned int nr_zones, zone_size; int ret; ret = ioctl(fd, ,BLKGETZONESZ, &zone_size); if (ret) goto error; ret = ioctl(fd, ,BLKGETNRZONES, &nr_zones); if (ret) goto error; printf(\"Device has %u zones of %u 512-Bytes sectors\\n\", nr_zones, zone_size); ... The command BLKGETNRZONES is especially useful to allocate an array of zone descriptors large enough for a zone report of all zones of a device.","title":"Zone Size and Number of Zones"},{"location":"projects/","text":"Applications and Libraries This collection of articles describes user level applications and libraries supporting zoned block devices. Linux System Utilities : Learn about the icollection of Linux\u00ae utilities supporting zoned block devices. SCSI Generic Utilities : Learn about ZBC feature set support of the SCSI generic package ( sg3utils ); libzbc User Library : Learn about the programming interface and tools provided by the libzbc user library to manipulate ZBC and ZAC devices. libnvme User Library : Learn about the programming interface and tools provided by the libnvme user library to manipulate NVMe controllers and namespaces supporting the Zoned Namespace command set. libzbd User Library : Learn about the programming interface and tools provided by the libzbd user library to facilitate the management of zoned block devices using a kernel including zoned block device support. tcmu-runner ZBC Disk Emulation : Learn how to use the tcmu-runner SCSI device emulation tool to setup emulated ZBC disks. QEMU : Learn how to attach zoned block devices to a QEMU guest. Linux Tools for ZNS : Learn about Linux kernel support and tooling for NVM Express' Zoned Namespace (ZNS) Command Set.","title":"Applications and Libraries"},{"location":"projects/#applications-and-libraries","text":"This collection of articles describes user level applications and libraries supporting zoned block devices. Linux System Utilities : Learn about the icollection of Linux\u00ae utilities supporting zoned block devices. SCSI Generic Utilities : Learn about ZBC feature set support of the SCSI generic package ( sg3utils ); libzbc User Library : Learn about the programming interface and tools provided by the libzbc user library to manipulate ZBC and ZAC devices. libnvme User Library : Learn about the programming interface and tools provided by the libnvme user library to manipulate NVMe controllers and namespaces supporting the Zoned Namespace command set. libzbd User Library : Learn about the programming interface and tools provided by the libzbd user library to facilitate the management of zoned block devices using a kernel including zoned block device support. tcmu-runner ZBC Disk Emulation : Learn how to use the tcmu-runner SCSI device emulation tool to setup emulated ZBC disks. QEMU : Learn how to attach zoned block devices to a QEMU guest. Linux Tools for ZNS : Learn about Linux kernel support and tooling for NVM Express' Zoned Namespace (ZNS) Command Set.","title":"Applications and Libraries"},{"location":"projects/libnvme/","text":"libnvme User Library libnvme is an open source user library providing defintions and functions for interacting with nvme devices. While nvme-cli provides convenient ways for a user to interact with nvme devices from the shell, libnvme provides similiar access for other programs. Overview libnvme provides functions for discoverying and managing all nvme devices in a linux environment. When the NVMe ZNS specification was ratified, libnvme incorporated defintions for all the types and commands that specification provides. The library can be used to construct nvme passthrough commands and dispatch these through the Linux nvme driver. For commands that return data, the library provides structures and enumerations to help decode the payloads. Library Functions All of the ZNS functions provided by libnvme are prefixed with the \"nvme_zns_\" name. The following are the admin commands defined from the ZNS specifcation. Function Description nvme_zns_identify_ns() Retrieves the nvme_zns_id_ns structure nvme_zns_identify_ctrl() Retrieves the nvme_zns_id_ctrl structure nvme_zns_get_log_changed_zones() Retrieves the nvme_zns_changed_zone_log structure In addition to admin commands, ZNS also defines new IO commands, and libnvme provides the following APIs to send them: Function Description nvme_zns_append() Append data to a zone nvme_zns_mgmt_send() Requests an action on one or all zones nvme_zns_mgmt_recv() Returns data containing information about zones The types of actions that the nvme_zns_mgmt_send() can be done are defined as follows: Action Description NVME_ZNS_ZSA_CLOSE Sets the zone state to Close NVME_ZNS_ZSA_FINISH Sets the zone state to Full NVME_ZNS_ZSA_OPEN Sets the zone state to Open NVME_ZNS_ZSA_RESET Sets the zone state to Empty NVME_ZNS_ZSA_OFFLINE Sets the zone state to Offline NVME_ZNS_ZSA_SET_DESC_EXT Sets the zone descriptor extention data, if available Library Types ZNS created new constant types and structures. The following are the structures provided by libnvme from the ZNS specification: Structure Description nvme_zns_id_ns ZNS specific Namespace Identification, returned from nvme_zns_identify_ns() nvme_zns_id_ctrl ZNS specific Controller Identification, returned from nvme_zns_identify_ctrl() nvme_zns_changed_zone_log Log page that indicaties if a zone descriptor has changed for one or more zones, returned from nvme_zns_get_log_changed_zones() nvme_zone_report Provides the structure returned from a ZNS Report Zones command, returned from nvme_zns_mgmt_recv() Additional Documentation libnvme provides more detailed documentation in html and man pages for all functions and types.","title":"libnvme User Library"},{"location":"projects/libnvme/#libnvme-user-library","text":"libnvme is an open source user library providing defintions and functions for interacting with nvme devices. While nvme-cli provides convenient ways for a user to interact with nvme devices from the shell, libnvme provides similiar access for other programs.","title":"libnvme User Library"},{"location":"projects/libnvme/#overview","text":"libnvme provides functions for discoverying and managing all nvme devices in a linux environment. When the NVMe ZNS specification was ratified, libnvme incorporated defintions for all the types and commands that specification provides. The library can be used to construct nvme passthrough commands and dispatch these through the Linux nvme driver. For commands that return data, the library provides structures and enumerations to help decode the payloads.","title":"Overview"},{"location":"projects/libnvme/#library-functions","text":"All of the ZNS functions provided by libnvme are prefixed with the \"nvme_zns_\" name. The following are the admin commands defined from the ZNS specifcation. Function Description nvme_zns_identify_ns() Retrieves the nvme_zns_id_ns structure nvme_zns_identify_ctrl() Retrieves the nvme_zns_id_ctrl structure nvme_zns_get_log_changed_zones() Retrieves the nvme_zns_changed_zone_log structure In addition to admin commands, ZNS also defines new IO commands, and libnvme provides the following APIs to send them: Function Description nvme_zns_append() Append data to a zone nvme_zns_mgmt_send() Requests an action on one or all zones nvme_zns_mgmt_recv() Returns data containing information about zones The types of actions that the nvme_zns_mgmt_send() can be done are defined as follows: Action Description NVME_ZNS_ZSA_CLOSE Sets the zone state to Close NVME_ZNS_ZSA_FINISH Sets the zone state to Full NVME_ZNS_ZSA_OPEN Sets the zone state to Open NVME_ZNS_ZSA_RESET Sets the zone state to Empty NVME_ZNS_ZSA_OFFLINE Sets the zone state to Offline NVME_ZNS_ZSA_SET_DESC_EXT Sets the zone descriptor extention data, if available","title":"Library Functions"},{"location":"projects/libnvme/#library-types","text":"ZNS created new constant types and structures. The following are the structures provided by libnvme from the ZNS specification: Structure Description nvme_zns_id_ns ZNS specific Namespace Identification, returned from nvme_zns_identify_ns() nvme_zns_id_ctrl ZNS specific Controller Identification, returned from nvme_zns_identify_ctrl() nvme_zns_changed_zone_log Log page that indicaties if a zone descriptor has changed for one or more zones, returned from nvme_zns_get_log_changed_zones() nvme_zone_report Provides the structure returned from a ZNS Report Zones command, returned from nvme_zns_mgmt_recv()","title":"Library Types"},{"location":"projects/libnvme/#additional-documentation","text":"libnvme provides more detailed documentation in html and man pages for all functions and types.","title":"Additional Documentation"},{"location":"projects/libzbc/","text":"libzbc User Library libzbc is a user library providing functions for manipulating ZBC and ZAC disks. libzbc command implementation is compliant with the latest published versions of the ZBC and ZAC standards defined by INCITS technical committees T10 and T13 (respectively). In addition to supporting ZBC and ZAC disks, libzbc also implements an emulation mode allowing the library to imitate the behavior of a host managed zoned disk using a regular file or a standard block device as the backing store. The libzbc project is hosted on GitHub . The project README file provides information on how to compile and install libzbc library and its tools. Note The libzbc project was formerly hosted on GitHub as part of the HGST organization . libzbc repository has since then moved to the Western Digital Corporation organization on GitHub . libzbc also provides a test suite allowing to test the conformance of disks and HBAs to the ZBC and ZAC standards. In order to make the test suite available, libzbc needs to be configured using --with-test option prior to building and installing the library. The usage of this test suite is described in more details here . Overview libzbc provides a unified application programming interface (API) that is independent of the zone model and interface of the disk being used. Internally, four different types of device drivers are used to handle device interface dependent commands. ZAC ATA Driver This driver is used to handle direct delivery of ATA commands to ZAC disks through the SCSI generic driver (direct device access interface). ZBC SCSI Driver This driver primarily handles SCSI commands directed at ZBC SCSI disks, but can also be used to control ZAC ATA disks if a functional SCSI to ATA (SAT) command translation layer is functional in the command path. SAT may be provided either by the kernel libata subsystem for ATA disks connected to SATA adapters or by a SAS host bus adapter (HBA) for SATA disks connected to such adapter. Zoned Block Device Driver This driver uses the kernel ZBD interface to control both ZBC and ZAC disks. This driver is only available if the kernel zoned block device support is present and enabled. File Emulation Driver This driver implements emulation of a host managed ZBC disk using a regular file or regular block device as backend storage. This driver is intended for development only. A more advanced ZBC disk emulation solution is provided by the tcmu-runner project. The figure below shows this structure. libzbc internal backend drivers organization libzbc provides functions for discovering the zone configuration of a zoned device and for accessing the device. Accesses to the device may result in changes to the condition, attributes or state of device zones (such as write pointer location in sequential zones). These changes are not internally tracked by libzbc . That is, libzbc is stateless. The functions provided to obtain the device zone information only make a \"snapshot\" of the zone condition and state when executed. It is the responsibility of applications to implement tracking of the device zone changes such as the increasing write pointer position of a sequential zone after the completion of write requests to the zone. Library Functions All libzbc functions, since version 5.0.0, use a 512 bytes sector unit for reporting zone information and as the sector addressing unit for device accesses regardless of the actual device logical block size. This unification in the unit used by all functions can simplify application development by hiding potential differences in logical block sizes between devices. However, application programmers must be careful to always implement accesses (read or write) to the device aligned to the device logical block size. Furthermore, on host managed zoned devices, write operations to sequential zones must be aligned to the device physical block size. The main functions provided by libzbc are as follows. Function Description zbc_open() Open a zoned device zbc_close() Close a zoned device zbc_get_device_info() Get device information zbc_report_nr_zones() Get the number of zones zbc_report_zones() zbc_list_zones() Get zone information zbc_zone_operation() Execute a zone operation zbc_open_zone() Explicitly open a zone zbc_close_zone() Close an open zone zbc_finish_zone() Finish a zone zbc_reset_zone() Reset a zone write pointer zbc_pread() Read data from a zone zbc_pwrite() Write data to a zone zbc_flush() Flush data to disk More detailed information about these functions usage and behavior can be found in the comments of libzbc header file . This header file is by default installed as /usr/include/libzbc/zbc.h . libzbc does not implement any mutual exclusion mechanism for multi-thread or multi-process applications. This implies that it is the responsibility of application to synchronize the execution of conflicting operations targeting the same zone. A typical example of such case is concurrent write operations to the same zone by multiple threads which may result in write errors without write ordering control by the application. The following functions are also provided by libzbc to facilitate application development and tests. Function Description zbc_set_log_level() Set log level of the library functions zbc_device_is_zoned() Test if a device is a zoned block device zbc_print_device_info() Print to a file (stream) a device information zbc_device_type_str() Get a string description of a device type zbc_device_model_str() Get a string description of a device model zbc_zone_type_str() Get a string description of a zone type zbc_zone_condition_str() Get a string description of a zone condition zbc_errno() Return sense key and sense code of the last command executed zbc_sk_str() Get a string description of a sense key zbc_asc_ascq_str() Get a string description of a sense code All functions will behave in the same manner regardless of the type of disk being used. The only exception to this principle is the zbc_errno() function inability to report detailed error information when the zoned block device driver is used. The reason for this is that the kernel I/O stack does not have the ability to propagate up to the application the detailed information provided in command sense data with failed commands. Utilities libzbc also provides several command line applications to manipulate zoned disks by calling the library functions. The list of applications provided is shown in the table below. Tool Description gzbc gzbc provides a graphical user interface showing zone information of a zoned device zbc_info Get information on a disk zbc_report_zones List the zones of a device zbc_reset_zone Reset the write pointer of sequential zones zbc_open_zone Explicitly open a sequential write zone zbc_close_zone Close an open sequential write zone zbc_finish_zone Transition a sequential write zone to the full condition zbc_read_zone Read a zone sectors zbc_write_zone Write a zone sectors The following tools are also provided to create and modify the regular file or regular block device used as backend storage with libzbc emulation mode. Tool Description zbc_set_zones Initialize (format) a regular file or regular disk to be used with libzbc emulation mode zbc_set_write_ptr Change the write pointer position of the sequential zones of an emulated disk (intended for testing purposes only as this is not a valid operation for physical ZBC devices All utilities output a help message when executed without any argument. # zbc_report_zones Usage: zbc_report_zones [options] <dev> Options: -v : Verbose mode -lba : Use LBA size unit (default is 512B sectors) -start <offset> : Start offset of report. if \"-lba\" is used <offset> is interpreted as an LBA. Otherwise, it is interpreted as a 512B sector number. Default is 0 -n : Get only the number of zones -nz <num> : Get at most <num> zones -ro <opt> : Specify reporting option: \"all\", \"empty\", \"imp_open\", \"exp_open\", \"closed\", \"full\", \"rdonly\", \"offline\", \"rwp\", \"non_seq\" or \"not_wp\". Default is \"all\" Getting a Disk Information zbc_info displays information about a disk, including regular block devices. # zbc_info /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 # zbc_info /dev/sg0 /dev/sg0 is not a zoned block device Zone Information zbc_report_zones illustrates the use of libzbc zone reporting functions zbc_report_zones(), zbc_report_nr_zones() and zbc_list_zones(). # zbc_report_zones /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors ... Zone 00522: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273678336, 524288 sectors Zone 00523: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 274202624, 524288 sectors Zone 00524: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 Zone 00525: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275251200 ... Zone 55878: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296164864, 524288 sectors, wp 29296164864 Zone 55879: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296689152, 524288 sectors, wp 29296689152 The output of zbc_report_zones can also be limited using the option -ro (reporting options) for filtering the zones reported, or the option -n to limit the output to the number of zones that would be reported. # zbc_report_zones -ro not_wp -n /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 524 zones from 0, reporting option 0x3f Writing and Resetting Zones zbc_write_zone sequentially writes data to a zone. If the zone is a sequential zone, the write pointer position advances until the zone is full. By default, zbc_write_zone will write the entire zone. For instance, using the disk used in the previous example, the first sequential write zone (zone number 524) can be written with 512 KB writes until full with the following command. # zbc_write_zone /dev/sg3 524 524288 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 Target zone: Zone 524 / 55880, type 0x2 (Sequential-write-required), cond 0x1 (Empty), rwp 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 Filling target zone 524, 524288 B I/Os Wrote 268435456 B (512 I/Os) in 1.082 sec IOPS 472 BW 247.952 MB/s # zbc_report_zones -start 274726912 -nz 1 /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55356 zones from 274726912, reporting option 0x00 1 / 55356 zone: Zone 00000: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 2251799813685240 The written zone can then be reset using zbc_reset_zone . # zbc_reset_zone /dev/sg3 524 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 reset zone 524/55880, sector 274726912... # zbc_report_zones -start 274726912 -nz 1 /dev/sg7 Device /dev/sg7: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55356 zones from 274726912, reporting option 0x00 1 / 55356 zone: Zone 00000: type 0x2 (Sequential-write-required), cond 0x1 (Empty) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 All sequential write zones of a device can be reset using the option -all . # zbc_reset_zone -all /dev/sg7 Device /dev/sg7: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 Operating on all zones... Graphical Interface gzbc provides a graphical user interface showing the zone configuration and state of a zoned device. gzbc also displays the write status (write pointer position) of zones graphically using color coding (red for written sectors and green for unwritten sectors). Some operations on zones can also be executed directly from the interface (reset zone write pointer, open zone, close zone, etc). gzbc screenshot Emulation Mode libzbc emulation mode requires a regular file or a regular block device as backend storage. The size of the file or the capacity of the block device used will be used as the emulated device capacity. For example, the following commands will create a 10GB file name zbc-disk that can be used for an emulated disk. # touch zbc-disk # truncate -s 10G zbc-disk # ls -l zbc-disk -rw-r--r-- 1 root root 10737418240 May 22 16:48 zbc-disk This file can now be initialized to be used with libzbc emulation mode. # zbc_set_zones zbc-disk set_ps 10 256 Device zbc-disk: Vendor ID: FAKE HGST HM libzbc Emulated zoned block device interface, Host-managed zone model 20971520 512-bytes sectors 20971520 logical blocks of 512 B 2621440 physical blocks of 4096 B 10.737 GB capacity Read commands are restricted Maximum number of open sequential write required zones: 32 Setting zones: Zone size: 256 MiB (524288 sectors) Conventional zones: 1024 MiB (2097152 sectors), 10.00 % of total capacity), 4 zones Sequential zones: 36 zones The file can now be used with libzbc library functions and tools similarly to any physical zoned block device. # zbc_report_zones zbc-disk Device zbc-disk: Vendor ID: FAKE HGST HM libzbc Emulated zoned block device interface, Host-managed zone model 20971520 512-bytes sectors 20971520 logical blocks of 512 B 2621440 physical blocks of 4096 B 10.737 GB capacity Read commands are restricted Maximum number of open sequential write required zones: 32 40 zones from 0, reporting option 0x00 40 / 40 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors Zone 00003: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1572864, 524288 sectors Zone 00004: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 2097152, 524288 sectors, wp 2097152 Zone 00005: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 2621440, 524288 sectors, wp 2621440 ... Zone 00038: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 19922944, 524288 sectors, wp 19922944 Zone 00039: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 20447232, 524288 sectors, wp 20447232","title":"libzbc User Library"},{"location":"projects/libzbc/#libzbc-user-library","text":"libzbc is a user library providing functions for manipulating ZBC and ZAC disks. libzbc command implementation is compliant with the latest published versions of the ZBC and ZAC standards defined by INCITS technical committees T10 and T13 (respectively). In addition to supporting ZBC and ZAC disks, libzbc also implements an emulation mode allowing the library to imitate the behavior of a host managed zoned disk using a regular file or a standard block device as the backing store. The libzbc project is hosted on GitHub . The project README file provides information on how to compile and install libzbc library and its tools. Note The libzbc project was formerly hosted on GitHub as part of the HGST organization . libzbc repository has since then moved to the Western Digital Corporation organization on GitHub . libzbc also provides a test suite allowing to test the conformance of disks and HBAs to the ZBC and ZAC standards. In order to make the test suite available, libzbc needs to be configured using --with-test option prior to building and installing the library. The usage of this test suite is described in more details here .","title":"libzbc User Library"},{"location":"projects/libzbc/#overview","text":"libzbc provides a unified application programming interface (API) that is independent of the zone model and interface of the disk being used. Internally, four different types of device drivers are used to handle device interface dependent commands. ZAC ATA Driver This driver is used to handle direct delivery of ATA commands to ZAC disks through the SCSI generic driver (direct device access interface). ZBC SCSI Driver This driver primarily handles SCSI commands directed at ZBC SCSI disks, but can also be used to control ZAC ATA disks if a functional SCSI to ATA (SAT) command translation layer is functional in the command path. SAT may be provided either by the kernel libata subsystem for ATA disks connected to SATA adapters or by a SAS host bus adapter (HBA) for SATA disks connected to such adapter. Zoned Block Device Driver This driver uses the kernel ZBD interface to control both ZBC and ZAC disks. This driver is only available if the kernel zoned block device support is present and enabled. File Emulation Driver This driver implements emulation of a host managed ZBC disk using a regular file or regular block device as backend storage. This driver is intended for development only. A more advanced ZBC disk emulation solution is provided by the tcmu-runner project. The figure below shows this structure. libzbc internal backend drivers organization libzbc provides functions for discovering the zone configuration of a zoned device and for accessing the device. Accesses to the device may result in changes to the condition, attributes or state of device zones (such as write pointer location in sequential zones). These changes are not internally tracked by libzbc . That is, libzbc is stateless. The functions provided to obtain the device zone information only make a \"snapshot\" of the zone condition and state when executed. It is the responsibility of applications to implement tracking of the device zone changes such as the increasing write pointer position of a sequential zone after the completion of write requests to the zone.","title":"Overview"},{"location":"projects/libzbc/#library-functions","text":"All libzbc functions, since version 5.0.0, use a 512 bytes sector unit for reporting zone information and as the sector addressing unit for device accesses regardless of the actual device logical block size. This unification in the unit used by all functions can simplify application development by hiding potential differences in logical block sizes between devices. However, application programmers must be careful to always implement accesses (read or write) to the device aligned to the device logical block size. Furthermore, on host managed zoned devices, write operations to sequential zones must be aligned to the device physical block size. The main functions provided by libzbc are as follows. Function Description zbc_open() Open a zoned device zbc_close() Close a zoned device zbc_get_device_info() Get device information zbc_report_nr_zones() Get the number of zones zbc_report_zones() zbc_list_zones() Get zone information zbc_zone_operation() Execute a zone operation zbc_open_zone() Explicitly open a zone zbc_close_zone() Close an open zone zbc_finish_zone() Finish a zone zbc_reset_zone() Reset a zone write pointer zbc_pread() Read data from a zone zbc_pwrite() Write data to a zone zbc_flush() Flush data to disk More detailed information about these functions usage and behavior can be found in the comments of libzbc header file . This header file is by default installed as /usr/include/libzbc/zbc.h . libzbc does not implement any mutual exclusion mechanism for multi-thread or multi-process applications. This implies that it is the responsibility of application to synchronize the execution of conflicting operations targeting the same zone. A typical example of such case is concurrent write operations to the same zone by multiple threads which may result in write errors without write ordering control by the application. The following functions are also provided by libzbc to facilitate application development and tests. Function Description zbc_set_log_level() Set log level of the library functions zbc_device_is_zoned() Test if a device is a zoned block device zbc_print_device_info() Print to a file (stream) a device information zbc_device_type_str() Get a string description of a device type zbc_device_model_str() Get a string description of a device model zbc_zone_type_str() Get a string description of a zone type zbc_zone_condition_str() Get a string description of a zone condition zbc_errno() Return sense key and sense code of the last command executed zbc_sk_str() Get a string description of a sense key zbc_asc_ascq_str() Get a string description of a sense code All functions will behave in the same manner regardless of the type of disk being used. The only exception to this principle is the zbc_errno() function inability to report detailed error information when the zoned block device driver is used. The reason for this is that the kernel I/O stack does not have the ability to propagate up to the application the detailed information provided in command sense data with failed commands.","title":"Library Functions"},{"location":"projects/libzbc/#utilities","text":"libzbc also provides several command line applications to manipulate zoned disks by calling the library functions. The list of applications provided is shown in the table below. Tool Description gzbc gzbc provides a graphical user interface showing zone information of a zoned device zbc_info Get information on a disk zbc_report_zones List the zones of a device zbc_reset_zone Reset the write pointer of sequential zones zbc_open_zone Explicitly open a sequential write zone zbc_close_zone Close an open sequential write zone zbc_finish_zone Transition a sequential write zone to the full condition zbc_read_zone Read a zone sectors zbc_write_zone Write a zone sectors The following tools are also provided to create and modify the regular file or regular block device used as backend storage with libzbc emulation mode. Tool Description zbc_set_zones Initialize (format) a regular file or regular disk to be used with libzbc emulation mode zbc_set_write_ptr Change the write pointer position of the sequential zones of an emulated disk (intended for testing purposes only as this is not a valid operation for physical ZBC devices All utilities output a help message when executed without any argument. # zbc_report_zones Usage: zbc_report_zones [options] <dev> Options: -v : Verbose mode -lba : Use LBA size unit (default is 512B sectors) -start <offset> : Start offset of report. if \"-lba\" is used <offset> is interpreted as an LBA. Otherwise, it is interpreted as a 512B sector number. Default is 0 -n : Get only the number of zones -nz <num> : Get at most <num> zones -ro <opt> : Specify reporting option: \"all\", \"empty\", \"imp_open\", \"exp_open\", \"closed\", \"full\", \"rdonly\", \"offline\", \"rwp\", \"non_seq\" or \"not_wp\". Default is \"all\"","title":"Utilities"},{"location":"projects/libzbc/#getting-a-disk-information","text":"zbc_info displays information about a disk, including regular block devices. # zbc_info /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 # zbc_info /dev/sg0 /dev/sg0 is not a zoned block device","title":"Getting a Disk Information"},{"location":"projects/libzbc/#zone-information","text":"zbc_report_zones illustrates the use of libzbc zone reporting functions zbc_report_zones(), zbc_report_nr_zones() and zbc_list_zones(). # zbc_report_zones /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55880 zones from 0, reporting option 0x00 55880 / 55880 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors ... Zone 00522: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 273678336, 524288 sectors Zone 00523: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 274202624, 524288 sectors Zone 00524: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 Zone 00525: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 275251200, 524288 sectors, wp 275251200 ... Zone 55878: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296164864, 524288 sectors, wp 29296164864 Zone 55879: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 29296689152, 524288 sectors, wp 29296689152 The output of zbc_report_zones can also be limited using the option -ro (reporting options) for filtering the zones reported, or the option -n to limit the output to the number of zones that would be reported. # zbc_report_zones -ro not_wp -n /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 Zoned block device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 524 zones from 0, reporting option 0x3f","title":"Zone Information"},{"location":"projects/libzbc/#writing-and-resetting-zones","text":"zbc_write_zone sequentially writes data to a zone. If the zone is a sequential zone, the write pointer position advances until the zone is full. By default, zbc_write_zone will write the entire zone. For instance, using the disk used in the previous example, the first sequential write zone (zone number 524) can be written with 512 KB writes until full with the following command. # zbc_write_zone /dev/sg3 524 524288 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 Target zone: Zone 524 / 55880, type 0x2 (Sequential-write-required), cond 0x1 (Empty), rwp 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 Filling target zone 524, 524288 B I/Os Wrote 268435456 B (512 I/Os) in 1.082 sec IOPS 472 BW 247.952 MB/s # zbc_report_zones -start 274726912 -nz 1 /dev/sg3 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55356 zones from 274726912, reporting option 0x00 1 / 55356 zone: Zone 00000: type 0x2 (Sequential-write-required), cond 0xe (Full) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 2251799813685240 The written zone can then be reset using zbc_reset_zone . # zbc_reset_zone /dev/sg3 524 Device /dev/sg3: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 reset zone 524/55880, sector 274726912... # zbc_report_zones -start 274726912 -nz 1 /dev/sg7 Device /dev/sg7: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 55356 zones from 274726912, reporting option 0x00 1 / 55356 zone: Zone 00000: type 0x2 (Sequential-write-required), cond 0x1 (Empty) , reset recommended 0, non_seq 0, sector 274726912, 524288 sectors, wp 274726912 All sequential write zones of a device can be reset using the option -all . # zbc_reset_zone -all /dev/sg7 Device /dev/sg7: Vendor ID: ATA HGST HSH721415AL T220 SCSI ZBC device interface, Host-managed zone model 29297213440 512-bytes sectors 3662151680 logical blocks of 4096 B 3662151680 physical blocks of 4096 B 15000.173 GB capacity Read commands are unrestricted Maximum number of open sequential write required zones: 128 Operating on all zones...","title":"Writing and Resetting Zones"},{"location":"projects/libzbc/#graphical-interface","text":"gzbc provides a graphical user interface showing the zone configuration and state of a zoned device. gzbc also displays the write status (write pointer position) of zones graphically using color coding (red for written sectors and green for unwritten sectors). Some operations on zones can also be executed directly from the interface (reset zone write pointer, open zone, close zone, etc). gzbc screenshot","title":"Graphical Interface"},{"location":"projects/libzbc/#emulation-mode","text":"libzbc emulation mode requires a regular file or a regular block device as backend storage. The size of the file or the capacity of the block device used will be used as the emulated device capacity. For example, the following commands will create a 10GB file name zbc-disk that can be used for an emulated disk. # touch zbc-disk # truncate -s 10G zbc-disk # ls -l zbc-disk -rw-r--r-- 1 root root 10737418240 May 22 16:48 zbc-disk This file can now be initialized to be used with libzbc emulation mode. # zbc_set_zones zbc-disk set_ps 10 256 Device zbc-disk: Vendor ID: FAKE HGST HM libzbc Emulated zoned block device interface, Host-managed zone model 20971520 512-bytes sectors 20971520 logical blocks of 512 B 2621440 physical blocks of 4096 B 10.737 GB capacity Read commands are restricted Maximum number of open sequential write required zones: 32 Setting zones: Zone size: 256 MiB (524288 sectors) Conventional zones: 1024 MiB (2097152 sectors), 10.00 % of total capacity), 4 zones Sequential zones: 36 zones The file can now be used with libzbc library functions and tools similarly to any physical zoned block device. # zbc_report_zones zbc-disk Device zbc-disk: Vendor ID: FAKE HGST HM libzbc Emulated zoned block device interface, Host-managed zone model 20971520 512-bytes sectors 20971520 logical blocks of 512 B 2621440 physical blocks of 4096 B 10.737 GB capacity Read commands are restricted Maximum number of open sequential write required zones: 32 40 zones from 0, reporting option 0x00 40 / 40 zones: Zone 00000: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 0, 524288 sectors Zone 00001: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 524288, 524288 sectors Zone 00002: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1048576, 524288 sectors Zone 00003: type 0x1 (Conventional), cond 0x0 (Not-write-pointer), sector 1572864, 524288 sectors Zone 00004: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 2097152, 524288 sectors, wp 2097152 Zone 00005: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 2621440, 524288 sectors, wp 2621440 ... Zone 00038: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 19922944, 524288 sectors, wp 19922944 Zone 00039: type 0x2 (Sequential-write-required), cond 0x1 (Empty), reset recommended 0, non_seq 0, sector 20447232, 524288 sectors, wp 20447232","title":"Emulation Mode"},{"location":"projects/libzbd/","text":"libzbd User Library libzbd is a user library providing functions for manipulating zoned block devices. Unlike the libzbc library, libzbd does not implement direct command access to zoned block devices. Rather, libzbd uses the kernel provided zoned block device interface based on the ioctl() system call. A direct consequence of this is that libzbd will only allow access to zoned block devices supported by the kernel running. This includes both physical devices such as hard-disks supporting the ZBC and ZAC standards, as well as all logical block devices implemented by various device drivers such as null_blk and device mapper drivers. The libzbd project is hosted on GitHub . The project README file provides information on how to compile and install libzbd library and its tools. Overview libzbd provides functions for discovering and managing the state of zones of zoned block devices. Read and write accesses to the devices can be done using standard standard I/O system calls. The execution of libzbd functions as well as of write operations to zones of a device may result in changes to the condition or attributes of the device zones (such as write pointer location in sequential zones). These changes are not internally tracked by libzbd . In other words, libzbd is stateless. It is the responsibility of applications to implement tracking of the changes to zones conditions such as the increasing write pointer position of a sequential zone after the completion of a write request to the zone. Library Functions All libzbd functions use bytes unit for zones information such as the zone start position on the device, a zone size or the zone write pointer location. Zoned block devices are identified using regular file descriptor numbers that can be used as is with standard I/O system calls. However, application programmers must be careful to always implement read accesses aligned to the device logical block size. Furthermore, on host managed zoned block devices, write operations to sequential zones must be aligned to the device physical block size. The main functions provided by libzbd are as follows. Function Description zbd_open() Open a zoned block device zbd_close() Close an open zoned block device zbd_get_info() Get a device information zbd_report_nr_zones() Get the number of zones of a device zbd_report_zones() zbd_list_zones() Get a device zone information zbd_zones_operation() Execute an operation on a range of zones zbd_open_zones() Explicitly open a range of zone zbd_close_zones() Close a range of zones zbd_reset_zones() Reset the write pointer of a range of zones zbd_finish_zones() Finish a range of zone More detailed information about these functions usage and behavior can be found in the comments of libzbd header file . This header file is by default installed as /usr/include/libzbd/zbd.h . libzbd does not implement any mutual exclusion mechanism for multi-thread or multi-process applications. This implies that it is the responsibility of applications to synchronize the execution of conflicting operations targeting the same zone. A typical example of such case is concurrent write operations to the same zone by multiple threads which may result in write errors without write ordering control by the application. The following functions are also provided by libzbd to facilitate application development and tests. Function Description zbd_device_is_zoned() Test if a device is a zoned block device zbd_device_model_str() Get a string description of a device model zbd_zone_type_str() Get a string description of a zone type zbd_zone_cond_str() Get a string description of a zone condition zbd_set_log_level() Set the library verbosity level All functions will behave in the same manner regardless of the type of disk being used. Utilities libzbd also provides several command line applications to manipulate zoned block devices by calling the library functions. The list of applications provided is shown in the table below. Tool Description zbd Command line utility to report, open, close, reset and finish zones of a device gzbd Similar to the zbd tool but using a graphical user interface gzbd-viewer Graphical user interface showing the condition and state of zones of a zoned block device All utilities output a help message when executed without any argument. # zbd Usage: zbd <command> [options] <dev> Command: report : Get zone information reset : Reset zone(s) open : Explicitly open zone(s) close : Close zone(s) finish : Finish zone(s) Common options: -v : Verbose mode (for debug) -i : Display device information -ofst <ofst (B)> : Start offset of the first zone of the target range (default: 0) -len <len (B)> : Size of the zone range to operate on (default: device capacity) -u <unit (B)> : Size unit for the ofst and len options and for displaying zone report results. (default: 1) Report command options: -csv : Use csv output format -n : Only output the number of zones in the report -ro <opt> : Specify zone report filter. * \"em\": empty zones * \"oi\": implicitly open zones * \"oe\": explicitly open zones * \"cl\": closed zones * \"fu\": full zones * \"ro\": read-only zones * \"ol\": offline zones * \"nw\": conventional zones * \"ns\": non-seq write resource zones * \"rw\": reset-wp recommended zones Manual pages are also provided for each tool. # man zbd ZBD(8) System Manager's Manual ZBD(8) NAME zbd - manage zoned block devices SYNOPSIS zbd command [options] device DESCRIPTION zbd is used to manipulate zones of a zoned block device. Zoned block devies are block devices that support the SCSI Zoned Block Commands (ZBC), ATA Zoned-device ATA Commands (ZAC) or NVMe Zoned NameSpace commands (ZNS). The zones to operate on can be specified using the offset and length options. The device argument must be the pathname of the target zoned block device. COMMANDS report The command zbd report is used to obtain and display the device zone information. By default, the command will report all zones from the start of the device up to the last zone of the device. Options may be used to modify this behavior, changing the starting zone or the size of the report. ... zbd Tool Examples The following examples use a null zoned block device with 4 conventional zones and 12 sequential zones of 32 MB created using the nullblk-zoned.sh script. # nullblk-zoned.sh 4096 32 4 12 ### Created /dev/nullb0 The following command can be used to list the zone information for all zones of a device, including the device information such as logical block size and capacity. # zbd report -i /dev/nullb0 Device /dev/nullb0: Vendor ID: Unknown Zone model: host-managed Capacity: 0.537 GB (1048576 512-bytes sectors) Logical blocks: 131072 blocks of 4096 B Physical blocks: 131072 blocks of 4096 B Zones: 16 zones of 32.0 MB Maximum number of open zones: no limit Maximum number of active zones: no limit Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000134217728, em, non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, em, non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 The same zone information can also be obtained in csv format to facilitate parsing using scripting languages, including shell scripts. # zbd report -csv /dev/nullb0 zone num, type, ofst, len, cap, wp, cond, non_seq, reset 00000, 1, 00000000000000, 00000033554432, 00000033554432, 00000033554432, 0x0, 0, 0 00001, 1, 00000033554432, 00000033554432, 00000033554432, 00000067108864, 0x0, 0, 0 00002, 1, 00000067108864, 00000033554432, 00000033554432, 00000100663296, 0x0, 0, 0 00003, 1, 00000100663296, 00000033554432, 00000033554432, 00000134217728, 0x0, 0, 0 00004, 2, 00000134217728, 00000033554432, 00000033554432, 00000134217728, 0x1, 0, 0 00005, 2, 00000167772160, 00000033554432, 00000033554432, 00000167772160, 0x1, 0, 0 00006, 2, 00000201326592, 00000033554432, 00000033554432, 00000201326592, 0x1, 0, 0 00007, 2, 00000234881024, 00000033554432, 00000033554432, 00000234881024, 0x1, 0, 0 00008, 2, 00000268435456, 00000033554432, 00000033554432, 00000268435456, 0x1, 0, 0 00009, 2, 00000301989888, 00000033554432, 00000033554432, 00000301989888, 0x1, 0, 0 00010, 2, 00000335544320, 00000033554432, 00000033554432, 00000335544320, 0x1, 0, 0 00011, 2, 00000369098752, 00000033554432, 00000033554432, 00000369098752, 0x1, 0, 0 00012, 2, 00000402653184, 00000033554432, 00000033554432, 00000402653184, 0x1, 0, 0 00013, 2, 00000436207616, 00000033554432, 00000033554432, 00000436207616, 0x1, 0, 0 00014, 2, 00000469762048, 00000033554432, 00000033554432, 00000469762048, 0x1, 0, 0 00015, 2, 00000503316480, 00000033554432, 00000033554432, 00000503316480, 0x1, 0, 0 Zone Operations The zbd tool also allow executing zone management operations over a range of zones. The following example explicitely opens the first 2 sequential zones of the null_blk device. # zbd open -ofst 134217728 -len 67108864 /dev/nullb0 # zbd report /dev/nullb0 Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000134217728, oe , non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, oe , non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 Writing 32MB to the first zone using dd will transition the zone to full state. # dd if=/dev/zero of=/dev/nullb0 oflag=direct bs=1M count=32 seek=128 32+0 records in 32+0 records out 33554432 bytes (34 MB, 32 MiB) copied, 0.00945045 s, 3.6 GB/s # zbd report /dev/nullb0 Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000167772160, fu , non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, oe, non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 Other possible zone operations are close , reset and finish . Graphical Interface gzbd provides a graphical user interface showing the zone configuration and state of a zoned block device. gzbd also displays the write status (write pointer position) of zones graphically using color coding (red for written sectors and green for unwritten sectors). Operations on zones can also be executed directly from the interface (reset zone write pointer, open zone, close zone, etc). gzbd screenshot The gzbd-viewer graphical interface is a simpler tool than gzbd that only allows displaying the current zone condition and state of a zoned block device. The zone state is refreshed by defaul twice per second. This period can be adjusted using a command line option. gzbd-viewer screenshot Using gzbd enables simple visual cues as to how an application is performaing and using the zones of a zoned block device. The following example illustrates this. *Application execution observed with gzbd-viewer*","title":"libzbd User Library"},{"location":"projects/libzbd/#libzbd-user-library","text":"libzbd is a user library providing functions for manipulating zoned block devices. Unlike the libzbc library, libzbd does not implement direct command access to zoned block devices. Rather, libzbd uses the kernel provided zoned block device interface based on the ioctl() system call. A direct consequence of this is that libzbd will only allow access to zoned block devices supported by the kernel running. This includes both physical devices such as hard-disks supporting the ZBC and ZAC standards, as well as all logical block devices implemented by various device drivers such as null_blk and device mapper drivers. The libzbd project is hosted on GitHub . The project README file provides information on how to compile and install libzbd library and its tools.","title":"libzbd User Library"},{"location":"projects/libzbd/#overview","text":"libzbd provides functions for discovering and managing the state of zones of zoned block devices. Read and write accesses to the devices can be done using standard standard I/O system calls. The execution of libzbd functions as well as of write operations to zones of a device may result in changes to the condition or attributes of the device zones (such as write pointer location in sequential zones). These changes are not internally tracked by libzbd . In other words, libzbd is stateless. It is the responsibility of applications to implement tracking of the changes to zones conditions such as the increasing write pointer position of a sequential zone after the completion of a write request to the zone.","title":"Overview"},{"location":"projects/libzbd/#library-functions","text":"All libzbd functions use bytes unit for zones information such as the zone start position on the device, a zone size or the zone write pointer location. Zoned block devices are identified using regular file descriptor numbers that can be used as is with standard I/O system calls. However, application programmers must be careful to always implement read accesses aligned to the device logical block size. Furthermore, on host managed zoned block devices, write operations to sequential zones must be aligned to the device physical block size. The main functions provided by libzbd are as follows. Function Description zbd_open() Open a zoned block device zbd_close() Close an open zoned block device zbd_get_info() Get a device information zbd_report_nr_zones() Get the number of zones of a device zbd_report_zones() zbd_list_zones() Get a device zone information zbd_zones_operation() Execute an operation on a range of zones zbd_open_zones() Explicitly open a range of zone zbd_close_zones() Close a range of zones zbd_reset_zones() Reset the write pointer of a range of zones zbd_finish_zones() Finish a range of zone More detailed information about these functions usage and behavior can be found in the comments of libzbd header file . This header file is by default installed as /usr/include/libzbd/zbd.h . libzbd does not implement any mutual exclusion mechanism for multi-thread or multi-process applications. This implies that it is the responsibility of applications to synchronize the execution of conflicting operations targeting the same zone. A typical example of such case is concurrent write operations to the same zone by multiple threads which may result in write errors without write ordering control by the application. The following functions are also provided by libzbd to facilitate application development and tests. Function Description zbd_device_is_zoned() Test if a device is a zoned block device zbd_device_model_str() Get a string description of a device model zbd_zone_type_str() Get a string description of a zone type zbd_zone_cond_str() Get a string description of a zone condition zbd_set_log_level() Set the library verbosity level All functions will behave in the same manner regardless of the type of disk being used.","title":"Library Functions"},{"location":"projects/libzbd/#utilities","text":"libzbd also provides several command line applications to manipulate zoned block devices by calling the library functions. The list of applications provided is shown in the table below. Tool Description zbd Command line utility to report, open, close, reset and finish zones of a device gzbd Similar to the zbd tool but using a graphical user interface gzbd-viewer Graphical user interface showing the condition and state of zones of a zoned block device All utilities output a help message when executed without any argument. # zbd Usage: zbd <command> [options] <dev> Command: report : Get zone information reset : Reset zone(s) open : Explicitly open zone(s) close : Close zone(s) finish : Finish zone(s) Common options: -v : Verbose mode (for debug) -i : Display device information -ofst <ofst (B)> : Start offset of the first zone of the target range (default: 0) -len <len (B)> : Size of the zone range to operate on (default: device capacity) -u <unit (B)> : Size unit for the ofst and len options and for displaying zone report results. (default: 1) Report command options: -csv : Use csv output format -n : Only output the number of zones in the report -ro <opt> : Specify zone report filter. * \"em\": empty zones * \"oi\": implicitly open zones * \"oe\": explicitly open zones * \"cl\": closed zones * \"fu\": full zones * \"ro\": read-only zones * \"ol\": offline zones * \"nw\": conventional zones * \"ns\": non-seq write resource zones * \"rw\": reset-wp recommended zones Manual pages are also provided for each tool. # man zbd ZBD(8) System Manager's Manual ZBD(8) NAME zbd - manage zoned block devices SYNOPSIS zbd command [options] device DESCRIPTION zbd is used to manipulate zones of a zoned block device. Zoned block devies are block devices that support the SCSI Zoned Block Commands (ZBC), ATA Zoned-device ATA Commands (ZAC) or NVMe Zoned NameSpace commands (ZNS). The zones to operate on can be specified using the offset and length options. The device argument must be the pathname of the target zoned block device. COMMANDS report The command zbd report is used to obtain and display the device zone information. By default, the command will report all zones from the start of the device up to the last zone of the device. Options may be used to modify this behavior, changing the starting zone or the size of the report. ...","title":"Utilities"},{"location":"projects/libzbd/#zbd-tool-examples","text":"The following examples use a null zoned block device with 4 conventional zones and 12 sequential zones of 32 MB created using the nullblk-zoned.sh script. # nullblk-zoned.sh 4096 32 4 12 ### Created /dev/nullb0 The following command can be used to list the zone information for all zones of a device, including the device information such as logical block size and capacity. # zbd report -i /dev/nullb0 Device /dev/nullb0: Vendor ID: Unknown Zone model: host-managed Capacity: 0.537 GB (1048576 512-bytes sectors) Logical blocks: 131072 blocks of 4096 B Physical blocks: 131072 blocks of 4096 B Zones: 16 zones of 32.0 MB Maximum number of open zones: no limit Maximum number of active zones: no limit Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000134217728, em, non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, em, non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 The same zone information can also be obtained in csv format to facilitate parsing using scripting languages, including shell scripts. # zbd report -csv /dev/nullb0 zone num, type, ofst, len, cap, wp, cond, non_seq, reset 00000, 1, 00000000000000, 00000033554432, 00000033554432, 00000033554432, 0x0, 0, 0 00001, 1, 00000033554432, 00000033554432, 00000033554432, 00000067108864, 0x0, 0, 0 00002, 1, 00000067108864, 00000033554432, 00000033554432, 00000100663296, 0x0, 0, 0 00003, 1, 00000100663296, 00000033554432, 00000033554432, 00000134217728, 0x0, 0, 0 00004, 2, 00000134217728, 00000033554432, 00000033554432, 00000134217728, 0x1, 0, 0 00005, 2, 00000167772160, 00000033554432, 00000033554432, 00000167772160, 0x1, 0, 0 00006, 2, 00000201326592, 00000033554432, 00000033554432, 00000201326592, 0x1, 0, 0 00007, 2, 00000234881024, 00000033554432, 00000033554432, 00000234881024, 0x1, 0, 0 00008, 2, 00000268435456, 00000033554432, 00000033554432, 00000268435456, 0x1, 0, 0 00009, 2, 00000301989888, 00000033554432, 00000033554432, 00000301989888, 0x1, 0, 0 00010, 2, 00000335544320, 00000033554432, 00000033554432, 00000335544320, 0x1, 0, 0 00011, 2, 00000369098752, 00000033554432, 00000033554432, 00000369098752, 0x1, 0, 0 00012, 2, 00000402653184, 00000033554432, 00000033554432, 00000402653184, 0x1, 0, 0 00013, 2, 00000436207616, 00000033554432, 00000033554432, 00000436207616, 0x1, 0, 0 00014, 2, 00000469762048, 00000033554432, 00000033554432, 00000469762048, 0x1, 0, 0 00015, 2, 00000503316480, 00000033554432, 00000033554432, 00000503316480, 0x1, 0, 0","title":"zbd Tool Examples"},{"location":"projects/libzbd/#zone-operations","text":"The zbd tool also allow executing zone management operations over a range of zones. The following example explicitely opens the first 2 sequential zones of the null_blk device. # zbd open -ofst 134217728 -len 67108864 /dev/nullb0 # zbd report /dev/nullb0 Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000134217728, oe , non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, oe , non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 Writing 32MB to the first zone using dd will transition the zone to full state. # dd if=/dev/zero of=/dev/nullb0 oflag=direct bs=1M count=32 seek=128 32+0 records in 32+0 records out 33554432 bytes (34 MB, 32 MiB) copied, 0.00945045 s, 3.6 GB/s # zbd report /dev/nullb0 Zone 00000: cnv, ofst 00000000000000, len 00000033554432, cap 00000033554432 Zone 00001: cnv, ofst 00000033554432, len 00000033554432, cap 00000033554432 Zone 00002: cnv, ofst 00000067108864, len 00000033554432, cap 00000033554432 Zone 00003: cnv, ofst 00000100663296, len 00000033554432, cap 00000033554432 Zone 00004: swr, ofst 00000134217728, len 00000033554432, cap 00000033554432, wp 00000167772160, fu , non_seq 0, reset 0 Zone 00005: swr, ofst 00000167772160, len 00000033554432, cap 00000033554432, wp 00000167772160, oe, non_seq 0, reset 0 Zone 00006: swr, ofst 00000201326592, len 00000033554432, cap 00000033554432, wp 00000201326592, em, non_seq 0, reset 0 Zone 00007: swr, ofst 00000234881024, len 00000033554432, cap 00000033554432, wp 00000234881024, em, non_seq 0, reset 0 Zone 00008: swr, ofst 00000268435456, len 00000033554432, cap 00000033554432, wp 00000268435456, em, non_seq 0, reset 0 Zone 00009: swr, ofst 00000301989888, len 00000033554432, cap 00000033554432, wp 00000301989888, em, non_seq 0, reset 0 Zone 00010: swr, ofst 00000335544320, len 00000033554432, cap 00000033554432, wp 00000335544320, em, non_seq 0, reset 0 Zone 00011: swr, ofst 00000369098752, len 00000033554432, cap 00000033554432, wp 00000369098752, em, non_seq 0, reset 0 Zone 00012: swr, ofst 00000402653184, len 00000033554432, cap 00000033554432, wp 00000402653184, em, non_seq 0, reset 0 Zone 00013: swr, ofst 00000436207616, len 00000033554432, cap 00000033554432, wp 00000436207616, em, non_seq 0, reset 0 Zone 00014: swr, ofst 00000469762048, len 00000033554432, cap 00000033554432, wp 00000469762048, em, non_seq 0, reset 0 Zone 00015: swr, ofst 00000503316480, len 00000033554432, cap 00000033554432, wp 00000503316480, em, non_seq 0, reset 0 Other possible zone operations are close , reset and finish .","title":"Zone Operations"},{"location":"projects/libzbd/#graphical-interface","text":"gzbd provides a graphical user interface showing the zone configuration and state of a zoned block device. gzbd also displays the write status (write pointer position) of zones graphically using color coding (red for written sectors and green for unwritten sectors). Operations on zones can also be executed directly from the interface (reset zone write pointer, open zone, close zone, etc). gzbd screenshot The gzbd-viewer graphical interface is a simpler tool than gzbd that only allows displaying the current zone condition and state of a zoned block device. The zone state is refreshed by defaul twice per second. This period can be adjusted using a command line option. gzbd-viewer screenshot Using gzbd enables simple visual cues as to how an application is performaing and using the zones of a zoned block device. The following example illustrates this. *Application execution observed with gzbd-viewer*","title":"Graphical Interface"},{"location":"projects/qemu/","text":"QEMU and KVM QEMU is a generic machine emulator and virtualizer. QEMU also provides the userspace components of the widely used KVM (Kernel-based Virtual Machine) . QEMU and zoned block devices Host managed SMR disks can be directly attached to a QEMU guest for running applications in a virtual machine environment. This is especially useful in the case of software and kernel development and tests. There are two supported methods to attach Host managed SMR zoned disks to a QEMU guest: virtio-scsi and vhost-scsi . Furthermore, QEMU also allows emulating NVMe devices implementing zoned namespaces. QEMU virtio-scsi This is the simplest method to attach a zoned block device for access from a QEMU guest. To do so, the QEMU option virtio-scsi-pci is used after defining a virtual PCI bus and SCSI host. For instance, the following command will run QEMU with # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device virtio-scsi-pci,bus=pcie.0,id=scsi0 \\ -drive file= /dev/sdf ,format=raw,if=none,id=zbc0 \\ -device scsi-block,bus=scsi0.0,drive=zbc0 The first line -device pcie-root-port,bus=pcie.0,id=rp1 creates a PCIe root complex. The second line -device virtio-scsi-pci,bus=pcie.0,id=scsi0 defines a virtio adapter connected to the PCIe bus previously defined. Finally, the last 2 lines -drive file=/dev/sdf,format=raw,if=none,id=zbc0 and -device scsi-block,bus=scsi0.0,drive=zbc0 define the device connected to the virtio SCSI adapter. In this example, since -device scsi-block is used, the host device to attach is specified using the device block device file ( /dev/sdf ) in this example. See here for a detailed description of these options. Though this method allows attaching a zoned block disk to a QEMU guest, SCSI command sense data is not processed correctly in QEMU versions prior to version 4.1. This causes the guest operating system to hang if the guest attempts to access a command sense data (for instance upon command failures). This attachment method should thus be avoided with versions of QEMU older than version 4.1. To avoid the sense data problem with QEMU versions preceding version 4.1, a zoned disk can be attached to a guest using the device SG node file as a specifier. In this case, the option -device scsi-generic must be used. The command line is changed as follows. # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device virtio-scsi-pci,bus=pcie.0,id=scsi0 \\ -drive file= /dev/sg5 ,format=raw,if=none,id=zbc0 \\ -device scsi-generic,bus=scsi0.0,drive=zbc0 On the host, the correspondance between a block device file and its SG node file can be discovered using the lsscsi command. # lsscsi -g ... [10:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdf /dev/sg5 ... Once the guest operating system is started, attachment of the host device can be checked with any of the methods shown here . For instance, the output of the lsscsi command will be as follows with the above example setup. # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdb /dev/sg1 QEMU vhost-scsi This attachment method uses a fabric module in the host kernel to provide KVM guests with a fast virtio-based connection to SCSI LUNs. This method cannot be used without QEMU KVM acceleration. Enabling the host vhost target module The host kernel configuration must have the CONFIG_VHOST_SCSI option enabled. This option is found in the top level Virtualization menu of the kernel configuration. vhost-scsi support option with make menuconfig To allow attaching physical disks as well as tcmu-runner emulated ZBC disks, the kernel configuration option COFNGI_TCM_PSCSI should also be enbaled. This option can be found in the menu Device Drivers -> Generic Target Core Mod (TCM) and ConfigFS Infrastructure . pSCSI TCM support option with make menuconfig Attaching a host physical disk To attach to a virtual machine guest a zoned device, a virtual TCM SAS adapter must first be prepared using the targetcli tool. The device to attach must be specified using a block device file. The example below illustrates this operation for the disk /dev/sdf . # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd backstores/pscsi /backstores/pscsi> create name=disk1 dev=/dev/sdf Note: block backstore recommended for SCSI block devices Created pscsi storage object disk1 using /dev/sdf /backstores/pscsi> cd /vhost /vhost> create Created target naa.5001405a160fe2e1 . Created TPG 1. /vhost/naa.5001405a160fe2e1> cd /vhost/naa.5001405a160fe2e1/tpg1/luns /vhost/naa.50...2e1/tpg1/luns> create /backstores/pscsi/disk1 Created LUN 0. /vhost/naa.50...2e1/tpg1/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 1] | | o- disk1 ............................................ [/dev/sdf activated] | | o- alua ............................................... [ALUA Groups: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:rbd ........................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 0] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 0] o- vhost ........................................................ [Targets: 1] o- naa.5001405a160fe2e1 .......................................... [TPGs: 1] o- tpg1 .............................. [naa.500140565cd16730, no-gen-acls] o- acls ...................................................... [ACLs: 0] o- luns ...................................................... [LUNs: 1] o- lun0 .............................. [pscsi/disk1 (/dev/sdf) (None)] /> exit The World-Wide port name assigned by targetcli can then be used to specify the device to attach on QEMU command line. # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device vhost-scsi-pci,wwpn= naa.5001405a160fe2e1 ,bus=pcie.0 The attached disk can then be seen from the guest OS using (for instance) the lsscsi command. # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdb /dev/sg1 Attaching an emulated ZBC disk tcmu-runner can be used to create emulated ZBC host managed SCSI disks. The emulated disk created can be used either locally on the host using the loopback fabric adapter, as explained here . Similarly to a physical device (previous section), the emulated ZBC disk can also be attached to a vhost virtual adapter for use within a KVM guest operating system. The following example illustrates this procedure, creating a small 20GB host managed SCSI disk with 256 MB zones including 10 conventional zones. # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /vhost /vhost> create Created target naa.5001405a0776dce3 . Created TPG 1. /vhost> /vhost/naa.5001405a0776dce3/tpg1/luns create /backstores/user:zbc/zbc0 Created LUN 0. /vhost> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:rbd ........................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 0] o- vhost ........................................................ [Targets: 1] o- naa.5001405a0776dce3 .......................................... [TPGs: 1] o- tpg1 .............................. [naa.500140533e375d94, no-gen-acls] o- acls ...................................................... [ACLs: 0] o- luns ...................................................... [LUNs: 1] o- lun0 ............................... [user/zbc0 (default_tg_pt_gp)] /> exit The virtual machine can then be started with the emulated ZBC disk attached using the World-Wide port name assigned by targetcli . # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device vhost-scsi-pci,wwpn= naa.5001405a0776dce3 ,bus=pcie.0 The disk is listed on the guest with tools such as lsscsi . # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sdb /dev/sg1 QEMU NVMe ZNS Device emulation This article describes in details, with examples, how QEMU can be configured to create an emulated NVMe ZNS namespace visible by the guest operating system.","title":"QEMU and KVM"},{"location":"projects/qemu/#qemu-and-kvm","text":"QEMU is a generic machine emulator and virtualizer. QEMU also provides the userspace components of the widely used KVM (Kernel-based Virtual Machine) .","title":"QEMU and KVM"},{"location":"projects/qemu/#qemu-and-zoned-block-devices","text":"Host managed SMR disks can be directly attached to a QEMU guest for running applications in a virtual machine environment. This is especially useful in the case of software and kernel development and tests. There are two supported methods to attach Host managed SMR zoned disks to a QEMU guest: virtio-scsi and vhost-scsi . Furthermore, QEMU also allows emulating NVMe devices implementing zoned namespaces.","title":"QEMU and zoned block devices"},{"location":"projects/qemu/#qemu-virtio-scsi","text":"This is the simplest method to attach a zoned block device for access from a QEMU guest. To do so, the QEMU option virtio-scsi-pci is used after defining a virtual PCI bus and SCSI host. For instance, the following command will run QEMU with # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device virtio-scsi-pci,bus=pcie.0,id=scsi0 \\ -drive file= /dev/sdf ,format=raw,if=none,id=zbc0 \\ -device scsi-block,bus=scsi0.0,drive=zbc0 The first line -device pcie-root-port,bus=pcie.0,id=rp1 creates a PCIe root complex. The second line -device virtio-scsi-pci,bus=pcie.0,id=scsi0 defines a virtio adapter connected to the PCIe bus previously defined. Finally, the last 2 lines -drive file=/dev/sdf,format=raw,if=none,id=zbc0 and -device scsi-block,bus=scsi0.0,drive=zbc0 define the device connected to the virtio SCSI adapter. In this example, since -device scsi-block is used, the host device to attach is specified using the device block device file ( /dev/sdf ) in this example. See here for a detailed description of these options. Though this method allows attaching a zoned block disk to a QEMU guest, SCSI command sense data is not processed correctly in QEMU versions prior to version 4.1. This causes the guest operating system to hang if the guest attempts to access a command sense data (for instance upon command failures). This attachment method should thus be avoided with versions of QEMU older than version 4.1. To avoid the sense data problem with QEMU versions preceding version 4.1, a zoned disk can be attached to a guest using the device SG node file as a specifier. In this case, the option -device scsi-generic must be used. The command line is changed as follows. # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device virtio-scsi-pci,bus=pcie.0,id=scsi0 \\ -drive file= /dev/sg5 ,format=raw,if=none,id=zbc0 \\ -device scsi-generic,bus=scsi0.0,drive=zbc0 On the host, the correspondance between a block device file and its SG node file can be discovered using the lsscsi command. # lsscsi -g ... [10:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdf /dev/sg5 ... Once the guest operating system is started, attachment of the host device can be checked with any of the methods shown here . For instance, the output of the lsscsi command will be as follows with the above example setup. # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdb /dev/sg1","title":"QEMU virtio-scsi"},{"location":"projects/qemu/#qemu-vhost-scsi","text":"This attachment method uses a fabric module in the host kernel to provide KVM guests with a fast virtio-based connection to SCSI LUNs. This method cannot be used without QEMU KVM acceleration.","title":"QEMU vhost-scsi"},{"location":"projects/qemu/#enabling-the-host-vhost-target-module","text":"The host kernel configuration must have the CONFIG_VHOST_SCSI option enabled. This option is found in the top level Virtualization menu of the kernel configuration. vhost-scsi support option with make menuconfig To allow attaching physical disks as well as tcmu-runner emulated ZBC disks, the kernel configuration option COFNGI_TCM_PSCSI should also be enbaled. This option can be found in the menu Device Drivers -> Generic Target Core Mod (TCM) and ConfigFS Infrastructure . pSCSI TCM support option with make menuconfig","title":"Enabling the host vhost target module"},{"location":"projects/qemu/#attaching-a-host-physical-disk","text":"To attach to a virtual machine guest a zoned device, a virtual TCM SAS adapter must first be prepared using the targetcli tool. The device to attach must be specified using a block device file. The example below illustrates this operation for the disk /dev/sdf . # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd backstores/pscsi /backstores/pscsi> create name=disk1 dev=/dev/sdf Note: block backstore recommended for SCSI block devices Created pscsi storage object disk1 using /dev/sdf /backstores/pscsi> cd /vhost /vhost> create Created target naa.5001405a160fe2e1 . Created TPG 1. /vhost/naa.5001405a160fe2e1> cd /vhost/naa.5001405a160fe2e1/tpg1/luns /vhost/naa.50...2e1/tpg1/luns> create /backstores/pscsi/disk1 Created LUN 0. /vhost/naa.50...2e1/tpg1/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 1] | | o- disk1 ............................................ [/dev/sdf activated] | | o- alua ............................................... [ALUA Groups: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:rbd ........................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 0] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 0] o- vhost ........................................................ [Targets: 1] o- naa.5001405a160fe2e1 .......................................... [TPGs: 1] o- tpg1 .............................. [naa.500140565cd16730, no-gen-acls] o- acls ...................................................... [ACLs: 0] o- luns ...................................................... [LUNs: 1] o- lun0 .............................. [pscsi/disk1 (/dev/sdf) (None)] /> exit The World-Wide port name assigned by targetcli can then be used to specify the device to attach on QEMU command line. # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device vhost-scsi-pci,wwpn= naa.5001405a160fe2e1 ,bus=pcie.0 The attached disk can then be seen from the guest OS using (for instance) the lsscsi command. # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc HGST HSH721415AL42M0 a250 /dev/sdb /dev/sg1","title":"Attaching a host physical disk"},{"location":"projects/qemu/#attaching-an-emulated-zbc-disk","text":"tcmu-runner can be used to create emulated ZBC host managed SCSI disks. The emulated disk created can be used either locally on the host using the loopback fabric adapter, as explained here . Similarly to a physical device (previous section), the emulated ZBC disk can also be attached to a vhost virtual adapter for use within a KVM guest operating system. The following example illustrates this procedure, creating a small 20GB host managed SCSI disk with 256 MB zones including 10 conventional zones. # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /vhost /vhost> create Created target naa.5001405a0776dce3 . Created TPG 1. /vhost> /vhost/naa.5001405a0776dce3/tpg1/luns create /backstores/user:zbc/zbc0 Created LUN 0. /vhost> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:rbd ........................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 0] o- vhost ........................................................ [Targets: 1] o- naa.5001405a0776dce3 .......................................... [TPGs: 1] o- tpg1 .............................. [naa.500140533e375d94, no-gen-acls] o- acls ...................................................... [ACLs: 0] o- luns ...................................................... [LUNs: 1] o- lun0 ............................... [user/zbc0 (default_tg_pt_gp)] /> exit The virtual machine can then be started with the emulated ZBC disk attached using the World-Wide port name assigned by targetcli . # qemu-kvm (your options) \\ -device pcie-root-port,bus=pcie.0,id=rp1 \\ -device vhost-scsi-pci,wwpn= naa.5001405a0776dce3 ,bus=pcie.0 The disk is listed on the guest with tools such as lsscsi . # lsscsi -g [0:0:0:0] disk ATA QEMU HARDDISK 2.5+ /dev/sda /dev/sg0 [6:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sdb /dev/sg1","title":"Attaching an emulated ZBC disk"},{"location":"projects/qemu/#qemu-nvme-zns-device-emulation","text":"This article describes in details, with examples, how QEMU can be configured to create an emulated NVMe ZNS namespace visible by the guest operating system.","title":"QEMU NVMe ZNS Device emulation"},{"location":"projects/sg3utils/","text":"SCSI Generic Utilities Various open source projects provide support for directly manipulating SCSI devices. The lsscsi command line tool and the sg3_utils library and utilities collection are among the most widely used and they are available as pre-compiled packages with most Linux\u00ae distributions. lsscsi has the capability to indicate if a device is a ZBC host managed zoned block device, and the sg3_utils collection gained ZBC support with version 1.42. lsscsi The lsscsi command lists information about the SCSI devices connected to a Linux system. lsscsi is generally available as a package with most Linux distributions. For instance, on Fedora\u00ae Linux, lssci can be installed using the following command. # dnf install lsscsi The name of the package may differ between distributions. Please refer to the distribution documentation to find out the correct package name. Identifying Host Managed Disks Executing lsscsi will list the disks that are managed using the kernel SCSI subsystem. This always includes SATA disks directly connected to a SATA port on the system mainboard or to a SATA PCIe adapter. # lsscsi [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb [10:0:1:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd The second column of the default output indicates the device type. For host managed disks, the type name is zbc . For regular disks, it is disk . Older versions of lsscsi may directly list the numerical value of the device type. In the case of host managed disks, the value displayed is 0x14 . Adding the option -g will output the SCSI Generic node file path associated with a device. This is useful when using libzbc or any of the sg3_utils command line tools. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb /dev/sg1 [10:0:1:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3 Disks Interface and Transport The third column of the output is the disk vendor ID. For ATA disks, this is always ATA even for ATA disks connected to a SAS host-bus-adapter (HBA). The transport used to communicate with the disk can be more precisely discovered using the -t option. # lsscsi -t [2:0:0:0] disk sata:55cd2e4000111f9b /dev/sda [4:0:0:0] zbc sata:5000cca25bc03731 /dev/sdb [10:0:1:0] zbc sas:0x5000cca0000025c5 /dev/sdc [10:0:3:0] zbc sas:0x300062b200f35d43 /dev/sdd sg3_utils The sg3_utils project provides a library and a collection of command line tools that directly send SCSI commands to a SCSI device using the kernel SCSI generic driver. The SCSI generic driver ( sg driver) is generally enabled by default on most distributions. The following command allows checking if the sg driver module is already loaded. # cat /proc/modules | grep sg If this command output is empty, the sg driver should be loaded. # modprobe sg These commands will work only if the sg driver was compiled as a loadable kernel module. In case of error, to verify if the sg driver was instead compiled as part of the kernel, the following command can be used. # modprobe sg modinfo: ERROR: Module sg not found. # cat /lib/modules/`uname -r`/modules.builtin | grep sg kernel/drivers/scsi/sg.ko Since all disks in Linux are exposed as SCSI devices, including all ATA drives, these utilities can be used to manage both SCSI ZBC disks and SATA ZAC disks. For SATA disks connected to SATA ports (e.g. an AHCI adapter), the kernel SCSI subsystem translates SCSI commands to ATA commands. sg3_utils includes three command line tools that are specific to ZBC disks. Utility Name Main SCSI Command Invoked Description sg_rep_zones REPORT ZONES Get a ZBC disk zone information sg_reset_wp RESET WRITE POINTER Reset one or all zones of a ZBC disk sg_zone CLOSE ZONE, FINISH ZONE, OPEN ZONE Sends one of these commands to the given ZBC device Caution The help output of the commands below uses the term LBA. In this context, the term LBA refers to a 512 bytes sector size regardless of the logical and physical block size of the disk. sg_rep_zone Executing the command with the --help option gives a simple usage explanation. # sg_rep_zones --help Usage: sg_rep_zones [--help] [--hex] [--maxlen=LEN] [--partial] [--raw] [--readonly] [--report=OPT] [--start=LBA] [--verbose] [--version] DEVICE where: --help|-h print out usage message --hex|-H output response in hexadecimal; used twice shows decoded values in hex --maxlen=LEN|-m LEN max response length (allocation length in cdb) (def: 0 -> 8192 bytes) --partial|-p sets PARTIAL bit in cdb --raw|-r output response in binary --readonly|-R open DEVICE read-only (def: read-write) --report=OPT|-o OP reporting options (def: 0: all zones) --start=LBA|-s LBA report zones from the LBA (def: 0) need not be a zone starting LBA --verbose|-v increase verbosity --version|-V print version string and exit Performs a SCSI REPORT ZONES command. Below is an example of the sg_rep_zone utility output. # sg_rep_zone /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x0 Write pointer LBA: 0xffffffffffff Zone descriptor: 1 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x10000 Write pointer LBA: 0xffffffffffff Zone descriptor: 2 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20000 Write pointer LBA: 0xffffffffffff ... Note The block device file path or the device SCSI Generic node file path can both be used to specify a disk. It is possible to start a zone report at a specific zone by using the --start option. For instance, to obtain the zone information starting at the first sequential zone of the disk (LBA 34340864), the following command can be used. # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 Zone descriptor: 1 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20d0000 Write pointer LBA: 0x20d0000 ... sg_reset_wp The command usage is as follows. # sg_reset_wp --help Usage: sg_reset_wp [--all] [--help] [--verbose] [--version] [--zone=ID] DEVICE where: --all|-a sets the ALL flag in the cdb --help|-h print out usage message --verbose|-v increase verbosity --version|-V print version string and exit --zone=ID|-z ID ID is the starting LBA of the zone whose write pointer is to be reset Performs a SCSI RESET WRITE POINTER command. ID is decimal by default, for hex use a leading '0x' or a trailing 'h'. Either the --zone=ID or --all option needs to be given. Resetting all sequential write zones of the disk can be done using the --all option. # sg_reset_wp --all /dev/sdd A single sequential zone write pointer can be reset using the --zone option. # sg_reset_wp --zone=34340864 /dev/sdd Specifying the zone ID (zone start LBA) of a conventional zone results in an error. # sg_reset_wp --zone=0 /dev/sdd Reset write pointer command: Illegal request Reseting the write pointer of an empty sequential write zone has no effect and does not result in an error. sg_zone The command usage is as follows. # sg_zone --help Usage: sg_zone [--all] [--close] [--finish] [--help] [--open] [--verbose] [--version] [--zone=ID] DEVICE where: --all|-a sets the ALL flag in the cdb --close|-c issue CLOSE ZONE command --finish|-f issue FINISH ZONE command --help|-h print out usage message --open|-o issue OPEN ZONE command --verbose|-v increase verbosity --version|-V print version string and exit --zone=ID|-z ID ID is the starting LBA of the zone *sg_zone* can perform OPEN ZONE, CLOSE ZONE or FINISH ZONE SCSI commands. ID is decimal by default. To enter a hexadecimal value, use a leading '0x' or a trailing 'h'. Either --close, --finish, or --open option needs to be given. There is no --reset option as it would duplicate the functionality provided by *sg_reset_wp* utility. The following example command sequence illustrates sg_zone and sg_reset_wp effects on condition of a zone as reported with sg_rep_zone . At first, the beginning sequential zone on the disk is explicitly open from empty condition. Then, the zone is transitioned to full condition using the zone finish command and, finally, reset again to return to empty condition. # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ... # sg_zone --open --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Explicitly opened Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ... # sg_zone --finish --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Full Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0xffffffffffff ... # sg_reset_wp --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ...","title":"SCSI Generic Utilities"},{"location":"projects/sg3utils/#scsi-generic-utilities","text":"Various open source projects provide support for directly manipulating SCSI devices. The lsscsi command line tool and the sg3_utils library and utilities collection are among the most widely used and they are available as pre-compiled packages with most Linux\u00ae distributions. lsscsi has the capability to indicate if a device is a ZBC host managed zoned block device, and the sg3_utils collection gained ZBC support with version 1.42.","title":"SCSI Generic Utilities"},{"location":"projects/sg3utils/#lsscsi","text":"The lsscsi command lists information about the SCSI devices connected to a Linux system. lsscsi is generally available as a package with most Linux distributions. For instance, on Fedora\u00ae Linux, lssci can be installed using the following command. # dnf install lsscsi The name of the package may differ between distributions. Please refer to the distribution documentation to find out the correct package name.","title":"lsscsi"},{"location":"projects/sg3utils/#identifying-host-managed-disks","text":"Executing lsscsi will list the disks that are managed using the kernel SCSI subsystem. This always includes SATA disks directly connected to a SATA port on the system mainboard or to a SATA PCIe adapter. # lsscsi [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb [10:0:1:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd The second column of the default output indicates the device type. For host managed disks, the type name is zbc . For regular disks, it is disk . Older versions of lsscsi may directly list the numerical value of the device type. In the case of host managed disks, the value displayed is 0x14 . Adding the option -g will output the SCSI Generic node file path associated with a device. This is useful when using libzbc or any of the sg3_utils command line tools. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb /dev/sg1 [10:0:1:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3","title":"Identifying Host Managed Disks"},{"location":"projects/sg3utils/#disks-interface-and-transport","text":"The third column of the output is the disk vendor ID. For ATA disks, this is always ATA even for ATA disks connected to a SAS host-bus-adapter (HBA). The transport used to communicate with the disk can be more precisely discovered using the -t option. # lsscsi -t [2:0:0:0] disk sata:55cd2e4000111f9b /dev/sda [4:0:0:0] zbc sata:5000cca25bc03731 /dev/sdb [10:0:1:0] zbc sas:0x5000cca0000025c5 /dev/sdc [10:0:3:0] zbc sas:0x300062b200f35d43 /dev/sdd","title":"Disks Interface and Transport"},{"location":"projects/sg3utils/#sg3_utils","text":"The sg3_utils project provides a library and a collection of command line tools that directly send SCSI commands to a SCSI device using the kernel SCSI generic driver. The SCSI generic driver ( sg driver) is generally enabled by default on most distributions. The following command allows checking if the sg driver module is already loaded. # cat /proc/modules | grep sg If this command output is empty, the sg driver should be loaded. # modprobe sg These commands will work only if the sg driver was compiled as a loadable kernel module. In case of error, to verify if the sg driver was instead compiled as part of the kernel, the following command can be used. # modprobe sg modinfo: ERROR: Module sg not found. # cat /lib/modules/`uname -r`/modules.builtin | grep sg kernel/drivers/scsi/sg.ko Since all disks in Linux are exposed as SCSI devices, including all ATA drives, these utilities can be used to manage both SCSI ZBC disks and SATA ZAC disks. For SATA disks connected to SATA ports (e.g. an AHCI adapter), the kernel SCSI subsystem translates SCSI commands to ATA commands. sg3_utils includes three command line tools that are specific to ZBC disks. Utility Name Main SCSI Command Invoked Description sg_rep_zones REPORT ZONES Get a ZBC disk zone information sg_reset_wp RESET WRITE POINTER Reset one or all zones of a ZBC disk sg_zone CLOSE ZONE, FINISH ZONE, OPEN ZONE Sends one of these commands to the given ZBC device Caution The help output of the commands below uses the term LBA. In this context, the term LBA refers to a 512 bytes sector size regardless of the logical and physical block size of the disk.","title":"sg3_utils"},{"location":"projects/sg3utils/#sg_rep_zone","text":"Executing the command with the --help option gives a simple usage explanation. # sg_rep_zones --help Usage: sg_rep_zones [--help] [--hex] [--maxlen=LEN] [--partial] [--raw] [--readonly] [--report=OPT] [--start=LBA] [--verbose] [--version] DEVICE where: --help|-h print out usage message --hex|-H output response in hexadecimal; used twice shows decoded values in hex --maxlen=LEN|-m LEN max response length (allocation length in cdb) (def: 0 -> 8192 bytes) --partial|-p sets PARTIAL bit in cdb --raw|-r output response in binary --readonly|-R open DEVICE read-only (def: read-write) --report=OPT|-o OP reporting options (def: 0: all zones) --start=LBA|-s LBA report zones from the LBA (def: 0) need not be a zone starting LBA --verbose|-v increase verbosity --version|-V print version string and exit Performs a SCSI REPORT ZONES command. Below is an example of the sg_rep_zone utility output. # sg_rep_zone /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x0 Write pointer LBA: 0xffffffffffff Zone descriptor: 1 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x10000 Write pointer LBA: 0xffffffffffff Zone descriptor: 2 Zone type: Conventional Zone condition: Not write pointer Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20000 Write pointer LBA: 0xffffffffffff ... Note The block device file path or the device SCSI Generic node file path can both be used to specify a disk. It is possible to start a zone report at a specific zone by using the --start option. For instance, to obtain the zone information starting at the first sequential zone of the disk (LBA 34340864), the following command can be used. # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 Zone descriptor: 1 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20d0000 Write pointer LBA: 0x20d0000 ...","title":"sg_rep_zone"},{"location":"projects/sg3utils/#sg_reset_wp","text":"The command usage is as follows. # sg_reset_wp --help Usage: sg_reset_wp [--all] [--help] [--verbose] [--version] [--zone=ID] DEVICE where: --all|-a sets the ALL flag in the cdb --help|-h print out usage message --verbose|-v increase verbosity --version|-V print version string and exit --zone=ID|-z ID ID is the starting LBA of the zone whose write pointer is to be reset Performs a SCSI RESET WRITE POINTER command. ID is decimal by default, for hex use a leading '0x' or a trailing 'h'. Either the --zone=ID or --all option needs to be given. Resetting all sequential write zones of the disk can be done using the --all option. # sg_reset_wp --all /dev/sdd A single sequential zone write pointer can be reset using the --zone option. # sg_reset_wp --zone=34340864 /dev/sdd Specifying the zone ID (zone start LBA) of a conventional zone results in an error. # sg_reset_wp --zone=0 /dev/sdd Reset write pointer command: Illegal request Reseting the write pointer of an empty sequential write zone has no effect and does not result in an error.","title":"sg_reset_wp"},{"location":"projects/sg3utils/#sg_zone","text":"The command usage is as follows. # sg_zone --help Usage: sg_zone [--all] [--close] [--finish] [--help] [--open] [--verbose] [--version] [--zone=ID] DEVICE where: --all|-a sets the ALL flag in the cdb --close|-c issue CLOSE ZONE command --finish|-f issue FINISH ZONE command --help|-h print out usage message --open|-o issue OPEN ZONE command --verbose|-v increase verbosity --version|-V print version string and exit --zone=ID|-z ID ID is the starting LBA of the zone *sg_zone* can perform OPEN ZONE, CLOSE ZONE or FINISH ZONE SCSI commands. ID is decimal by default. To enter a hexadecimal value, use a leading '0x' or a trailing 'h'. Either --close, --finish, or --open option needs to be given. There is no --reset option as it would duplicate the functionality provided by *sg_reset_wp* utility. The following example command sequence illustrates sg_zone and sg_reset_wp effects on condition of a zone as reported with sg_rep_zone . At first, the beginning sequential zone on the disk is explicitly open from empty condition. Then, the zone is transitioned to full condition using the zone finish command and, finally, reset again to return to empty condition. # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ... # sg_zone --open --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Explicitly opened Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ... # sg_zone --finish --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Full Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0xffffffffffff ... # sg_reset_wp --zone=34340864 /dev/sdd # sg_rep_zones --start=34340864 /dev/sdd Report zones response: Same=1: zone type and length same in each descriptor Maximum LBA: 0xda47ffff Zone descriptor: 0 Zone type: Sequential write required Zone condition: Empty Non_seq: 0 Reset: 0 Zone Length: 0x10000 Zone start LBA: 0x20c0000 Write pointer LBA: 0x20c0000 ...","title":"sg_zone"},{"location":"projects/tcmu-runner/","text":"tcmu-runner ZBC Disk Emulation tcmu-runner is an application daemon that can handle the execution of SCSI commands sent by the kernel SCSI target sub component, allowing exporting SCSI Logical Units (LUNs) to be backed by regular files or block devices. Overview LinuxIO (LIO\u2122) is the standard open-source SCSI target implementation in the Linux\u00ae kernel. LIO supports all prevalent storage fabrics, including Fibre Channel, FCoE, iEEE 1394, iSCSI, NVMe-OF, iSER, SRP, etc. The Target Core Module Userspace (TCMU) implements a fabric that creates a link between the kernel SCSI target infrastructure and a user space application. The kernel level module involved is target_core_user and can be viewed as a virtual HBA. tcmu-runner implements the userspace level side of the ank processing, handling the details of the TCMU interface (UIO, netlink, pthreads, and DBus). tcmu-runner exports a more simple C plugin API allowing the creation of file handlers to emulate various device types. This organization is shown in the figure below. tcmu-runner Overview The ZBC file handler implements a SCSI ZBC host aware or host managed disk emulation using TCMU C plugin API. This handler uses a regular file as the backend storage for the emulated device. With this infrastructure setup, any command issued by an application or by a kernel component (e.g. a file system) will be sent to the tcmu-runner daemon through the TCMU kernel driver. The file handler can process the command in user space using regular POSIX system calls and a reply sent back on completion of the command processing. From the point of view of the application or kernel component using the emulated disk, all accesses appear as executing on actual hardware. Compilation and Installation The tcmu-runner project is hosted on GitHub . The project README file provides detailed information on how to compile, install and execute tcmu-runner daemon. The control of tcmu-runner emulated devices is achieved using the targetcli utility available as a package with most distributions. For instance, on Fedora\u00ae Linux, tcmu-runner and targetcli can be installed using the following commands. # dnf install tcmu-runner # dnf install targetcli Kernel Components tcmu-runner relies on the loopback virtual SAS adapter kernel module to expose the emulated device as a regular disk to the kernel SCSI stack. Enabling this kernel module first requires that support for the Generic Target Core Mod (TCM) and ConfigFS Infrastructure be enabled from the top-level Device Drivers menu. Target Core Module support option with make menuconfig With this infrastructure enabled, the configuration option CONFIG_TCM_USER2 and CONFIG_LOOPBACK_TARGET can be enabled. TCM user and loopback adapter support option with make menuconfig ZBC File Handler tcmu-runner ZBC file handler is compiled and installed by default. This handler allows the creation of emulated ZBC disks with a regular file used as backing storage. The ZBC file handler supports the emulation of both host aware and host managed SCSI disks. Furthermore, the characteristics of the emulated device can all be configured. The following table shows the configuration parameters available. Option Description Default value model- type Device model type, HA for host aware or HM for host managed HM lba- size (B) LBA size in bytes (512 or 4096) 512 zsize- size (MiB) Zone size in MiB 256 MiB conv- num Number of conventional zones at LBA 0 (can be 0) Number of zones corresponding to 1% of the device capacity open- num Optimal (for host aware) or maximum (for host managed) number of open zones 128 These parameters are always grouped together into a configuration string with the format /[opt1[/opt2][...]@]path_to_backing_file . For instance, to specify a host managed disk with 128MB zone size, 100 conventional zones and the file /var/local/zbc0.raw as backing storage, the following configuration string can be used. cfgstring=model-HM/zsize-128/conv-100@/var/local/zbc.raw Creating an Emulated disk The following example shows how to create a small 20 GB host managed ZBC disk with 10 conventional zones and a 256 MiB zone size, with the file /var/local/zbc0.raw used as backing storage. The emulated disk will be locally emulated using the loopback interface. This require that tcmu-runner be executed on the system. With tcmu-runner running, the targetcli command is used to create the emulated disk. # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /loopback /loopback> create Created target naa.500140529100d742. /loopback> cd naa.500140529100d742/luns /loopback/naa...9100d742/luns> create /backstores/user:zbc/zbc0 0 Created LUN 0. /loopback/naa...9100d742/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:poma .......................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 1] | o- naa.500140529100d742 ............................. [naa.50014059e05d5424] | o- luns ........................................................ [LUNs: 1] | o- lun0 ................................. [user/zbc0 (default_tg_pt_gp)] o- vhost ........................................................ [Targets: 0] /> exit The backstore create command specifies the emulated disk capacity with the argument size=20G . The backing file /var/local/zbc0.raw will be created if necessary and resized to match the requested capacity. When the backstore is linked to lun0 of the loopback link, the emulated device becomes visible by the kernel and its management initialized in the same manner as with physical devices. This can be seen in the kernel messages log. # dmesg ... scsi host11: TCM_Loopback scsi 11:0:1:0: Direct-Access-ZBC LIO-ORG TCMU ZBC device 0002 PQ: 0 ANSI: 5 sd 11:0:1:0: Attached scsi generic sg4 type 20 sd 11:0:1:0: [sde] Host-managed zoned block device sd 11:0:1:0: [sde] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB) sd 11:0:1:0: [sde] 80 zones of 524288 logical blocks sd 11:0:1:0: [sde] Write Protect is off sd 11:0:1:0: [sde] Mode Sense: 0f 00 00 00 sd 11:0:1:0: [sde] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 11:0:1:0: [sde] Optimal transfer size 65536 bytes sd 11:0:1:0: [sde] Attached SCSI disk The disk can now be listed with tools such as lsblk and lsscsi . # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb /dev/sg1 [10:0:1:0] zbc ATA HGST HSH721414AL W209 /dev/sdc /dev/sg2 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdd /dev/sg3 [11:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sde /dev/sg4 Using an Emulated disk All ZBD compliant tools and applications will be able to access and control the emulated disk in exactly the same manner as a physical device. For instance, libzbc graphical interface (gzbc) can be used to display the emulated disk zones. tcmu-runner ZBC emulated disk view in gzbc Scripts The following script is useful to create an emulated disk with a single command. #!/bin/bash if [ $# != 5 ]; then echo \"Usage: $0 <disk name> <cap (GB)> HM|HA <zone size (MB)> <conv zones num>\" exit 1; fi dname=\"$1\" cap=\"$2\" model=\"$3\" zs=\"$4\" cnum=\"$5\" naa=\"naa.50014059cfa9ba75\" # Setup emulated disk cat << EOF | targetcli cd /backstores/user:zbc create name=${dname} size=${cap}G cfgstring=model-${model}/zsize-${zs}/conv-${cnum}@/var/local/${dname}.raw cd /loopback create ${naa} cd ${naa}/luns create /backstores/user:zbc/${dname} 0 cd / exit EOF sleep 1 disk=`lsscsi | grep \"TCMU ZBC device\" | cut -d '/' -f3 | sed 's/ //g'` echo \"mq-deadline\" > /sys/block/\"${disk}\"/queue/scheduler Tearing down an emulated disk can also be automated with a single command line as shown below. #!/bin/bash if [ $# != 1 ]; then echo \"Usage: $0 <disk name (e.g. zbc0)\" exit 1; fi dname=\"$1\" naa=\"naa.50014059cfa9ba75\" # Delete emulated disk cat << EOF | targetcli cd /loopback/${naa}/luns delete 0 cd /loopback delete ${naa} cd /backstores/user:zbc delete ${dname} cd / exit EOF","title":"tcmu-runner ZBC Disk Emulation"},{"location":"projects/tcmu-runner/#tcmu-runner-zbc-disk-emulation","text":"tcmu-runner is an application daemon that can handle the execution of SCSI commands sent by the kernel SCSI target sub component, allowing exporting SCSI Logical Units (LUNs) to be backed by regular files or block devices.","title":"tcmu-runner ZBC Disk Emulation"},{"location":"projects/tcmu-runner/#overview","text":"LinuxIO (LIO\u2122) is the standard open-source SCSI target implementation in the Linux\u00ae kernel. LIO supports all prevalent storage fabrics, including Fibre Channel, FCoE, iEEE 1394, iSCSI, NVMe-OF, iSER, SRP, etc. The Target Core Module Userspace (TCMU) implements a fabric that creates a link between the kernel SCSI target infrastructure and a user space application. The kernel level module involved is target_core_user and can be viewed as a virtual HBA. tcmu-runner implements the userspace level side of the ank processing, handling the details of the TCMU interface (UIO, netlink, pthreads, and DBus). tcmu-runner exports a more simple C plugin API allowing the creation of file handlers to emulate various device types. This organization is shown in the figure below. tcmu-runner Overview The ZBC file handler implements a SCSI ZBC host aware or host managed disk emulation using TCMU C plugin API. This handler uses a regular file as the backend storage for the emulated device. With this infrastructure setup, any command issued by an application or by a kernel component (e.g. a file system) will be sent to the tcmu-runner daemon through the TCMU kernel driver. The file handler can process the command in user space using regular POSIX system calls and a reply sent back on completion of the command processing. From the point of view of the application or kernel component using the emulated disk, all accesses appear as executing on actual hardware.","title":"Overview"},{"location":"projects/tcmu-runner/#compilation-and-installation","text":"The tcmu-runner project is hosted on GitHub . The project README file provides detailed information on how to compile, install and execute tcmu-runner daemon. The control of tcmu-runner emulated devices is achieved using the targetcli utility available as a package with most distributions. For instance, on Fedora\u00ae Linux, tcmu-runner and targetcli can be installed using the following commands. # dnf install tcmu-runner # dnf install targetcli","title":"Compilation and Installation"},{"location":"projects/tcmu-runner/#kernel-components","text":"tcmu-runner relies on the loopback virtual SAS adapter kernel module to expose the emulated device as a regular disk to the kernel SCSI stack. Enabling this kernel module first requires that support for the Generic Target Core Mod (TCM) and ConfigFS Infrastructure be enabled from the top-level Device Drivers menu. Target Core Module support option with make menuconfig With this infrastructure enabled, the configuration option CONFIG_TCM_USER2 and CONFIG_LOOPBACK_TARGET can be enabled. TCM user and loopback adapter support option with make menuconfig","title":"Kernel Components"},{"location":"projects/tcmu-runner/#zbc-file-handler","text":"tcmu-runner ZBC file handler is compiled and installed by default. This handler allows the creation of emulated ZBC disks with a regular file used as backing storage. The ZBC file handler supports the emulation of both host aware and host managed SCSI disks. Furthermore, the characteristics of the emulated device can all be configured. The following table shows the configuration parameters available. Option Description Default value model- type Device model type, HA for host aware or HM for host managed HM lba- size (B) LBA size in bytes (512 or 4096) 512 zsize- size (MiB) Zone size in MiB 256 MiB conv- num Number of conventional zones at LBA 0 (can be 0) Number of zones corresponding to 1% of the device capacity open- num Optimal (for host aware) or maximum (for host managed) number of open zones 128 These parameters are always grouped together into a configuration string with the format /[opt1[/opt2][...]@]path_to_backing_file . For instance, to specify a host managed disk with 128MB zone size, 100 conventional zones and the file /var/local/zbc0.raw as backing storage, the following configuration string can be used. cfgstring=model-HM/zsize-128/conv-100@/var/local/zbc.raw","title":"ZBC File Handler"},{"location":"projects/tcmu-runner/#creating-an-emulated-disk","text":"The following example shows how to create a small 20 GB host managed ZBC disk with 10 conventional zones and a 256 MiB zone size, with the file /var/local/zbc0.raw used as backing storage. The emulated disk will be locally emulated using the loopback interface. This require that tcmu-runner be executed on the system. With tcmu-runner running, the targetcli command is used to create the emulated disk. # targetcli targetcli shell version 2.1.fb49 Copyright 2011-2013 by Datera, Inc and others. For help on commands, type 'help'. /> cd /backstores/user:zbc /backstores/user:zbc> create name=zbc0 size=20G cfgstring=model-HM/zsize-256/conv-10@/var/local/zbc0.raw Created user-backed storage object zbc0 size 21474836480. /backstores/user:zbc> cd /loopback /loopback> create Created target naa.500140529100d742. /loopback> cd naa.500140529100d742/luns /loopback/naa...9100d742/luns> create /backstores/user:zbc/zbc0 0 Created LUN 0. /loopback/naa...9100d742/luns> cd / /> ls o- / ..................................................................... [...] o- backstores .......................................................... [...] | o- block .............................................. [Storage Objects: 0] | o- fileio ............................................. [Storage Objects: 0] | o- pscsi .............................................. [Storage Objects: 0] | o- ramdisk ............................................ [Storage Objects: 0] | o- user:fbo ........................................... [Storage Objects: 0] | o- user:poma .......................................... [Storage Objects: 0] | o- user:zbc ........................................... [Storage Objects: 1] | o- zbc0 [model-HM/zsize-256/conv-10@/var/local/zbc0.raw (20.0GiB) activated] | o- alua ............................................... [ALUA Groups: 1] | o- default_tg_pt_gp ................... [ALUA state: Active/optimized] o- iscsi ........................................................ [Targets: 0] o- loopback ..................................................... [Targets: 1] | o- naa.500140529100d742 ............................. [naa.50014059e05d5424] | o- luns ........................................................ [LUNs: 1] | o- lun0 ................................. [user/zbc0 (default_tg_pt_gp)] o- vhost ........................................................ [Targets: 0] /> exit The backstore create command specifies the emulated disk capacity with the argument size=20G . The backing file /var/local/zbc0.raw will be created if necessary and resized to match the requested capacity. When the backstore is linked to lun0 of the loopback link, the emulated device becomes visible by the kernel and its management initialized in the same manner as with physical devices. This can be seen in the kernel messages log. # dmesg ... scsi host11: TCM_Loopback scsi 11:0:1:0: Direct-Access-ZBC LIO-ORG TCMU ZBC device 0002 PQ: 0 ANSI: 5 sd 11:0:1:0: Attached scsi generic sg4 type 20 sd 11:0:1:0: [sde] Host-managed zoned block device sd 11:0:1:0: [sde] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB) sd 11:0:1:0: [sde] 80 zones of 524288 logical blocks sd 11:0:1:0: [sde] Write Protect is off sd 11:0:1:0: [sde] Mode Sense: 0f 00 00 00 sd 11:0:1:0: [sde] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA sd 11:0:1:0: [sde] Optimal transfer size 65536 bytes sd 11:0:1:0: [sde] Attached SCSI disk The disk can now be listed with tools such as lsblk and lsscsi . # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [4:0:0:0] zbc ATA HGST HSH721414AL T220 /dev/sdb /dev/sg1 [10:0:1:0] zbc ATA HGST HSH721414AL W209 /dev/sdc /dev/sg2 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdd /dev/sg3 [11:0:1:0] zbc LIO-ORG TCMU ZBC device 0002 /dev/sde /dev/sg4","title":"Creating an Emulated disk"},{"location":"projects/tcmu-runner/#using-an-emulated-disk","text":"All ZBD compliant tools and applications will be able to access and control the emulated disk in exactly the same manner as a physical device. For instance, libzbc graphical interface (gzbc) can be used to display the emulated disk zones. tcmu-runner ZBC emulated disk view in gzbc","title":"Using an Emulated disk"},{"location":"projects/tcmu-runner/#scripts","text":"The following script is useful to create an emulated disk with a single command. #!/bin/bash if [ $# != 5 ]; then echo \"Usage: $0 <disk name> <cap (GB)> HM|HA <zone size (MB)> <conv zones num>\" exit 1; fi dname=\"$1\" cap=\"$2\" model=\"$3\" zs=\"$4\" cnum=\"$5\" naa=\"naa.50014059cfa9ba75\" # Setup emulated disk cat << EOF | targetcli cd /backstores/user:zbc create name=${dname} size=${cap}G cfgstring=model-${model}/zsize-${zs}/conv-${cnum}@/var/local/${dname}.raw cd /loopback create ${naa} cd ${naa}/luns create /backstores/user:zbc/${dname} 0 cd / exit EOF sleep 1 disk=`lsscsi | grep \"TCMU ZBC device\" | cut -d '/' -f3 | sed 's/ //g'` echo \"mq-deadline\" > /sys/block/\"${disk}\"/queue/scheduler Tearing down an emulated disk can also be automated with a single command line as shown below. #!/bin/bash if [ $# != 1 ]; then echo \"Usage: $0 <disk name (e.g. zbc0)\" exit 1; fi dname=\"$1\" naa=\"naa.50014059cfa9ba75\" # Delete emulated disk cat << EOF | targetcli cd /loopback/${naa}/luns delete 0 cd /loopback delete ${naa} cd /backstores/user:zbc delete ${dname} cd / exit EOF","title":"Scripts"},{"location":"projects/util-linux/","text":"Linux System Utilities As defined by the project itself, util-linux is a random collection of Linux\u00ae utilities. This project is hosted on GitHub . This project generally packaged in most distributions under the name util-linux and installed by default. Among many utilities, util-linux provides the lsblk and blkzone command line tools to list zoned block devices and to obtain zone configuration. The blkzone tool also allows resetting write pointer of sequential zones. These utilities are especially useful for shell scripting and for troubleshooting of zone management problems in user applications. lsblk The lsblk command lists all block devices of a system, regardless of the block device type, that is, also including zoned block devices. The output of lsblk is as follows. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 167.7G 0 disk \u251c\u2500sda1 8:1 0 1G 0 part /boot \u251c\u2500sda2 8:2 0 150.7G 0 part / \u2514\u2500sda3 8:3 0 16G 0 part [SWAP] sdb 8:16 0 12.8T 0 disk sdc 8:32 0 12.8T 0 disk sdd 8:48 0 13.7T 0 disk By default, there is no indication of the zone model of the listed block devices. To discover this information, the option -z can be used. # lsblk -z NAME ZONED sda none \u251c\u2500sda1 none \u251c\u2500sda2 none \u2514\u2500sda3 none sdb host-managed sdc host-managed sdd host-managed The output of lsblk can also be formatted as needed using the -o option. For instance, the following command will display block device names, size and zone model. # lsblk -o NAME,SIZE,ZONED NAME SIZE ZONED sda 167.7G none \u251c\u2500sda1 1G none \u251c\u2500sda2 150.7G none \u2514\u2500sda3 16G none sdb 12.8T host-managed sdc 12.8T host-managed sdd 13.7T host-managed blkzone The blkzone command line utility allows listing (reporting) the zones of a zoned block device and resetting the write pointer of sequential zones. Unlike the sg_rep_zone and sg_reset_wp utilities of the sg3utils project, blkzone relies on the kernel provided ZBD ioctl() interface to perform zone report and zone reset operations. SCSI commands are not issued directly to the device by blkzone . blkzone command usage is as shown below. # blkzone --help Usage: blkzone <command> [options] <device> Run zone command on the given block device. Commands: report Report zone information about the given device capacity Report zone capacity for the given device reset Reset a range of zones. open Open a range of zones. close Close a range of zones. finish Set a range of zones to Full. Options: -o, --offset <sector> start sector of zone to act (in 512-byte sectors) -l, --length <sectors> maximum sectors to act (in 512-byte sectors) -c, --count <number> maximum number of zones -f, --force enforce on block devices used by the system -v, --verbose display more details -h, --help display this help -V, --version display version Arguments: <sector> and <sectors> arguments may be followed by the suffixes for GiB, TiB, PiB, EiB, ZiB, and YiB (the \"iB\" is optional) For more details see blkzone(8). Note The open, close and finish commands of blkzone are available with util-linux version 2.36 onward. The capacity command is available on the master branch. Zone Report For listing the zones of device, the following command can be used. # blkzone report /dev/sdd start: 0x000000000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000200000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000280000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000300000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000380000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000400000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000480000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010500000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010780000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010800000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... To restrict the range of zones reported, the options --offset and --count can be used. For instance, to report only the first sequential zone of a disk starting at sector 274726912, the following command can be used. # blkzone report --offset 274726912 --count 1 /dev/sdd start: 0x010600000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] Device capacity If zone capacity is smaller than zone size, the size listed in blockdev and lsblk is not indicating how much data that can be stored on on the zoned block device. The storage capacity of the device is the sum of the capacity of all zones. For determining the storage capcity of a device in sectors, the following command can be used: # blkzone capacity /dev/nullb1 0x00c350000 Zone Reset Sequential write zones can be reset with blkzone using the reset operation. For instance, to reset the first sequential zone of a disk starting at sector 274726912, the following command can be used. # blkzone reset --offset 274726912 --count 1 /dev/sdd If the range of zones specified with the reset operation includes conventional zones, the command will fail. # blkzone reset /dev/sdd blkzone: /dev/sdh: BLKRESETZONE ioctl failed: Remote I/O error The user must exclude all conventional zones. With the disk used for the above example, all conventional zones are located between sector 0 and 274726912. The remaining of the disk is composed of sequential write zones. Therefore, the following command will reset write pointer in all zones. # blkzone reset --offset 274726912 /dev/sdd Note This command results in the kernel looping over all sequential zone of the disk and executing a zone reset command on each zone. This can be time consuming and takes a significantly longer time compared to using the sg_reset_wp command with the --all option specified.","title":"Linux System Utilities"},{"location":"projects/util-linux/#linux-system-utilities","text":"As defined by the project itself, util-linux is a random collection of Linux\u00ae utilities. This project is hosted on GitHub . This project generally packaged in most distributions under the name util-linux and installed by default. Among many utilities, util-linux provides the lsblk and blkzone command line tools to list zoned block devices and to obtain zone configuration. The blkzone tool also allows resetting write pointer of sequential zones. These utilities are especially useful for shell scripting and for troubleshooting of zone management problems in user applications.","title":"Linux System Utilities"},{"location":"projects/util-linux/#lsblk","text":"The lsblk command lists all block devices of a system, regardless of the block device type, that is, also including zoned block devices. The output of lsblk is as follows. # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 167.7G 0 disk \u251c\u2500sda1 8:1 0 1G 0 part /boot \u251c\u2500sda2 8:2 0 150.7G 0 part / \u2514\u2500sda3 8:3 0 16G 0 part [SWAP] sdb 8:16 0 12.8T 0 disk sdc 8:32 0 12.8T 0 disk sdd 8:48 0 13.7T 0 disk By default, there is no indication of the zone model of the listed block devices. To discover this information, the option -z can be used. # lsblk -z NAME ZONED sda none \u251c\u2500sda1 none \u251c\u2500sda2 none \u2514\u2500sda3 none sdb host-managed sdc host-managed sdd host-managed The output of lsblk can also be formatted as needed using the -o option. For instance, the following command will display block device names, size and zone model. # lsblk -o NAME,SIZE,ZONED NAME SIZE ZONED sda 167.7G none \u251c\u2500sda1 1G none \u251c\u2500sda2 150.7G none \u2514\u2500sda3 16G none sdb 12.8T host-managed sdc 12.8T host-managed sdd 13.7T host-managed","title":"lsblk"},{"location":"projects/util-linux/#blkzone","text":"The blkzone command line utility allows listing (reporting) the zones of a zoned block device and resetting the write pointer of sequential zones. Unlike the sg_rep_zone and sg_reset_wp utilities of the sg3utils project, blkzone relies on the kernel provided ZBD ioctl() interface to perform zone report and zone reset operations. SCSI commands are not issued directly to the device by blkzone . blkzone command usage is as shown below. # blkzone --help Usage: blkzone <command> [options] <device> Run zone command on the given block device. Commands: report Report zone information about the given device capacity Report zone capacity for the given device reset Reset a range of zones. open Open a range of zones. close Close a range of zones. finish Set a range of zones to Full. Options: -o, --offset <sector> start sector of zone to act (in 512-byte sectors) -l, --length <sectors> maximum sectors to act (in 512-byte sectors) -c, --count <number> maximum number of zones -f, --force enforce on block devices used by the system -v, --verbose display more details -h, --help display this help -V, --version display version Arguments: <sector> and <sectors> arguments may be followed by the suffixes for GiB, TiB, PiB, EiB, ZiB, and YiB (the \"iB\" is optional) For more details see blkzone(8). Note The open, close and finish commands of blkzone are available with util-linux version 2.36 onward. The capacity command is available on the master branch.","title":"blkzone"},{"location":"projects/util-linux/#zone-report","text":"For listing the zones of device, the following command can be used. # blkzone report /dev/sdd start: 0x000000000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000080000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000100000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000180000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000200000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000280000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000300000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000380000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000400000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x000480000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] ... start: 0x010500000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010580000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 0(nw) [type: 1(CONVENTIONAL)] start: 0x010600000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010680000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010700000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010780000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] start: 0x010800000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)] ... To restrict the range of zones reported, the options --offset and --count can be used. For instance, to report only the first sequential zone of a disk starting at sector 274726912, the following command can be used. # blkzone report --offset 274726912 --count 1 /dev/sdd start: 0x010600000, len 0x080000, cap 0x080000, wptr 0x000000 reset:0 non-seq:0, zcond: 1(em) [type: 2(SEQ_WRITE_REQUIRED)]","title":"Zone Report"},{"location":"projects/util-linux/#device-capacity","text":"If zone capacity is smaller than zone size, the size listed in blockdev and lsblk is not indicating how much data that can be stored on on the zoned block device. The storage capacity of the device is the sum of the capacity of all zones. For determining the storage capcity of a device in sectors, the following command can be used: # blkzone capacity /dev/nullb1 0x00c350000","title":"Device capacity"},{"location":"projects/util-linux/#zone-reset","text":"Sequential write zones can be reset with blkzone using the reset operation. For instance, to reset the first sequential zone of a disk starting at sector 274726912, the following command can be used. # blkzone reset --offset 274726912 --count 1 /dev/sdd If the range of zones specified with the reset operation includes conventional zones, the command will fail. # blkzone reset /dev/sdd blkzone: /dev/sdh: BLKRESETZONE ioctl failed: Remote I/O error The user must exclude all conventional zones. With the disk used for the above example, all conventional zones are located between sector 0 and 274726912. The remaining of the disk is composed of sequential write zones. Therefore, the following command will reset write pointer in all zones. # blkzone reset --offset 274726912 /dev/sdd Note This command results in the kernel looping over all sequential zone of the disk and executing a zone reset command on each zone. This can be time consuming and takes a significantly longer time compared to using the sg_reset_wp command with the --all option specified.","title":"Zone Reset"},{"location":"projects/zns/","text":"Linux Tools for ZNS Zoned namespace support was added to the Linux kernel with version 5.9. The initial driver release requires the namespace implement the Zone Append command in order to use with the kernel's block stack. nvme-cli Open source tooling for zns is provided by nvme-cli in the current master branch starting from version 1.12 and onward. It is recommended to use the latest version, which is currently at version 1.13. The ZNS specific commands all use the zns command line prefix. You can view further available commands by checking its help: # nvme zns help nvme-1.12 usage: nvme zns <command> [<device>] [<args>] The '<device>' may be either an NVMe character device (ex: /dev/nvme0) or an nvme block device (ex: /dev/nvme0n1). Zoned Namespace Command Set The following are all implemented sub-commands: id-ctrl Retrieve ZNS controller identification id-ns Retrieve ZNS namespace identification zone-mgmt-recv Sends the zone management receive command zone-mgmt-send Sends the zone management send command report-zones Retrieve the Report Zones report close-zone Closes one or more zones finish-zone Finishes one or more zones open-zone Opens one or more zones reset-zone Resets one or more zones offline-zone Offlines one or more zones set-zone-desc Attaches zone descriptor extension data zone-append Writes data and metadata (if applicable), appended to the end of the requested zone changed-zone-list Retrieves the changed zone list log Identify ZNS Controller The Zoned Namespace Command Set specification currently defines only one field in the command set's Identify Controller: the Zone Append Size Limit (ZASL), encoding the maximum command size for a Zone Append command. The example below returns '5', which corresponds to 128k bytes for maximum append: # nvme zns id-ctrl /dev/nvme1n1 NVMe ZNS Identify Controller: zasl : 5 Identify ZNS Namespace Information specific to a Zoned Namespace can be found in this command set's Identify Namespace. # nvme zns id-ns /dev/nvme1n1 ZNS Command Set Identify Namespace: zoc : 0 ozcs : 1 mar : 0xffffffff mor : 0xffffffff rrl : 0 frl : 0 lbafe 0: zsze:0x100000 zdes:0 (in use) More detailed information can be found with the '-H' (human readable) option: # nvme zns id-ns /dev/nvme1n1 -H ZNS Command Set Identify Namespace: zoc : 0 Zone Operation Characteristics [1:1] : 0 Zone Active Excursions: No [0:0] : 0 Variable Zone Capacity: No ozcs : 1 Optional Zoned Command Support [2:2] : 0x1 Read Across Zone Boundaries: Yes mar : 0xffffffff mor : 0xffffffff rrl : Not Reported frl : Not Reported LBA Format Extension 0 : Zone Size: 0x100000 LBAs - Zone Descriptor Extension Size: 0 bytes (in use) If the output is intended to be processed by another script, a more computer friendly json format can be requested with the '-o json' option: # nvme zns id-ns /dev/nvme1n1 -o json { \"zoc\" : 0, \"ozcs\" : 1, \"mar\" : 4294967295, \"mor\" : 4294967295, \"rrl\" : 0, \"frl\" : 0, \"lbafe\" : [ { \"zsze\" : 1048576, \"zdes\" : 0 } ] } Reporting Zones The 'report-zones' command can get information on individual zones, including their current zone state and write pointer. The following example retreives the first 10 zone descriptors. # nvme zns report-zones /dev/nvme1n1 -d 10 nr_zones: 373 SLBA: 0x0 WP: 0x2000 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x200000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x300000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x400000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x500000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x600000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x700000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x800000 WP: 0x8c1000 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x900000 WP: 0x900000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Resetting the zone To reset the write pointer and return the zone to the EMPTY state, the 'zone-reset' command can be used. The following example resets all zones with the '-a' option (WARNING: this effectively deletes the zone's data). # nvme zns reset-zone /dev/nvme1n1 -a zns-reset-zone: Success, action:4 zone:0 nsid:1 The 'report-zones' will now show all the zones are reset to the empty state: # nvme zns report-zones /dev/nvme1n1 -d 10 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x200000 WP: 0x200000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x300000 WP: 0x300000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x400000 WP: 0x400000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x500000 WP: 0x500000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x600000 WP: 0x600000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x700000 WP: 0x700000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x800000 WP: 0x800000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x900000 WP: 0x900000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Opening a zone Explicitly opening a zone will make it ready for immediate write access and consumes an Open Resource. Opening the first zone: # nvme zns open-zone /dev/nvme1n1 zns-open-zone: Success, action:3 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: EXP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Closing a zone Closing the zone releases the open resource and can be done on either explicitly or implicitly open zones. Closing the first zone: # nvme zns close-zone /dev/nvme1n1 zns-close-zone: Success, action:1 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: CLOSED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Finishing a zone Finishing a zone sets its state to 'full'. Finishing the first zone: # nvme zns finish-zone /dev/nvme1n1 zns-finish-zone: Success, action:2 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Offlining a zone Offlining a zone makes the zone inaccessible. The data on the zone will no longer be accessible, and writes to the zone will not be possible until the zone is reset. Offlining the first zone: # nvme zns offline-zone /dev/nvme1n1 zns-offline-zone: Success, action:5 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0 Cap: 0x100000 State: OFFLINE Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 Zone Append You can append data to specific zones. In this method, you only specify which zone to append data to, and the device will return the LBA where it stored the data. Here are a few examples below. Append \"hello world\" to the first zone block (512 bytes in this example): # echo \"hello world\" | nvme zns zone-append /dev/nvme1n1 -z 512 Success appended data to LBA 0 Read the data back from LBA 0 to verify it saved our data: # nvme read /dev/nvme1n1 -z 512 hello world read: Success Now append more data and verify its contents: # echo \"goodbye world\" | nvme zns zone-append /dev/nvme1n1 -z 512 Success appended data to LBA 1 # nvme read /dev/nvme1n1 -z 512 -s 1 goodbye world read: Success Since we've appended two blocks to zone 0, we can check the current report zones to verify the current write pointer: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x2 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Linux Tools for ZNS"},{"location":"projects/zns/#linux-tools-for-zns","text":"Zoned namespace support was added to the Linux kernel with version 5.9. The initial driver release requires the namespace implement the Zone Append command in order to use with the kernel's block stack.","title":"Linux Tools for ZNS"},{"location":"projects/zns/#nvme-cli","text":"Open source tooling for zns is provided by nvme-cli in the current master branch starting from version 1.12 and onward. It is recommended to use the latest version, which is currently at version 1.13. The ZNS specific commands all use the zns command line prefix. You can view further available commands by checking its help: # nvme zns help nvme-1.12 usage: nvme zns <command> [<device>] [<args>] The '<device>' may be either an NVMe character device (ex: /dev/nvme0) or an nvme block device (ex: /dev/nvme0n1). Zoned Namespace Command Set The following are all implemented sub-commands: id-ctrl Retrieve ZNS controller identification id-ns Retrieve ZNS namespace identification zone-mgmt-recv Sends the zone management receive command zone-mgmt-send Sends the zone management send command report-zones Retrieve the Report Zones report close-zone Closes one or more zones finish-zone Finishes one or more zones open-zone Opens one or more zones reset-zone Resets one or more zones offline-zone Offlines one or more zones set-zone-desc Attaches zone descriptor extension data zone-append Writes data and metadata (if applicable), appended to the end of the requested zone changed-zone-list Retrieves the changed zone list log","title":"nvme-cli"},{"location":"projects/zns/#identify-zns-controller","text":"The Zoned Namespace Command Set specification currently defines only one field in the command set's Identify Controller: the Zone Append Size Limit (ZASL), encoding the maximum command size for a Zone Append command. The example below returns '5', which corresponds to 128k bytes for maximum append: # nvme zns id-ctrl /dev/nvme1n1 NVMe ZNS Identify Controller: zasl : 5","title":"Identify ZNS Controller"},{"location":"projects/zns/#identify-zns-namespace","text":"Information specific to a Zoned Namespace can be found in this command set's Identify Namespace. # nvme zns id-ns /dev/nvme1n1 ZNS Command Set Identify Namespace: zoc : 0 ozcs : 1 mar : 0xffffffff mor : 0xffffffff rrl : 0 frl : 0 lbafe 0: zsze:0x100000 zdes:0 (in use) More detailed information can be found with the '-H' (human readable) option: # nvme zns id-ns /dev/nvme1n1 -H ZNS Command Set Identify Namespace: zoc : 0 Zone Operation Characteristics [1:1] : 0 Zone Active Excursions: No [0:0] : 0 Variable Zone Capacity: No ozcs : 1 Optional Zoned Command Support [2:2] : 0x1 Read Across Zone Boundaries: Yes mar : 0xffffffff mor : 0xffffffff rrl : Not Reported frl : Not Reported LBA Format Extension 0 : Zone Size: 0x100000 LBAs - Zone Descriptor Extension Size: 0 bytes (in use) If the output is intended to be processed by another script, a more computer friendly json format can be requested with the '-o json' option: # nvme zns id-ns /dev/nvme1n1 -o json { \"zoc\" : 0, \"ozcs\" : 1, \"mar\" : 4294967295, \"mor\" : 4294967295, \"rrl\" : 0, \"frl\" : 0, \"lbafe\" : [ { \"zsze\" : 1048576, \"zdes\" : 0 } ] }","title":"Identify ZNS Namespace"},{"location":"projects/zns/#reporting-zones","text":"The 'report-zones' command can get information on individual zones, including their current zone state and write pointer. The following example retreives the first 10 zone descriptors. # nvme zns report-zones /dev/nvme1n1 -d 10 nr_zones: 373 SLBA: 0x0 WP: 0x2000 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x200000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x300000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x400000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x500000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x600000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x700000 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x800000 WP: 0x8c1000 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x900000 WP: 0x900000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Reporting Zones"},{"location":"projects/zns/#resetting-the-zone","text":"To reset the write pointer and return the zone to the EMPTY state, the 'zone-reset' command can be used. The following example resets all zones with the '-a' option (WARNING: this effectively deletes the zone's data). # nvme zns reset-zone /dev/nvme1n1 -a zns-reset-zone: Success, action:4 zone:0 nsid:1 The 'report-zones' will now show all the zones are reset to the empty state: # nvme zns report-zones /dev/nvme1n1 -d 10 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x200000 WP: 0x200000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x300000 WP: 0x300000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x400000 WP: 0x400000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x500000 WP: 0x500000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x600000 WP: 0x600000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x700000 WP: 0x700000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x800000 WP: 0x800000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x900000 WP: 0x900000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Resetting the zone"},{"location":"projects/zns/#opening-a-zone","text":"Explicitly opening a zone will make it ready for immediate write access and consumes an Open Resource. Opening the first zone: # nvme zns open-zone /dev/nvme1n1 zns-open-zone: Success, action:3 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: EXP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Opening a zone"},{"location":"projects/zns/#closing-a-zone","text":"Closing the zone releases the open resource and can be done on either explicitly or implicitly open zones. Closing the first zone: # nvme zns close-zone /dev/nvme1n1 zns-close-zone: Success, action:1 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x0 Cap: 0x100000 State: CLOSED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Closing a zone"},{"location":"projects/zns/#finishing-a-zone","text":"Finishing a zone sets its state to 'full'. Finishing the first zone: # nvme zns finish-zone /dev/nvme1n1 zns-finish-zone: Success, action:2 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0xffffffffffffffff Cap: 0x100000 State: FULL Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Finishing a zone"},{"location":"projects/zns/#offlining-a-zone","text":"Offlining a zone makes the zone inaccessible. The data on the zone will no longer be accessible, and writes to the zone will not be possible until the zone is reset. Offlining the first zone: # nvme zns offline-zone /dev/nvme1n1 zns-offline-zone: Success, action:5 zone:0 nsid:1 Verifying its current state: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0 Cap: 0x100000 State: OFFLINE Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Offlining a zone"},{"location":"projects/zns/#zone-append","text":"You can append data to specific zones. In this method, you only specify which zone to append data to, and the device will return the LBA where it stored the data. Here are a few examples below. Append \"hello world\" to the first zone block (512 bytes in this example): # echo \"hello world\" | nvme zns zone-append /dev/nvme1n1 -z 512 Success appended data to LBA 0 Read the data back from LBA 0 to verify it saved our data: # nvme read /dev/nvme1n1 -z 512 hello world read: Success Now append more data and verify its contents: # echo \"goodbye world\" | nvme zns zone-append /dev/nvme1n1 -z 512 Success appended data to LBA 1 # nvme read /dev/nvme1n1 -z 512 -s 1 goodbye world read: Success Since we've appended two blocks to zone 0, we can check the current report zones to verify the current write pointer: # nvme zns report-zones /dev/nvme1n1 -d 2 nr_zones: 373 SLBA: 0x0 WP: 0x2 Cap: 0x100000 State: IMP_OPENED Type: SEQWRITE_REQ Attrs: 0x0 SLBA: 0x100000 WP: 0x100000 Cap: 0x100000 State: EMPTY Type: SEQWRITE_REQ Attrs: 0x0","title":"Zone Append"},{"location":"tests/","text":"System Compliance Tests These articles provide information and how-to step by step explanations for executing various test suites allowing verifying a system compliance to zoned block device requirements. ZBC/ZAC Compliance Tests : Learn how to verify a device compliance to the ZBC and ZAC standards. Kernel Block Layer Tests : Learn how to verify and test handling of zoned block devices of a Linux\u00ae kernel.","title":"System Compliance Tests"},{"location":"tests/#system-compliance-tests","text":"These articles provide information and how-to step by step explanations for executing various test suites allowing verifying a system compliance to zoned block device requirements. ZBC/ZAC Compliance Tests : Learn how to verify a device compliance to the ZBC and ZAC standards. Kernel Block Layer Tests : Learn how to verify and test handling of zoned block devices of a Linux\u00ae kernel.","title":"System Compliance Tests"},{"location":"tests/blktests/","text":"Kernel Block Layer Tests blktests is a test suite for Linux;reg; kernel storage stack, that is, the block I/O layer as well as underlying device specific layers (SCSI, NVMe, SRP, etc). blktests is heavily inspired by the xfstests framework for testing file systems. Recent contributions to blktests added zoned block device tests support. The blktests project is hosted on GitHub . Overview blktests organizes test cases into groups. The test groups currently available are as shown in the table below. Group name Description block Block layer generic tests loop Loopback device tests meta blktests self tests nbd Network block device driver tests nvme NVMe driver tests nvmeof-mp NVME-over-fabrics multipath tests scsi SCSI layer tests srp SCSI RDMA Protocol driver tests zbd Zoned block device tests The test groups supporting zoned block devices are block and zbd . blkzone and fio version 3.9 or higher must be installed for executing these test groups. Configuration Detailed generic information on how to configure and run blktests is provided here . For executing tests against a physical zoned block device (e.g. a ZBC/ZAC disk), the following config file should be prepared and copied to the blktests installation directory. Tests can also be executed directly from the source directory if the config file is copied in that location. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 With this configuration, all tests relevant to zoned block devices will be executed. The execution duration can be several 10s of minutes depending on the target device and host system. To Shorten execution, the options TIMEOUT and QUICK_RUN can be added to the configuration. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 Of note is that the block layer generic tests of the block group also include test cases executed against a logical device ( null_blk block device). These tests are executed twice, once with the null_blk device configured as a regular block device and a second time with the null_blk device configured as a zoned block device. To reduce the test cases execution to only the physical device specified in the configuration file, the DEVICE_ONLY option can be set. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 # Exercise only the devices in TEST_DEVS DEVICE_ONLY=1 Execution blktests execution is done using the check script present in the top level directory. This script optionally takes as argument a list of test groups or test cases to execute. Bu default, without any argument, all test groups will be executed. For zoned block device tests, executing the test cases of the block and zbd test groups is sufficient. As discussed in the configuration section above, several options can speedup the execution of blktests . Such quick runs are indeed faster but at the cost of weaker testing compared to full runs. Full Run The following command executes all test cases relevant to zoned block devices with the device specified in the config configuration file as target. In this example, no options are added. # ./check block zbd block/001 (stress device hotplugging) [passed] runtime 120.013s ... 123.440s block/002 (remove a device while running blktrace) [passed] runtime 0.741s ... 0.761s block/003 => sdd (run various discard sizes) [not run] /dev/sdd is a zoned block device block/004 => sdd (run lots of flushes) [passed] runtime 98.876s ... 82.665s write iops 170 ... 203 block/005 => sdd (switch schedulers while doing IO) [passed] read iops 1415 ... 791 runtime 6.765s ... 11.242s block/006 (run null-blk in blocking mode) [passed] read iops 154220 ... 152356 runtime 19.158s ... 19.229s block/006 (zoned) (run null-blk in blocking mode) [passed] read iops 150692 ... 146826 runtime 19.128s ... 19.720s block/007 => sdd (test classic and hybrid IO polling) [not run] /dev/sdd is a zoned block device block/008 (do IO while hotplugging CPUs) [not run] CPU hotplugging is not supported block/009 (check page-cache coherency after BLKDISCARD) [passed] runtime 0.827s ... 0.868s block/010 (run I/O on null_blk with shared and non-shared tags) [passed] Individual tags read iops 254661 ... 253371 Shared tags read iops 174876 ... 174933 runtime 695.714s ... 715.579s block/010 (zoned) (run I/O on null_blk with shared and non-shared tags) [passed] Individual tags read iops 253356 ... 252487 Shared tags read iops 174871 ... 174780 runtime 667.479s ... 652.577s block/011 => sdd (disable PCI device while doing I/O) [passed] runtime 8.293s ... 17.760s block/012 => sdd (check that a read-only block device fails writes) [not run] /dev/sdd is a zoned block device block/013 => sdd (try BLKRRPART on a mounted device) [not run] /dev/sdd is a zoned block device block/014 (run null-blk with blk-mq and timeout injection configured) [not run] null_blk module does not have parameter timeout block/015 (run null-blk on different schedulers with requeue injection configured) [not run] null_blk module does not have parameter requeue block/016 (send a signal to a process waiting on a frozen queue) [passed] runtime 8.055s ... 8.055s block/016 (zoned) (send a signal to a process waiting on a frozen queue) [passed] runtime 8.055s ... 8.055s block/017 (do I/O and check the inflight counter) [passed] runtime 1.691s ... 1.675s block/017 (zoned) (do I/O and check the inflight counter) [passed] runtime 1.690s ... 1.692s block/018 (do I/O and check iostats times) [passed] runtime 5.075s ... 5.077s block/019 => sdd (break PCI link device while doing I/O) [not run] /dev/sdd is a zoned block device block/020 (run null-blk on different schedulers with only one hardware tag) [passed] runtime 43.139s ... 43.191s block/020 (zoned) (run null-blk on different schedulers with only one hardware tag) [passed] runtime 43.120s ... 43.206s block/021 (read/write nr_requests on null-blk with different schedulers) [passed] runtime 1.396s ... 1.393s block/021 (zoned) (read/write nr_requests on null-blk with different schedulers) [passed] runtime 1.399s ... 1.419s block/023 (do I/O on all null_blk queue modes) [passed] runtime 0.276s ... 0.280s block/023 (zoned) (do I/O on all null_blk queue modes) [passed] runtime 0.279s ... 0.302s block/024 (do I/O faster than a jiffy and check iostats times) [passed] runtime 4.902s ... 4.904s block/025 (do a huge discard with 4k sector size) [passed] runtime 8.869s ... 8.921s block/027 (stress device hotplugging with running fio jobs and different schedulers) [not run] no support for io cgroup controller; if it is enabled, you may need to boot with cgroup_no_v1=io block/028 (do I/O on scsi_debug with DIF/DIX enabled) [passed] runtime 19.924s ... 19.942s zbd/001 => sdd (sysfs and ioctl) [passed] runtime ... 0.706s zbd/002 => sdd (report zone) [passed] runtime ... 6.534s zbd/003 => sdd (reset sequential required zones) [passed] runtime ... 13.324s zbd/004 => sdd (write split across sequential zones) [passed] runtime ... 17.019s zbd/005 => sdd (write command ordering) [passed] runtime ... 178.335s write io ... 7196672 write iops ... 933 zbd/006 => sdd (revalidate) [passed] runtime ... 61.028s write io ... 1805004 write iops ... 15041 The output of some tests is [not run] . This is not a failure. This only indicates that the target device specified in the configuration file does not support the feature being tested. For example, in the above run, test case block/003 is not executed against the specified host managed disk because the disk does not support the discard command. Quick Run blkltests execution can be accelerated using the TIMEOUT , QUICK_RUN and DEVICE_ONLY configuration options. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 # Exercise only the devices in TEST_DEVS DEVICE_ONLY=1 Running only the block and zbd test groups with the above configuration gives the following results. # ./check block zbd block/003 => sdd (run various discard sizes) [not run] /dev/sdd is a zoned block device block/004 => sdd (run lots of flushes) [passed] runtime 913.097s ... 33.714s write iops ... 239 block/005 => sdd (switch schedulers while doing IO) [passed] read iops 2975 ... 3573 runtime 22.510s ... 19.194s block/007 => sdd (test classic and hybrid IO polling) [not run] /dev/sdd is a zoned block device block/008 => sdd (do IO while hotplugging CPUs) [not run] /dev/sdd is a zoned block device block/011 => sdd (disable PCI device while doing I/O) [passed] runtime ... 30.322s block/012 => sdd (check that a read-only block device fails writes) [not run] /dev/sdd is a zoned block device block/013 => sdd (try BLKRRPART on a mounted device) [not run] /dev/sdd is a zoned block device block/019 => sdd (break PCI link device while doing I/O) [not run] /dev/sdd is a zoned block device zbd/001 => sdd (sysfs and ioctl) [passed] runtime 1.328s ... 0.521s zbd/003 => sdd (reset sequential required zones) [passed] runtime 9.591s ... 11.008s zbd/004 => sdd (write split across sequential zones) [passed] runtime 11.772s ... 11.743s zbd/005 => sdd (write command ordering) [passed] runtime 59.688s ... 167.521s write io 7201536 ... 7113472 write iops 933 ... 921 zbd/006 => sdd (revalidate) [passed] runtime 7.063s ... 39.464s write io ... 2097152 write iops ... 48971","title":"Kernel Block Layer Tests"},{"location":"tests/blktests/#kernel-block-layer-tests","text":"blktests is a test suite for Linux;reg; kernel storage stack, that is, the block I/O layer as well as underlying device specific layers (SCSI, NVMe, SRP, etc). blktests is heavily inspired by the xfstests framework for testing file systems. Recent contributions to blktests added zoned block device tests support. The blktests project is hosted on GitHub .","title":"Kernel Block Layer Tests"},{"location":"tests/blktests/#overview","text":"blktests organizes test cases into groups. The test groups currently available are as shown in the table below. Group name Description block Block layer generic tests loop Loopback device tests meta blktests self tests nbd Network block device driver tests nvme NVMe driver tests nvmeof-mp NVME-over-fabrics multipath tests scsi SCSI layer tests srp SCSI RDMA Protocol driver tests zbd Zoned block device tests The test groups supporting zoned block devices are block and zbd . blkzone and fio version 3.9 or higher must be installed for executing these test groups.","title":"Overview"},{"location":"tests/blktests/#configuration","text":"Detailed generic information on how to configure and run blktests is provided here . For executing tests against a physical zoned block device (e.g. a ZBC/ZAC disk), the following config file should be prepared and copied to the blktests installation directory. Tests can also be executed directly from the source directory if the config file is copied in that location. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 With this configuration, all tests relevant to zoned block devices will be executed. The execution duration can be several 10s of minutes depending on the target device and host system. To Shorten execution, the options TIMEOUT and QUICK_RUN can be added to the configuration. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 Of note is that the block layer generic tests of the block group also include test cases executed against a logical device ( null_blk block device). These tests are executed twice, once with the null_blk device configured as a regular block device and a second time with the null_blk device configured as a zoned block device. To reduce the test cases execution to only the physical device specified in the configuration file, the DEVICE_ONLY option can be set. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 # Exercise only the devices in TEST_DEVS DEVICE_ONLY=1","title":"Configuration"},{"location":"tests/blktests/#execution","text":"blktests execution is done using the check script present in the top level directory. This script optionally takes as argument a list of test groups or test cases to execute. Bu default, without any argument, all test groups will be executed. For zoned block device tests, executing the test cases of the block and zbd test groups is sufficient. As discussed in the configuration section above, several options can speedup the execution of blktests . Such quick runs are indeed faster but at the cost of weaker testing compared to full runs.","title":"Execution"},{"location":"tests/blktests/#full-run","text":"The following command executes all test cases relevant to zoned block devices with the device specified in the config configuration file as target. In this example, no options are added. # ./check block zbd block/001 (stress device hotplugging) [passed] runtime 120.013s ... 123.440s block/002 (remove a device while running blktrace) [passed] runtime 0.741s ... 0.761s block/003 => sdd (run various discard sizes) [not run] /dev/sdd is a zoned block device block/004 => sdd (run lots of flushes) [passed] runtime 98.876s ... 82.665s write iops 170 ... 203 block/005 => sdd (switch schedulers while doing IO) [passed] read iops 1415 ... 791 runtime 6.765s ... 11.242s block/006 (run null-blk in blocking mode) [passed] read iops 154220 ... 152356 runtime 19.158s ... 19.229s block/006 (zoned) (run null-blk in blocking mode) [passed] read iops 150692 ... 146826 runtime 19.128s ... 19.720s block/007 => sdd (test classic and hybrid IO polling) [not run] /dev/sdd is a zoned block device block/008 (do IO while hotplugging CPUs) [not run] CPU hotplugging is not supported block/009 (check page-cache coherency after BLKDISCARD) [passed] runtime 0.827s ... 0.868s block/010 (run I/O on null_blk with shared and non-shared tags) [passed] Individual tags read iops 254661 ... 253371 Shared tags read iops 174876 ... 174933 runtime 695.714s ... 715.579s block/010 (zoned) (run I/O on null_blk with shared and non-shared tags) [passed] Individual tags read iops 253356 ... 252487 Shared tags read iops 174871 ... 174780 runtime 667.479s ... 652.577s block/011 => sdd (disable PCI device while doing I/O) [passed] runtime 8.293s ... 17.760s block/012 => sdd (check that a read-only block device fails writes) [not run] /dev/sdd is a zoned block device block/013 => sdd (try BLKRRPART on a mounted device) [not run] /dev/sdd is a zoned block device block/014 (run null-blk with blk-mq and timeout injection configured) [not run] null_blk module does not have parameter timeout block/015 (run null-blk on different schedulers with requeue injection configured) [not run] null_blk module does not have parameter requeue block/016 (send a signal to a process waiting on a frozen queue) [passed] runtime 8.055s ... 8.055s block/016 (zoned) (send a signal to a process waiting on a frozen queue) [passed] runtime 8.055s ... 8.055s block/017 (do I/O and check the inflight counter) [passed] runtime 1.691s ... 1.675s block/017 (zoned) (do I/O and check the inflight counter) [passed] runtime 1.690s ... 1.692s block/018 (do I/O and check iostats times) [passed] runtime 5.075s ... 5.077s block/019 => sdd (break PCI link device while doing I/O) [not run] /dev/sdd is a zoned block device block/020 (run null-blk on different schedulers with only one hardware tag) [passed] runtime 43.139s ... 43.191s block/020 (zoned) (run null-blk on different schedulers with only one hardware tag) [passed] runtime 43.120s ... 43.206s block/021 (read/write nr_requests on null-blk with different schedulers) [passed] runtime 1.396s ... 1.393s block/021 (zoned) (read/write nr_requests on null-blk with different schedulers) [passed] runtime 1.399s ... 1.419s block/023 (do I/O on all null_blk queue modes) [passed] runtime 0.276s ... 0.280s block/023 (zoned) (do I/O on all null_blk queue modes) [passed] runtime 0.279s ... 0.302s block/024 (do I/O faster than a jiffy and check iostats times) [passed] runtime 4.902s ... 4.904s block/025 (do a huge discard with 4k sector size) [passed] runtime 8.869s ... 8.921s block/027 (stress device hotplugging with running fio jobs and different schedulers) [not run] no support for io cgroup controller; if it is enabled, you may need to boot with cgroup_no_v1=io block/028 (do I/O on scsi_debug with DIF/DIX enabled) [passed] runtime 19.924s ... 19.942s zbd/001 => sdd (sysfs and ioctl) [passed] runtime ... 0.706s zbd/002 => sdd (report zone) [passed] runtime ... 6.534s zbd/003 => sdd (reset sequential required zones) [passed] runtime ... 13.324s zbd/004 => sdd (write split across sequential zones) [passed] runtime ... 17.019s zbd/005 => sdd (write command ordering) [passed] runtime ... 178.335s write io ... 7196672 write iops ... 933 zbd/006 => sdd (revalidate) [passed] runtime ... 61.028s write io ... 1805004 write iops ... 15041 The output of some tests is [not run] . This is not a failure. This only indicates that the target device specified in the configuration file does not support the feature being tested. For example, in the above run, test case block/003 is not executed against the specified host managed disk because the disk does not support the discard command.","title":"Full Run"},{"location":"tests/blktests/#quick-run","text":"blkltests execution can be accelerated using the TIMEOUT , QUICK_RUN and DEVICE_ONLY configuration options. # Tests target device list TEST_DEVS=(/dev/sdd) # Enable zoned block device mode for the block group genertic tests RUN_ZONED_TESTS=1 # Speed up execution (weaker tests) QUICK_RUN=1 TIMEOUT=30 # Exercise only the devices in TEST_DEVS DEVICE_ONLY=1 Running only the block and zbd test groups with the above configuration gives the following results. # ./check block zbd block/003 => sdd (run various discard sizes) [not run] /dev/sdd is a zoned block device block/004 => sdd (run lots of flushes) [passed] runtime 913.097s ... 33.714s write iops ... 239 block/005 => sdd (switch schedulers while doing IO) [passed] read iops 2975 ... 3573 runtime 22.510s ... 19.194s block/007 => sdd (test classic and hybrid IO polling) [not run] /dev/sdd is a zoned block device block/008 => sdd (do IO while hotplugging CPUs) [not run] /dev/sdd is a zoned block device block/011 => sdd (disable PCI device while doing I/O) [passed] runtime ... 30.322s block/012 => sdd (check that a read-only block device fails writes) [not run] /dev/sdd is a zoned block device block/013 => sdd (try BLKRRPART on a mounted device) [not run] /dev/sdd is a zoned block device block/019 => sdd (break PCI link device while doing I/O) [not run] /dev/sdd is a zoned block device zbd/001 => sdd (sysfs and ioctl) [passed] runtime 1.328s ... 0.521s zbd/003 => sdd (reset sequential required zones) [passed] runtime 9.591s ... 11.008s zbd/004 => sdd (write split across sequential zones) [passed] runtime 11.772s ... 11.743s zbd/005 => sdd (write command ordering) [passed] runtime 59.688s ... 167.521s write io 7201536 ... 7113472 write iops 933 ... 921 zbd/006 => sdd (revalidate) [passed] runtime 7.063s ... 39.464s write io ... 2097152 write iops ... 48971","title":"Quick Run"},{"location":"tests/zbc-tests/","text":"ZBC/ZAC Compliance Tests Developers may face many problems with an application development if the system being used has non-compliant components, either software or hardware. The typical problems that can be faced may be due to a SAS HBA not fully compatible with the ZBC and ZAC standards (e.g. The HBA has a defective translation layer implementation) or the kernel version includes a bug resulting for instance in write commands not being translated correctly or an invalid command failure processing. Many of these problems can be identified early by executing libzbc conformance test suite. libzbc Conformance Test Suite libzbc implements a test suite primarily aiming at checking that a disk fully conforms to the definition and constraints of the ZAC and ZBC standards. libzbc test suite works equally well with physical disks (SAS and SATA) as well as emulated disks created with tcmu-runner . Information on how to compile and install libzbc with the test suite enabled can be found here . In this chapter, the following disk configuration is used throughout the examples shown. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3 /dev/sdb is a SATA disk connected to an AHCI controller (SATA port), /dev/sdc is a SAS disk connected to a SAS HBA and /dev/sdc is a SATA disk connected to the same SAS HBA. Checking Serial ATA Disks To check a SATA ZAC disk correct operation and conformance to the ZAC standard, libzbc test suite must be executed using the \"--ata\" option. Note libzbc test suite must be executed against the disk SCSI generic node file (e.g. /dev/sg path) to enable the full range of direct SCSI or ATA command execution without any interference from the kernel block I/O stack. libzbc test suite will not run if the disk block device file ( /dev/sd ) is specified. Using the AHCI connected SATA disk /dev/sdb , which corresponds to the SCSI generic node file /dev/sg1 , the execution of the 107 test cases of libzbc test suite result in the following output. # cd libzbc/test # ./zbc_test.sh --ata /dev/sg1 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] 00.013: REPORT_ZONES (reporting option 0x11) command completion... [Passed] 00.014: REPORT_ZONES (reporting option 0x3F) command completion... [Passed] 00.020: OPEN_ZONE command completion... [Passed] 00.030: CLOSE_ZONE command completion... [Passed] 00.040: FINISH_ZONE command completion... [Passed] 00.050: RESET_WRITE_PTR command completion... [Passed] 00.060: WRITE command completion... [Passed] 00.070: READ command completion... [Passed] Executing section 01 - sense key, sense code tests... 01.010: REPORT_ZONES logical block out of range... [Passed] 01.011: REPORT_ZONES invalid reporting option... [Passed] 01.020: OPEN_ZONE invalid zone start lba... [Passed] 01.021: OPEN_ZONE insufficient zone resources... [Passed] 01.022: OPEN_ZONE insufficient zone resources (ALL bit set)... [Passed] 01.023: OPEN_ZONE conventional zone... [Passed] 01.024: OPEN zone LBA at End of Medium... [Passed] 01.030: CLOSE_ZONE invalid zone start lba... [Passed] 01.031: CLOSE_ZONE conventional zone... [Passed] 01.032: CLOSE zone LBA at End of Medium... [Passed] 01.040: FINISH_ZONE invalid zone start lba... [Passed] 01.041: FINISH_ZONE conventional zone... [Passed] 01.042: FINISH zone LBA at End of Medium... [Passed] 01.050: RESET_WRITE_PTR invalid zone start lba... [Passed] 01.051: RESET_WRITE_PTR conventional zone... [Passed] 01.052: RESET zone LBA at End of Medium... [Passed] 01.060: READ access sequential zone LBAs after write pointer... [Passed] 01.061: READ sequential zones boundary violation... [Passed] 01.062: READ conventional/sequential zones boundary violation... [Passed] 01.063: READ across write-pointer zones (FULL->FULL)... [Passed] 01.064: READ access write pointer zone LBAs starting after write pointer... [Passed] 01.070: WRITE unaligned write in sequential zone... [Passed] 01.071: WRITE sequential zone boundary violation... [Passed] 01.072: WRITE insufficient zone resources... [Passed] 01.073: WRITE full zone... [Passed] 01.074: WRITE physical sector unaligned write to sequential zone... [Passed] 01.075: WRITE unaligned ending below write pointer... [Passed] 01.076: WRITE unaligned crossing write pointer... [Passed] 01.077: WRITE across zone-type spaces (cross-type boundary violation)... [Passed] 01.080: READ cross-zone FULL->IOPENL and ending above Write Pointer... [Passed] 01.081: READ cross-zone FULL->IOPENL and ending below Write Pointer... [Passed] 01.082: READ cross-zone IOPENH->FULL starting below Write Pointer... [Passed] 01.083: READ cross-zone IOPENL->FULL starting above Write Pointer... [Passed] 01.090: WRITE cross-zone FULL->IOPENL and ending above Write Pointer... [Passed] 01.091: WRITE cross-zone FULL->IOPENL and ending below Write Pointer... [Passed] 01.092: WRITE cross-zone IOPENH->FULL starting below Write Pointer... [Passed] 01.093: WRITE cross-zone IOPENL->FULL starting above Write Pointer... [Passed] 01.094: WRITE cross-zone FULL->EMPTY... [Passed] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... 02.001: OPEN_ZONE empty to explicit open... [Passed] 02.002: CLOSE_ZONE empty to empty... [Passed] 02.003: FINISH_ZONE empty to full... [Passed] 02.004: RESET_WRITE_PTR empty to empty... [Passed] 02.005: OPEN_ZONE implicit open to explicit open... [Passed] 02.006: CLOSE_ZONE implicit open to closed... [Passed] 02.007: FINISH_ZONE implicit open to full... [Passed] 02.008: RESET_WRITE_PTR implicit open to empty... [Passed] 02.009: OPEN_ZONE empty to explicit open to explicit open... [Passed] 02.010: CLOSE_ZONE empty to explicit open to empty... [Passed] 02.011: FINISH_ZONE empty to explicit open to full... [Passed] 02.012: RESET_WRITE_PTR empty to explicit open to empty... [Passed] 02.013: OPEN_ZONE implicit open to explicit open to explicit open... [Passed] 02.014: CLOSE_ZONE implicit open to explicit open to closed... [Passed] 02.015: FINISH_ZONE implicit open to explicit open to full... [Passed] 02.016: RESET_WRITE_PTR implicit open to explicit open to empty... [Passed] 02.017: OPEN_ZONE closed to explicit open... [Passed] 02.018: CLOSE_ZONE closed to closed... [Passed] 02.019: FINISH_ZONE closed to full... [Passed] 02.020: RESET_WRITE_PTR closed to empty... [Passed] 02.021: OPEN_ZONE full to full... [Passed] 02.022: CLOSE_ZONE full to full... [Passed] 02.023: FINISH_ZONE full to full... [Passed] 02.024: RESET_WRITE_PTR full to empty... [Passed] 02.025: OPEN_ZONE empty to empty (ALL bit set)... [Passed] 02.026: CLOSE_ZONE empty to empty (ALL bit set)... [Passed] 02.027: FINISH_ZONE empty to empty (ALL bit set)... [Passed] 02.028: RESET_WRITE_PTR empty to empty (ALL bit set)... [Passed] 02.029: OPEN_ZONE implicit open to implicit open (ALL bit set)... [Passed] 02.030: CLOSE_ZONE implicit open to close (ALL bit set)... [Passed] 02.031: FINISH_ZONE implicit open to full (ALL bit set)... [Passed] 02.032: RESET_WRITE_PTR implicit open to empty (ALL bit set)... [Passed] 02.033: OPEN_ZONE empty to explicit open to explicit open (ALL bit set)... [Passed] 02.034: CLOSE_ZONE empty to explicit open to empty (ALL bit set)... [Passed] 02.035: FINISH_ZONE empty to explicit open to full (ALL bit set)... [Passed] 02.036: RESET_WRITE_PTR empty to explicit open to empty (ALL bit set)... [Passed] 02.037: OPEN_ZONE implicit open to explicit open to explicit_open (ALL bit set)... [Passed] 02.038: CLOSE_ZONE implicit open to explicit open to closed (ALL bit set)... [Passed] 02.039: FINISH_ZONE implicit open to explicit open to full (ALL bit set)... [Passed] 02.040: RESET_WRITE_PTR implicit open to explicit open to empty (ALL bit set)... [Passed] 02.041: OPEN_ZONE closed to explicit open (ALL bit set)... [Passed] 02.042: CLOSE_ZONE closed to closed (ALL bit set)... [Passed] 02.043: FINISH_ZONE closed to full (ALL bit set)... [Passed] 02.044: RESET_WRITE_PTR closed to empty test (ALL bit set)... [Passed] 02.045: OPEN_ZONE full to full (ALL bit set)... [Passed] 02.046: CLOSE_ZONE full to full (ALL bit set)... [Passed] 02.047: FINISH_ZONE full to full (ALL bit set)... [Passed] 02.048: RESET_WRITE_PTR full to empty (ALL bit set)... [Passed] 02.070: WRITE empty to implicit open... [Passed] 02.071: WRITE empty to full... [Passed] 02.072: WRITE implicit open to implicit open... [Passed] 02.073: WRITE implicit open to full... [Passed] 02.074: WRITE closed to implicit open... [Passed] 02.075: WRITE closed to full... [Passed] 02.076: WRITE explicit open to explicit open... [Passed] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] The same execution procedure can be applied to the SATA disk /dev/sdd connected to the SAS HBA. This disk has the SCSI generic node file /dev/sg3 . # cd libzbc/test # ./zbc_test.sh --ata /dev/sg3 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 00.070: READ command completion... [Passed] Executing section 01 - sense key, sense code tests... 01.010: REPORT_ZONES logical block out of range... [Passed] [...] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... 02.001: OPEN_ZONE empty to explicit open... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] If a test case fails, an error message similar to the example below is shown and the test suite stops execution. # cd libzbc/test # ./zbc_test.sh --ata XXX Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] [...] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... [...] 02.078: WRITE full to full... [Failed] => Expected Illegal-request / Invalid-field-in-cdb Got Illegal-request / Unaligned-write-command # A test case failure is a strong indicator that the disk may not be fully compliant with the ZAC specifications. Most failures can generally be confirmed using a bus analyzer to obtain a command trace for the failing test sequence. Other root cause for any test failure are always a possibility. Checking SAS Disks Testing physical SAS disks as well as tcmu-runner emulated SCSI disks do not require the --ata option. Using the SAS disk /dev/sg2 , the test suite output is the same as with SATA disks if all tests pass. # cd libzbc/test # ./zbc_test.sh /dev/sg2 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] 00.013: REPORT_ZONES (reporting option 0x11) command completion... [Passed] 00.014: REPORT_ZONES (reporting option 0x3F) command completion... [Passed] 00.020: OPEN_ZONE command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] Checking the Kernel SCSI-To-ATA Translation Layer When libzbc test suite is executed with the \"--ata\" option, ATA commands are sent directly to the disk using the SCSI generic driver passthrough facility. These ATA commands are directly embedded into the ATA16 SCSI passthrough command, allowing to completely bypass the kernel SCSI to ATA translation layer implemented by the kernel libata component. Executing libzbc test suite against a SATA disk connected to a SATA port (e.g. An AHCI adapter port) will result in the test suite issuing only SCSI commands that will be translated by the kernel libata before sending as ATA commands to the disk. That is, the kernel SAT can be exercised. # cd libzbc/test # ./zbc_test.sh /dev/sg1 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] If all tests pass, then the kernel SAT layer can be considered as correct. On the other hand, if some tests fails while all tests pass with the --ata option used, then the kernel SAT implementation becomes a candidate as the root cause for the failure. This analysis is summarized in the table below. Probable failure root cause with SATA disks on AHCI ports Checking the HBA SCSI-TO-ATA Translation Layer Similarly to the previous case, executing libzbc test suite without the --ata option against a SATA disk connected to a SAS HBA will exercise the HBA SCSI to ATA command translation by removing the use of the ATA16 passthrough command. # cd libzbc/test # ./zbc_test.sh /dev/sg3 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] Again comparing the result of the test suite execution for the two cases (with and without the \"--ata\" option) gives hints on the root cause of eventual test failures. The table below indicates the possible failure root cause when the test suite reports errors. Probable failure root cause with SATA disks on SAS HBA","title":"ZBC/ZAC Compliance Tests"},{"location":"tests/zbc-tests/#zbczac-compliance-tests","text":"Developers may face many problems with an application development if the system being used has non-compliant components, either software or hardware. The typical problems that can be faced may be due to a SAS HBA not fully compatible with the ZBC and ZAC standards (e.g. The HBA has a defective translation layer implementation) or the kernel version includes a bug resulting for instance in write commands not being translated correctly or an invalid command failure processing. Many of these problems can be identified early by executing libzbc conformance test suite.","title":"ZBC/ZAC Compliance Tests"},{"location":"tests/zbc-tests/#libzbc-conformance-test-suite","text":"libzbc implements a test suite primarily aiming at checking that a disk fully conforms to the definition and constraints of the ZAC and ZBC standards. libzbc test suite works equally well with physical disks (SAS and SATA) as well as emulated disks created with tcmu-runner . Information on how to compile and install libzbc with the test suite enabled can be found here . In this chapter, the following disk configuration is used throughout the examples shown. # lsscsi -g [2:0:0:0] disk ATA INTEL SSDSC2CT18 335u /dev/sda /dev/sg0 [5:0:0:0] zbc ATA HGST HSH721415AL T220 /dev/sdb /dev/sg1 [10:0:2:0] zbc HGST HSH721414AL52M0 a220 /dev/sdc /dev/sg2 [10:0:3:0] zbc ATA HGST HSH721415AL T220 /dev/sdd /dev/sg3 /dev/sdb is a SATA disk connected to an AHCI controller (SATA port), /dev/sdc is a SAS disk connected to a SAS HBA and /dev/sdc is a SATA disk connected to the same SAS HBA.","title":"libzbc Conformance Test Suite"},{"location":"tests/zbc-tests/#checking-serial-ata-disks","text":"To check a SATA ZAC disk correct operation and conformance to the ZAC standard, libzbc test suite must be executed using the \"--ata\" option. Note libzbc test suite must be executed against the disk SCSI generic node file (e.g. /dev/sg path) to enable the full range of direct SCSI or ATA command execution without any interference from the kernel block I/O stack. libzbc test suite will not run if the disk block device file ( /dev/sd ) is specified. Using the AHCI connected SATA disk /dev/sdb , which corresponds to the SCSI generic node file /dev/sg1 , the execution of the 107 test cases of libzbc test suite result in the following output. # cd libzbc/test # ./zbc_test.sh --ata /dev/sg1 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] 00.013: REPORT_ZONES (reporting option 0x11) command completion... [Passed] 00.014: REPORT_ZONES (reporting option 0x3F) command completion... [Passed] 00.020: OPEN_ZONE command completion... [Passed] 00.030: CLOSE_ZONE command completion... [Passed] 00.040: FINISH_ZONE command completion... [Passed] 00.050: RESET_WRITE_PTR command completion... [Passed] 00.060: WRITE command completion... [Passed] 00.070: READ command completion... [Passed] Executing section 01 - sense key, sense code tests... 01.010: REPORT_ZONES logical block out of range... [Passed] 01.011: REPORT_ZONES invalid reporting option... [Passed] 01.020: OPEN_ZONE invalid zone start lba... [Passed] 01.021: OPEN_ZONE insufficient zone resources... [Passed] 01.022: OPEN_ZONE insufficient zone resources (ALL bit set)... [Passed] 01.023: OPEN_ZONE conventional zone... [Passed] 01.024: OPEN zone LBA at End of Medium... [Passed] 01.030: CLOSE_ZONE invalid zone start lba... [Passed] 01.031: CLOSE_ZONE conventional zone... [Passed] 01.032: CLOSE zone LBA at End of Medium... [Passed] 01.040: FINISH_ZONE invalid zone start lba... [Passed] 01.041: FINISH_ZONE conventional zone... [Passed] 01.042: FINISH zone LBA at End of Medium... [Passed] 01.050: RESET_WRITE_PTR invalid zone start lba... [Passed] 01.051: RESET_WRITE_PTR conventional zone... [Passed] 01.052: RESET zone LBA at End of Medium... [Passed] 01.060: READ access sequential zone LBAs after write pointer... [Passed] 01.061: READ sequential zones boundary violation... [Passed] 01.062: READ conventional/sequential zones boundary violation... [Passed] 01.063: READ across write-pointer zones (FULL->FULL)... [Passed] 01.064: READ access write pointer zone LBAs starting after write pointer... [Passed] 01.070: WRITE unaligned write in sequential zone... [Passed] 01.071: WRITE sequential zone boundary violation... [Passed] 01.072: WRITE insufficient zone resources... [Passed] 01.073: WRITE full zone... [Passed] 01.074: WRITE physical sector unaligned write to sequential zone... [Passed] 01.075: WRITE unaligned ending below write pointer... [Passed] 01.076: WRITE unaligned crossing write pointer... [Passed] 01.077: WRITE across zone-type spaces (cross-type boundary violation)... [Passed] 01.080: READ cross-zone FULL->IOPENL and ending above Write Pointer... [Passed] 01.081: READ cross-zone FULL->IOPENL and ending below Write Pointer... [Passed] 01.082: READ cross-zone IOPENH->FULL starting below Write Pointer... [Passed] 01.083: READ cross-zone IOPENL->FULL starting above Write Pointer... [Passed] 01.090: WRITE cross-zone FULL->IOPENL and ending above Write Pointer... [Passed] 01.091: WRITE cross-zone FULL->IOPENL and ending below Write Pointer... [Passed] 01.092: WRITE cross-zone IOPENH->FULL starting below Write Pointer... [Passed] 01.093: WRITE cross-zone IOPENL->FULL starting above Write Pointer... [Passed] 01.094: WRITE cross-zone FULL->EMPTY... [Passed] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... 02.001: OPEN_ZONE empty to explicit open... [Passed] 02.002: CLOSE_ZONE empty to empty... [Passed] 02.003: FINISH_ZONE empty to full... [Passed] 02.004: RESET_WRITE_PTR empty to empty... [Passed] 02.005: OPEN_ZONE implicit open to explicit open... [Passed] 02.006: CLOSE_ZONE implicit open to closed... [Passed] 02.007: FINISH_ZONE implicit open to full... [Passed] 02.008: RESET_WRITE_PTR implicit open to empty... [Passed] 02.009: OPEN_ZONE empty to explicit open to explicit open... [Passed] 02.010: CLOSE_ZONE empty to explicit open to empty... [Passed] 02.011: FINISH_ZONE empty to explicit open to full... [Passed] 02.012: RESET_WRITE_PTR empty to explicit open to empty... [Passed] 02.013: OPEN_ZONE implicit open to explicit open to explicit open... [Passed] 02.014: CLOSE_ZONE implicit open to explicit open to closed... [Passed] 02.015: FINISH_ZONE implicit open to explicit open to full... [Passed] 02.016: RESET_WRITE_PTR implicit open to explicit open to empty... [Passed] 02.017: OPEN_ZONE closed to explicit open... [Passed] 02.018: CLOSE_ZONE closed to closed... [Passed] 02.019: FINISH_ZONE closed to full... [Passed] 02.020: RESET_WRITE_PTR closed to empty... [Passed] 02.021: OPEN_ZONE full to full... [Passed] 02.022: CLOSE_ZONE full to full... [Passed] 02.023: FINISH_ZONE full to full... [Passed] 02.024: RESET_WRITE_PTR full to empty... [Passed] 02.025: OPEN_ZONE empty to empty (ALL bit set)... [Passed] 02.026: CLOSE_ZONE empty to empty (ALL bit set)... [Passed] 02.027: FINISH_ZONE empty to empty (ALL bit set)... [Passed] 02.028: RESET_WRITE_PTR empty to empty (ALL bit set)... [Passed] 02.029: OPEN_ZONE implicit open to implicit open (ALL bit set)... [Passed] 02.030: CLOSE_ZONE implicit open to close (ALL bit set)... [Passed] 02.031: FINISH_ZONE implicit open to full (ALL bit set)... [Passed] 02.032: RESET_WRITE_PTR implicit open to empty (ALL bit set)... [Passed] 02.033: OPEN_ZONE empty to explicit open to explicit open (ALL bit set)... [Passed] 02.034: CLOSE_ZONE empty to explicit open to empty (ALL bit set)... [Passed] 02.035: FINISH_ZONE empty to explicit open to full (ALL bit set)... [Passed] 02.036: RESET_WRITE_PTR empty to explicit open to empty (ALL bit set)... [Passed] 02.037: OPEN_ZONE implicit open to explicit open to explicit_open (ALL bit set)... [Passed] 02.038: CLOSE_ZONE implicit open to explicit open to closed (ALL bit set)... [Passed] 02.039: FINISH_ZONE implicit open to explicit open to full (ALL bit set)... [Passed] 02.040: RESET_WRITE_PTR implicit open to explicit open to empty (ALL bit set)... [Passed] 02.041: OPEN_ZONE closed to explicit open (ALL bit set)... [Passed] 02.042: CLOSE_ZONE closed to closed (ALL bit set)... [Passed] 02.043: FINISH_ZONE closed to full (ALL bit set)... [Passed] 02.044: RESET_WRITE_PTR closed to empty test (ALL bit set)... [Passed] 02.045: OPEN_ZONE full to full (ALL bit set)... [Passed] 02.046: CLOSE_ZONE full to full (ALL bit set)... [Passed] 02.047: FINISH_ZONE full to full (ALL bit set)... [Passed] 02.048: RESET_WRITE_PTR full to empty (ALL bit set)... [Passed] 02.070: WRITE empty to implicit open... [Passed] 02.071: WRITE empty to full... [Passed] 02.072: WRITE implicit open to implicit open... [Passed] 02.073: WRITE implicit open to full... [Passed] 02.074: WRITE closed to implicit open... [Passed] 02.075: WRITE closed to full... [Passed] 02.076: WRITE explicit open to explicit open... [Passed] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] The same execution procedure can be applied to the SATA disk /dev/sdd connected to the SAS HBA. This disk has the SCSI generic node file /dev/sg3 . # cd libzbc/test # ./zbc_test.sh --ata /dev/sg3 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 00.070: READ command completion... [Passed] Executing section 01 - sense key, sense code tests... 01.010: REPORT_ZONES logical block out of range... [Passed] [...] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... 02.001: OPEN_ZONE empty to explicit open... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] If a test case fails, an error message similar to the example below is shown and the test suite stops execution. # cd libzbc/test # ./zbc_test.sh --ata XXX Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] [...] 01.095: WRITE cross-zone EMPTY->EMPTY starting above Write Pointer... [Passed] Executing section 02 - zone state machine tests... [...] 02.078: WRITE full to full... [Failed] => Expected Illegal-request / Invalid-field-in-cdb Got Illegal-request / Unaligned-write-command # A test case failure is a strong indicator that the disk may not be fully compliant with the ZAC specifications. Most failures can generally be confirmed using a bus analyzer to obtain a command trace for the failing test sequence. Other root cause for any test failure are always a possibility.","title":"Checking Serial ATA Disks"},{"location":"tests/zbc-tests/#checking-sas-disks","text":"Testing physical SAS disks as well as tcmu-runner emulated SCSI disks do not require the --ata option. Using the SAS disk /dev/sg2 , the test suite output is the same as with SATA disks if all tests pass. # cd libzbc/test # ./zbc_test.sh /dev/sg2 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] 00.013: REPORT_ZONES (reporting option 0x11) command completion... [Passed] 00.014: REPORT_ZONES (reporting option 0x3F) command completion... [Passed] 00.020: OPEN_ZONE command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed]","title":"Checking SAS Disks"},{"location":"tests/zbc-tests/#checking-the-kernel-scsi-to-ata-translation-layer","text":"When libzbc test suite is executed with the \"--ata\" option, ATA commands are sent directly to the disk using the SCSI generic driver passthrough facility. These ATA commands are directly embedded into the ATA16 SCSI passthrough command, allowing to completely bypass the kernel SCSI to ATA translation layer implemented by the kernel libata component. Executing libzbc test suite against a SATA disk connected to a SATA port (e.g. An AHCI adapter port) will result in the test suite issuing only SCSI commands that will be translated by the kernel libata before sending as ATA commands to the disk. That is, the kernel SAT can be exercised. # cd libzbc/test # ./zbc_test.sh /dev/sg1 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] If all tests pass, then the kernel SAT layer can be considered as correct. On the other hand, if some tests fails while all tests pass with the --ata option used, then the kernel SAT implementation becomes a candidate as the root cause for the failure. This analysis is summarized in the table below. Probable failure root cause with SATA disks on AHCI ports","title":"Checking the Kernel SCSI-To-ATA Translation Layer"},{"location":"tests/zbc-tests/#checking-the-hba-scsi-to-ata-translation-layer","text":"Similarly to the previous case, executing libzbc test suite without the --ata option against a SATA disk connected to a SAS HBA will exercise the HBA SCSI to ATA command translation by removing the use of the ATA16 passthrough command. # cd libzbc/test # ./zbc_test.sh /dev/sg3 Executing section 00 - command completion tests... 00.010: REPORT_ZONES command completion... [Passed] 00.011: REPORT_ZONES (partial bit) command completion... [Passed] 00.012: REPORT_ZONES (reporting option 0x10) command completion... [Passed] [...] 02.077: WRITE explicit open to full... [Passed] 02.078: WRITE full to full... [Passed] Again comparing the result of the test suite execution for the two cases (with and without the \"--ata\" option) gives hints on the root cause of eventual test failures. The table below indicates the possible failure root cause when the test suite reports errors. Probable failure root cause with SATA disks on SAS HBA","title":"Checking the HBA SCSI-TO-ATA Translation Layer"}]}